# Lecture: Introduction to Multimodal Systems

---

**Slide 1–2: Title & Introduction**

Alright, let's get started. This is Multimodal Systems — I'm Gualtiero Volpe, Casa Paganini, InfoMus, DIBRIS, University of Genoa. Today we're covering the introduction: what multimodal systems are, why they matter, and how we got here from traditional interfaces.

---

**Slide 3–4: WIMP**

So let's start with what most of you have used your entire lives: the WIMP paradigm. WIMP stands for Windows, Icons, Menus, and a Pointing device — the classic GUI. You're looking at it right now on your laptop. These interfaces are graphical user interfaces, GUIs, and they're everywhere. They've been the dominant interaction paradigm for decades.

Why? Because they're extremely good at what they do. The four components — windows as containers, icons as visual shortcuts, menus for actions, and a mouse or trackpad for selection — these map cleanly onto abstract workspaces and documents.

---

**Slide 5–6: Why WIMP Became Prevalent**

WIMP succeeded for a few reasons. First, they're excellent at abstracting workspaces and documents and actions on those documents. Think about it: you have a "file," you put it in a "folder," you "drag" it somewhere. This analogous paradigm — documents as paper, folders as physical folders — makes WIMP intuitive even for novice users. There's almost no learning curve for basic operations.

Second, from the system programmer's perspective, WIMP is elegant. Rectangular regions on a 2D screen are computationally simple. Windows tile, overlap, resize — it's all tractable geometry.

Third, generality. WIMP interfaces handle multitasking well. You can have a browser, a terminal, a text editor, all open in separate windows. That flexibility made them suitable for office work, software development, basically anything involving documents.

---

**Slide 7–8: The Desktop Metaphor**

The WIMP paradigm is typically implemented through what we call the desktop metaphor. This was introduced by Alan Kay at Xerox PARC in 1970. The first computer to really popularize it as a standard feature — replacing the command-line interface — was the Apple Macintosh in 1984.

The idea is simple: the monitor is your desk. You place objects on it — documents, folders. Opening a document spawns a window, which represents a paper copy on your desk. You've got desk accessories: a calculator, a notepad. The whole thing is a skeuomorphic mapping from the physical office to the digital environment.

---

**Slide 9: Post-WIMP**

Now here's the issue. WIMP works beautifully for document-centric tasks, but it breaks down for other use cases. Computer-aided design, for example — manipulating 3D objects with a 2D mouse and dropdown menus is clunky. Interactive games, same problem. Any application requiring more natural, embodied interaction doesn't fit the WIMP model well.

This brings us to Post-WIMP interfaces, a term coined by Van Dam in 1997. Post-WIMP systems aim to overcome these limitations. They're characterized by widgetless interaction — no buttons, no menus, no windows. Think virtual reality, gesture-based interfaces, speech recognition, physical controllers. The key distinguishing feature: Post-WIMP interfaces integrate input from multiple sensory channels and typically produce multimedia output.

---

**Slide 10–14: Post-WIMP Examples**

Let me show you where this is actually deployed.

In automotive, we have systems like Affectiva's Interior Sensing AI — this uses cameras and AI to monitor driver state: drowsiness, attention, emotion. The car responds to the driver's physiological and behavioral state, not just explicit button presses.

In sports science, motion capture systems like Vicon track athletes' movements in 3D space. The interface isn't a GUI — it's the athlete's body. The system captures biomechanical data for performance analysis.

In performing arts, and this is work from our own lab at Casa Paganini, we track musicians' movements — a violinist, for example — to analyze expressive gesture and potentially generate responsive audio-visual content.

In education, you see full-body interaction setups where a student stands in front of a projected display and uses gesture to interact with educational content. No mouse, no keyboard.

In rehabilitation, similar idea — patients interact with therapeutic games using body movement. The system tracks their motion and provides real-time feedback. This is valuable for motor rehabilitation where engaging, gamified interaction improves outcomes.

---

**Slide 15–20: A Simplified Processing Pipeline**

Now let's look at the architecture. Any multimodal system can be understood through a simplified processing pipeline. Let's build it up step by step.

First, you have input devices. These are your sensors: cameras, microphones, depth sensors like Kinect, wearables like smartwatches, IMUs, eye trackers, physiological sensors. These devices capture the user's behavior.

From input devices, you get raw data. Audio waveforms, video frames, accelerometer readings, skeletal joint positions, gaze coordinates — whatever the sensors produce.

Next comes representation. Raw data is too noisy and high-dimensional to work with directly. So we extract features — either hand-crafted features based on domain knowledge, or learned features via neural networks. For motion data, you might compute joint angles, velocities, moments. For audio, spectrograms, MFCCs. The goal is a compact representation that captures the phenomenon you care about.

Then prediction. Given the representation, you want to infer something: a gesture class, an emotional state, an action label, a semantic interpretation. Machine learning is typically applied here — random forests, neural networks, transformers, whatever fits the problem.

After prediction comes mapping. This is where you decide what the system does in response. You have strategies — rules or procedures that map predictions to outputs. You make decisions by applying those strategies to the current input. And you may use metaphors — ways of relating the system's behavior to familiar real-world activities, which helps users form intuitions about how to interact.

Finally, output devices. The system produces feedback: audio, visuals, haptics, lighting, whatever modality is appropriate. This closes the loop back to the user.

When this pipeline involves multiple sensory modalities — say, both audio and video input, or gesture plus speech — you have a multimodal system.

---

**Slide 21–22: Sensory Modalities**

Let's define terms precisely. A sensory modality is a channel through which information is perceived — a communication channel for acquiring or transferring information. Vision, audition, touch, proprioception — these are sensory modalities.

Multimodal, then, refers to the integration of information from multiple sensory channels. Don't confuse this with multimedia, which refers to the output side — presenting content through multiple media like audio and video. Multimodal is about input and integration; multimedia is about output and presentation.

---

**Slide 23–24: Formal Definitions**

Let's get more formal. The W3C Multimodal Interaction Working Group defines multimodal systems as "systems that support a user communicating with an application by using different modalities such as voice, gesture, handwriting, typing, audio-visual speech, etc."

Jaimes and Sebe, in their 2007 survey, put it simply: "A multimodal HCI system is one that responds to inputs in more than one modality or communication channel."

The key word in both definitions is integration. It's not enough to have multiple input channels — the system must combine them meaningfully.

---

**Slide 25–26: Multimodal Systems — Input**

Let's break down the three main components of a multimodal system, starting with input.

The system observes user behavior. Multiple modalities are captured by sensors — microphones for speech, cameras for gesture and facial expression, accelerometers for motion, and so on.

Captured streams undergo processing. First, analysis: for each modality, you extract features characterizing the user's behavior. Then fusion: features from different modalities are integrated. This is a core research challenge — when and how do you combine information from speech and gesture? Early fusion at the feature level? Late fusion at the decision level? Hybrid approaches?

Finally, interpretation: you build semantic representations. What did the user mean? What's their goal? What's their emotional state? For speech, this might involve an ASR system plus semantic parsing. For gesture, a gesture recognizer plus contextual interpretation.

---

**Slide 27–28: Multimodal Systems — Mapping**

The mapping component handles input-to-output correspondence. This is the synchronization behavior — how input in one modality is reflected in output in another, and how inputs are combined across modalities.

Three elements here. Strategies are the rules or procedures that define how to map input to output. Decisions are the actual choices made by applying strategies to real input. And metaphors provide conceptual frameworks — they relate the system's behavior to familiar activities, making interaction intuitive.

---

**Slide 29–30: Multimodal Systems — Output**

Output is the feedback the system provides. This is real-time multimedia feedback based on input analysis, internal models, and the current task.

Forms of output include audio, visuals, haptic feedback, even lighting. The generation process has two phases: synthesis, where you create models describing what the output should be, and rendering, where you actually produce it using software and hardware.

---

**Slide 31–32: The First Multimodal System — Put That There**

Let's close with some history. The first real multimodal system was "Put That There," developed by Richard Bolt at MIT in 1980 and published at SIGGRAPH.

The setup: a user sits in front of a large projected display showing a map. They wear a device that tracks their pointing direction. They can say "put that there" while pointing — first at an object, then at a destination — and the system moves the object. Speech and gesture are combined: the deictic references "that" and "there" are resolved by the pointing gesture.

This was groundbreaking. It demonstrated that combining modalities isn't just additive — speech alone can't specify spatial locations precisely; pointing alone can't specify actions. Together, they enable interactions neither could support alone. This complementarity is the fundamental insight behind multimodal interaction.

---

That's the introduction. Next time we'll go deeper into specific input modalities and the technical challenges of multimodal fusion.


# Lecture: Designing a Multimodal System

---

**Slide 1–2: Title & Is a Multimodal System Needed?**

Alright, let's move on to design. Last time we covered what multimodal systems are — the pipeline, the concepts, the historical context. Today we're asking: how do you actually design one? And before that, an even more fundamental question: should you?

Look at these two images. On one side, a CAVE — a Cave Automatic Virtual Environment — fully immersive, sensors everywhere, projections on every wall. On the other, LibreOffice Calc. A spreadsheet. Both are perfectly valid interfaces for their respective tasks. The point is: multimodality isn't always the answer. Sometimes WIMP is exactly what you need. The question is knowing when.

---

**Slide 3: Motivations for Multimodality**

So let's talk about why you'd go multimodal. Michael Johnston at AT&T Labs compiled a nice summary back in 2006, and it still holds.

First, human-human communication is inherently multimodal. Unimodal communication — just text, just audio — is actually an artifact of communications technology, not the natural state. When we interact with other humans, we use speech, gesture, gaze, facial expression, prosody, all simultaneously.

Second, different modalities are optimal for different content. Rudnicky and Hauptmann in 1992 put it well: certain tasks and functions cry out for particular modalities.

Third, adaptation. Physical environment changes — noise, lighting, mobility constraints. Social environment changes — are you alone, in a meeting, in public? Multimodal systems let users switch modalities or combine them as context demands.

Fourth, migration away from the desktop. Smartphones, PDAs, wall-sized displays, in-car systems — these contexts break the WIMP paradigm.

Fifth, empirical evidence shows multimodal interfaces improve task performance and user preference. We'll look at the studies shortly.

Sixth, error handling. This is crucial. Multimodal systems provide escape hatches — when one modality fails, another can compensate.

---

**Slide 4: Human-Human Communication**

Let's ground this in human communication. When you talk to someone face-to-face, what's happening?

Speech is accompanied by gesture and body movement. This isn't ornamental — gesture carries semantic content. McNeill's work on gesture linguistics shows that gesture and speech form an integrated system; they're co-expressive.

Gaze and eye contact regulate turn-taking, signal attention, establish social connection. Without gaze cues, conversation becomes awkward — think about phone calls versus video calls.

Facial expression conveys affect, emphasis, irony, skepticism — things that are hard or impossible to encode in words alone.

And prosody — the rhythm, stress, and intonation of speech — is a crucial component. The same sentence with different prosody can mean completely different things. "That's great" can be enthusiastic or sarcastic depending entirely on how you say it.

The takeaway: if you want human-computer interaction to feel natural, you need to account for these channels.

---

**Slide 5–7: The Famous Interview Example**

Let me show you a concrete example of why multimodality matters. This is the transcript of a BBC interview from 2006:

The interviewer says: "Guy Kewney is the editor of the technology website News Wireless. Hello, good morning to you."

The guest responds: "Good morning."

"Were you surprised by this verdict today?"

"I'm very surprised to see this verdict to come on me. Because I was not expecting that. When I came, they told me something else and I'm coming. And they told me something else. Big surprise any way."

Reading the transcript, you might think: okay, this person seems confused, maybe English isn't their first language, but the content is plausible.

Now watch the video. What you see is Guy Goma — not Guy Kewney — a job applicant who was waiting in the lobby when a producer grabbed him and put him on live television, mistaking him for the tech journalist. His face shows pure panic. His body language screams "I have no idea what's happening." The prosody is hesitant, bewildered.

The transcript alone is interpretable. The audio adds confusion. The video makes it obvious something is catastrophically wrong. That's multimodal information integration — each channel adds signal that disambiguates the others.

---

**Slide 8: Most Effective Means**

Here's a fundamental design principle: match modality to content type.

Consider drawing the borders of a region on a map. Can you do this with speech? Technically yes — you could say "start at coordinates 45.7, 12.3, then proceed northeast to 45.9, 12.5, then..." But that's absurd. Spatial information is naturally expressed through spatial modalities — pointing, drawing, gesturing.

Conversely, some information is better suited to verbal communication. If you want to refer to "the left bank of the river," language handles that effortlessly. Trying to convey that purely through pointing would be ambiguous — you'd need to somehow indicate the concept of "left bank" as opposed to just "over there."

This is the complementarity principle: modalities have different expressive affordances. Good multimodal design exploits this, assigning each type of content to the modality that handles it most naturally.

---

**Slide 9: Adapting to the Environment**

Multimodal systems enable dynamic adaptation. This is a major advantage over unimodal interfaces.

Physical environment adaptation: you're in a noisy factory, so speech recognition becomes unreliable — switch to gesture or touch. You're in a dark room, so visual displays are useless — switch to audio or haptic feedback. You're driving, so your hands and eyes are occupied — switch to speech.

Social environment adaptation: you're at home alone, so you can talk freely to your device. You're in a quiet office, so you switch to typing to avoid disturbing colleagues. You're in a meeting, so you use subtle gestures rather than overt commands.

The key insight is that context changes dynamically, and a multimodal system can adapt. A unimodal system forces you to use the same modality regardless of whether it's appropriate.

---

**Slide 10: Performance and Preference**

Now let's look at the empirical evidence. Multiple studies show multimodal interfaces outperform unimodal ones:

Oviatt 1996: For map-based tasks, multimodal input (speech plus pen) outperformed unimodal speech. Users were faster and made fewer errors.

Cohen et al. 1998: Multimodal interaction was faster than GUI-only for map tasks. The efficiency gain came from being able to specify locations by pointing while specifying actions by speech.

Nishimoto et al. 1995: For drawing applications, speech plus mouse plus keyboard was faster than GUI alone. Users could issue commands verbally while their hands stayed on drawing tasks.

Hauptmann 1989: Users showed clear preference for speech and gesture in object manipulation tasks, even when GUI alternatives were available.

The consistent finding: when users have the option, they prefer multimodal interaction, and their performance improves. This isn't just about speed — it's about cognitive load distribution and natural expression.

---

**Slide 11: Error Handling**

One of the most important benefits of multimodal systems is error resilience. There are three mechanisms here:

Mode switching: When one modality starts producing errors, users can switch to another. This breaks error spirals — those frustrating loops where you keep trying the same thing and it keeps failing. In a speech-only system, if recognition fails, you're stuck. In a multimodal system, you can fall back to typing.

Cross-modal compensation: Information from one modality can compensate for errors in another. The example on the slide mentions Tangible Acoustic Interfaces, where visual information compensates for imprecise audio-based localization. More generally, if your speech recognition produces an ambiguous result, gesture or gaze data might disambiguate it.

Multimodal confirmation: One modality can confirm results from another. If the system interprets your speech as "delete file X," it can use a visual confirmation dialog. Or the user might naturally provide redundant input — saying "this one" while pointing — which confirms intent.

These mechanisms make multimodal systems more robust than the sum of their parts.

---

**Slide 12: Design Tools**

So how do you actually design a multimodal system? The design process typically follows standard HCI procedures — iterative design, user-centered methods, prototyping, evaluation. But we have domain-specific conceptual tools:

Design guidelines provide principles for good multimodal interface design.

Models for modality choice help predict how users will select modalities in different contexts.

Models for multimodal combination describe how modalities can be combined — sequentially, in parallel, redundantly, complementarily.

Frameworks for multimodal systems provide architectural blueprints showing components and their interactions.

Let's go through each of these.

---

**Slide 13–14: Guidelines (Reeves et al., 2004)**

Reeves and colleagues compiled guidelines for multimodal UI design in a 2004 CACM article. Let me walk through the key ones:

Be consistent. This applies to output format, presentation style, prompts, shortcuts, state transitions. Users build mental models; inconsistency breaks those models.

Provide good error prevention and handling. Make functionality discoverable. Users shouldn't have to guess what's possible.

Design for the broadest range of users and contexts. Support the best modality or combination for anticipated environments. An in-car system has different constraints than an office system.

Address privacy and security. Speech input in public contexts raises obvious issues — provide non-speech alternatives. Biometric modalities like face recognition have security implications.

Maximize human cognitive and physical abilities. Understand information processing limitations. Don't overload working memory. Don't require precise motor control when users are mobile.

Integrate modalities compatibly with user preferences, context, and functionality. Match output style to acceptable input style — if you accept unconstrained natural language, your responses should be conversational; if you use constrained grammars, responses can be more structured.

Adapt to different users and contexts. Capture individual differences in user profiles. Some users prefer speech; others prefer touch. Let the system learn and adapt.

---

**Slide 15–16: How Do Users Choose Modalities?**

Now let's talk about modality choice from the user's perspective.

A modality combination is the set of input modalities a user adopts for a specific task. Sometimes this is a single modality; sometimes it's several.

Modality choice is the selection of a modality combination for a given context. Critically, this selection can be conscious or unconscious. Users don't always deliberately think "I'll use speech for this" — they just do it.

The table on slide 16 shows sensory modalities at the neurophysiological level — chemical (internal and external), somatic (touch, pressure, temperature, pain), muscle senses and kinesthesia, balance, hearing, vision. Each has different receptor organs and cell types. This reminds us that "modality" is grounded in physical sensory channels, not just interface widgets.

---

**Slide 17: Modalities in Interactive Systems**

Blattner and Glinert in 1996 catalogued modalities specifically for multimodal interactive systems:

Visual modality includes: face location, gaze direction, facial expression, lipreading, face-based identity (plus demographics like age, sex, race), gesture (of head, face, hands, body), and sign language.

Auditory modality includes: speech input and non-speech audio (environmental sounds, music, etc.).

Touch modality includes: pressure, location and selection, gesture via touchscreens or touch surfaces.

Other sensors: sensor-based motion capture via IMUs, marker systems, etc.

---

**Slide 18: The Pokémon Go Case**

Pokémon Go is a nice real-world example of multimodal design. Think about the modalities involved:

Touch for UI interaction — tapping, swiping, throwing Pokéballs.

Location via GPS — the game knows where you are in the physical world.

Camera for AR overlay — you see Pokémon superimposed on the real environment.

Motion sensors for distance tracking — the game tracks how far you've walked.

Audio for notifications and ambient sound.

The combination creates an experience that no single modality could provide. Location alone wouldn't be engaging. Camera alone would be static. Touch alone would be a standard mobile game. It's the integration that makes it compelling.

---

**Slide 19: Variables for Modality Choice**

Jameson and Kristensson in 2017 developed a comprehensive model of variables affecting modality choice. The diagram shows four categories:

Features of the situation: nature of the task, content to be conveyed, evaluation criteria, demands of competing tasks, cognitive resource constraints, social and physical environment, presence of others, ambient noise, system properties like recognition accuracy.

Which combination of modalities: this is the decision being made.

User characteristics: skills and abilities with specific modalities, perceptual and motor capabilities, evaluation criteria for interaction, preference for novelty, cognitive load tolerance, global attributes like age and gender.

Consequences for interaction: objective performance aspects (speed, accuracy, error rates, physical injury risk), impact on other tasks, subjective responses (enjoyment, familiarity, embarrassment, stress).

Important caveat from Jameson and Kristensson: "The diagram is by no means intended to suggest that users actually make their choices by considering so many variables." Users don't consciously weigh all these factors — they make quick, often automatic decisions. But these are the variables that influence those decisions.

---

**Slide 20–21: ASPECT and ARCADE Models**

ASPECT and ARCADE are two complementary models from decision-making research, applied here to modality choice.

ASPECT describes the variety of ways people make choices. ARCADE describes ways to help people make better choices.

Both are grounded in psychological research on human decision-making, not specific to HCI, but directly applicable to modality selection.

---

**Slide 22–25: The ASPECT Model**

ASPECT specifies six patterns that users can apply when making choices:

Attribute-based choice: The options can be described in terms of attributes. The user evaluates attributes, filters the option set based on attribute values, and chooses from a manageable set. Example: "I'll use speech because it's hands-free" — the attribute is hands-free operation.

Consequence-based choice: The choices have consequences. The user identifies possible actions, anticipates consequences, evaluates them, and chooses based on expected outcomes. Example: "If I speak in this meeting, others will hear my command, so I'll type instead."

Experience-based choice: The user has made similar choices before. They apply recognition-primed decision making — a pattern match to past experience, habit, or affect heuristic. Example: "I always use speech for navigation; I'll do that here too."

Socially-based choice: The user considers what others do, expect, or recommend. They look at examples, social expectations, or explicit advice. Example: "Everyone in this office uses gesture for the smart display, so I will too."

Policy-based choice: The user has a policy for this type of choice. They recognize the choice situation, identify the applicable policy, and apply it. Example: "In public contexts, I never use speech."

Trial-and-error-based choice: The choice will be made repeatedly or can be revised. The user tries an option, observes consequences, learns, and adjusts. Example: "I'll try pen input; if it doesn't work well, I'll switch to speech."

Users don't apply just one pattern — they combine them, sometimes unconsciously.

---

**Slide 26–30: The ARCADE Model**

ARCADE provides six strategies for supporting users' modality choices:

Access information and experience: Give users relevant information about the system and help them anticipate what interaction will feel like. Example: Provide tutorial videos demonstrating each available modality. Show users the text of audio messages that will be played (relevant for privacy awareness).

Represent the choice situation: Make users aware that modality choice exists, or subtly encourage appropriate modalities. Example: Shaping a tablet controller like a stylus suggests stylus use; shaping it differently invites exploration. If the system speaks first, the user naturally responds with speech.

Combine and compute: Provide high-level relevant information without computational details. Example: A colored LED indicates audio quality is good; the user doesn't need to know the bit rate or signal-to-noise ratio.

Advise about processing: Suggest a procedure for choosing modalities. Example: "Consider your past experience with similar situations rather than just following what friends do."

Design the domain: Hide modalities that aren't appropriate for this user or context. Example: For a visually impaired user, don't present visual-only modality options even though they technically exist.

Evaluate on behalf of the chooser: The system autonomously determines appropriate modalities. Example: The system detects visual impairment and automatically switches to auditory feedback.

---

**Slide 31–32: How Do Systems Use Modalities?**

Now let's shift from user choice to system design. How do systems combine modalities?

Interaction and combination of modalities follow complex patterns. A pointing gesture might involve gaze, arm movement, facial expression, and speech — each with different temporal dynamics.

The CASE design space, introduced by Nigay and Coutaz in 1993, provides vocabulary for this. CASE stands for Concurrent, Alternate, Synergistic, or Exclusive.

CASE defines three dimensions:

Level of abstraction: Data is represented at different levels — raw signals, features, semantic interpretations.

Use of modalities: How modalities are available over time — sequentially or in parallel.

Fusion: How data from different modalities is combined — independently or jointly.

---

**Slide 33–34: The CASE Design Space**

The CASE space has eight cells based on three binary dimensions.

Use of modalities: Sequential vs. Parallel. Are modalities used one at a time, or simultaneously?

Fusion: Independent vs. Combined. Is each modality processed separately, or are they fused?

Level of abstraction: Meaning vs. No Meaning. Are we working with raw data or semantic interpretations?

The four named quadrants at the semantic level:

Exclusive: Sequential use, independent processing. Only one modality at a time, no fusion. Like a system that accepts either speech or touch, but not both together.

Alternate: Sequential use, combined processing. Modalities used one at a time, but their results are fused. Like "Put That There" — speech then gesture, fused to interpret the command.

Concurrent: Parallel use, independent processing. Multiple modalities simultaneously, each processed separately. Like a system monitoring speech and facial expression independently.

Synergistic: Parallel use, combined processing. Multiple modalities simultaneously, fused together. Like audiovisual speech recognition, where lip movement and audio are jointly decoded.

CASE's advantages: It makes explicit how a system handles modalities. The classification is precise. It's complete for the machine side — how the system interprets multimodal input. It gives developers clear guidance on processing architectures.

---

**Slide 35–41: The CARE Properties**

CARE is another foundational model, from Coutaz et al. 1995 and extended by Serrano and Nigay 2009. CARE stands for Complementarity, Assignment, Redundancy, and Equivalence.

Unlike CASE, which describes system architectures, CARE characterizes the properties of multimodal dialogue — the relationship between modalities, goals, and state transitions.

The formal framework uses these concepts:

State s: A set of properties at a particular time.

Agent: An entity capable of performing actions (user or system).

Goal g: A state the agent intends to reach.

Modality m: A method to reach a goal.

Temporal relationship TR: How modalities relate in time.

Temporal window TW: The time interval under consideration.

Temporality follows Allen's interval logic: anachronism, sequence, concomitance, coincidence, parallelism.

Now the four CARE properties:

Complementarity: Modalities are complementary if all must be used together to reach the goal — no single modality suffices alone. The "Put That There" example is complementarity: neither speech ("put that there") nor gesture (pointing) alone conveys the complete command. They must be combined.

Assignment: A modality is assigned if it's the only way to reach the goal — no choice exists. Assignment expresses absence of choice. For a specific function, you must use the assigned modality.

Redundancy: Modalities are redundant if they're equivalent and all are used in the same temporal window. The user produces repetitive behavior without increasing expressive power. Example: saying "zoom out" while also using a pinch gesture — both convey the same command.

Equivalence: Modalities are equivalent if any one of them suffices to reach the goal. Equivalence expresses availability of choice with no temporal constraint. You can say "Pittsburgh" or type "Pittsburgh" or select it from a menu — any one works.

Some notes on CARE:

Complementarity can have a dominant modality that requires others. In "put that there," speech dominates structurally.

Equivalence and assignment are about choice availability at a point in dialogue.

Temporal relationships matter: sequential versus parallel use have different implications for usability and implementation.

---

**Slide 42: The U-CARE Properties**

U-CARE is the user-side counterpart to CARE. Users have preferences based on knowledge, background, physical abilities, and behavioral tendencies.

The key insight: CARE properties (what the system supports) must match U-CARE properties (what the user prefers). For each system CARE property, at least one supported modality must match user preferences.

U-assignment: User can only use modality Ua.
U-equivalence: User can use any modality from set Ue.
U-redundancy: User tends to use multiple modalities redundantly.
U-complementarity: User naturally combines modalities.

If there's mismatch — the system assigns modality Sa but the user can only use Ua, and Sa ≠ Ua — interaction fails.

---

**Slide 43: CARE and U-CARE Compatibility**

Let me give you compatibility conditions:

Example 1: User has U-assignment to modality Ua (they can only use Ua).

If system has Assignment to Sa: compatible iff Sa = Ua.
If system has Equivalence or Redundancy over set Se: compatible iff Ua ∈ Se.

Example 2: User has U-equivalence over set Ue (they can use any modality in Ue).

If system has Assignment to Sa: compatible iff Sa ∈ Ue.
If system has Equivalence over Se: compatible iff Ue ∩ Se ≠ ∅.

The design implication: know your users' capabilities and preferences, then ensure system CARE properties are compatible.

---

**Slide 44: Frameworks**

Now let's talk about frameworks — higher-level architectural blueprints for multimodal systems.

Frameworks are not architectures. Architectures include implementation details — hardware allocation, communication protocols, deployment configurations. Frameworks are one level of abstraction above: they show components and their connections without implementation specifics.

Frameworks exist for both verbal communication systems (speech, dialogue) and non-verbal communication systems (gesture, movement, facial expression).

---

**Slide 45–52: The W3C Framework**

The W3C Multimodal Interaction Working Group developed a standard framework in 2003. It identifies major components and markup languages for data exchange.

The main components:

Input component: Handles capture, recognition, interpretation, and integration of input modalities. The diagram shows speech going through a speech recognizer (using SRGS grammars) and semantic interpretation, producing EMMA (Extensible MultiModal Annotation) markup. Similarly for handwriting, keyboard, and other inputs. Integration combines EMMA from multiple modalities.

EMMA is worth understanding — it's an XML-based markup for representing multimodal input. It can express multiple interpretations with confidence scores, timestamps, modality sources. For example, a speech utterance "flights from boston to denver" might have two interpretations: Boston→Denver (0.75 confidence) or Austin→Denver (0.68 confidence). EMMA captures both with their scores.

Output component: Handles generation, styling, and rendering. Voice output uses SSML (Speech Synthesis Markup Language). Graphics use SVG, XHTML. Audio plays directly. The generation component decides what to output; styling determines presentation; renderers produce actual audio/visual/haptic output.

Interaction manager: The central coordinator. It maintains interaction state and context, responds to inputs, manages execution flow, coordinates across component interfaces. In some architectures it's monolithic; in others it's distributed.

Application functions: Interface to domain applications. The multimodal system is typically a front-end to some backend service or application.

Session component: State management for temporary and persistent sessions. Essential when: the application runs on multiple devices, the application is inherently session-based (like a game), or the application provides multiple input/output modes.

System and Environment component: Tracks device capabilities, user preferences, environmental conditions. Which modes does the user want? Has audio been muted? What's the ambient noise level? This enables adaptive behavior.

The slides show two examples:

Example 1: In-car navigation. User initiates via steering wheel button, then uses speech plus touchscreen plus GPS input. Output through display plus voice. The framework shows how these modalities flow through recognition, interpretation, integration, and output generation.

Example 2: Voice-initiated phone call. User speaks a name, system uses visual and spoken dialogue to narrow selection, can exchange multimedia messages if the callee is unavailable. Shows how the framework handles multi-turn dialogue with modality switching.

---

**Slide 53–65: A Layered Framework**

Now let's look at a framework designed specifically for non-verbal communication — movement, gesture, expressive behavior. This comes from Camurri and colleagues at Casa Paganini.

The framework has a layered architecture, inspired by the OSI model. Both input and output are organized as stacks:

Physical signals: Raw sensor data. Video frames, audio samples, accelerometer readings, motion capture coordinates.

Low-level features: Extracted signal characteristics. For movement: velocity, acceleration, trajectory curvature. For audio: pitch, intensity, spectral features. These are computed directly from physical signals.

Mid-level features: Higher-order descriptors. For movement: energy, contraction index, symmetry, impulsiveness, directness. These capture expressive qualities, not just raw kinematics.

Concepts and structures: Semantic interpretations. Emotion, activity type, movement quality (in Laban terms), gesture categories.

The input stack processes upward: sensors capture physical signals, feature extraction produces low-level features, analysis produces mid-level features, interpretation yields concepts and structures.

Mapping strategies connect input stack to output stack. These can be:

Direct mapping: Input physical signals directly drive output physical signals — low latency, no interpretation.

Dialogical interaction: Higher-level interpretation drives higher-level output planning — like a conversation.

The output stack processes downward: concepts and structures determine what to express, mid-level features specify expressive qualities, low-level features are synthesized, physical signals are rendered via actuators.

For example, on the output side:

Concepts and structures: The system decides to express "happiness" or "energetic greeting."

Mid-level features: This maps to high energy, expanded posture, quick movements.

Low-level features: These become specific joint angle trajectories, velocities.

Physical signals: The avatar or robot actually moves.

The framework shows this can apply to any expressive channel: movement, facial expression, speech prosody, music, lighting.

An updated input pipeline shows more detail: motion capture produces time series of poses, feature extraction produces time series of features, unitizing segments the stream into meaningful chunks, functionals compute summary statistics over chunks, and analysis/interpretation/annotation produces final representations.

---

**Slide 66–71: Ten Myths of Multimodal Interaction**

Let me close with Sharon Oviatt's influential 1999 paper on ten myths of multimodal interaction. These are common assumptions that empirical research has shown to be false.

**Myth 1: If you build a multimodal system, users will interact multimodally.**

Reality: Users intermix unimodal and multimodal interactions. They don't always use multiple modalities; they use what's appropriate for the moment. Design must support both.

**Myth 2: Speech and pointing is the dominant multimodal integration pattern.**

Reality: This was the pattern in "Put That There," but modalities like handwriting, manual gesturing, and facial expression can generate symbolic information richer than simple pointing. Don't over-design for one pattern.

**Myth 3: Multimodal input involves simultaneous signals.**

Reality: Multimodal signals often don't co-occur temporally. Sequential use is common — speech then gesture, gesture then speech. Your fusion architecture must handle asynchrony.

**Myth 4: Speech is the primary input mode in any multimodal system that includes it.**

Reality: Speech isn't always dominant in content or timing. Other modalities can carry information absent from speech — spatial information, manner of action. And speech doesn't necessarily come first temporally.

**Myth 5: Multimodal language doesn't differ linguistically from unimodal language.**

Reality: When users interact multimodally, their speech becomes briefer, syntactically simpler, and less disfluent. They offload information to other modalities. Your speech recognizer should expect different input than in speech-only systems.

**Myth 6: Multimodal integration involves redundancy of content between modes.**

Reality: Complementarity may be more significant than redundancy. Modalities often carry different information that must be combined, not the same information repeated.

**Myth 7: Individual error-prone recognition technologies combine to produce even greater unreliability.**

Reality: In well-designed multimodal interfaces, users adaptively choose modalities to maximize success. Mutual disambiguation between modalities can increase robustness beyond any single modality.

**Myth 8: All users integrate multimodal commands uniformly.**

Reality: Individual differences exist in integration patterns. Some users integrate tightly; others use modalities more sequentially. Systems should adapt to the user's dominant pattern.

**Myth 9: Different input modes can transmit comparable content.**

Reality: Modalities differ in what they can express, their communicative function, how they integrate with other modes, and their fit for different interface styles. They're not interchangeable.

**Myth 10: Enhanced efficiency is the main advantage of multimodal systems.**

Reality: The main advantages may be elsewhere: decreased errors, increased flexibility, increased user satisfaction. Don't evaluate only on speed.

Oviatt's conclusion: designing multimodal systems that blend modes synergistically requires intimate knowledge of modality properties, what's unique about multimodal language, and how multimodal input is integrated and synchronized.

---

That's the design lecture. We've covered motivations for going multimodal, guidelines for design, models of how users choose modalities (ASPECT, ARCADE), models of how systems use modalities (CASE, CARE), and frameworks for building multimodal systems (W3C, layered). And we've debunked common misconceptions.

Next time we'll dive into specific technical components — fusion architectures, recognition technologies, and implementation patterns.

# Body Movement and Gesture - Part 1
## Comprehensive Lecture Notes (Slides 1-40)

---

## INTRODUCTION AND CONCEPTUAL FRAMEWORK
### [Slides 1-2]

Welcome to the third major component of our multimodal systems course: body movement and gesture. If you recall, we've already covered speech and music signals. Now we're turning our attention to how we capture and analyze human movement - a fundamentally different kind of signal that carries rich information about intent, emotion, and communication.

Let me remind you of our conceptual framework. We're working with a four-layer hierarchy:

At the bottom, we have **Physical Signals** - the raw data coming from our sensors. This could be voltage readings from accelerometers, pixel values from cameras, or depth measurements from range sensors.

Moving up, we have **Low-Level Features** - these are computed directly from the physical signals. Think of velocity, acceleration, or simple motion descriptors.

Then we have **Mid-Level Features** - these are more abstract representations derived from the low-level features. This might include things like motion energy, fluidity of movement, or contraction/expansion patterns.

Finally, at the top, we have **Concepts and Structures** - high-level interpretations like gesture types, emotional states, or activity recognition.

Today and in our next session, we'll focus primarily on the first two layers: how we capture movement and how we extract basic features from it. This foundation is essential because the quality of everything that follows depends on getting these fundamentals right.

---

## DEVICES FOR CAPTURING MOVEMENT
### [Slides 3-4]

Let's begin with the hardware - the devices we use to capture human movement. There's quite a variety, and each has its strengths and limitations.

**Inertial Measurement Units (IMUs)** are electronic devices that measure a body's specific force, angular rate, and sometimes orientation. They're the workhorses of wearable motion sensing.

An IMU typically combines three types of sensors:

**Accelerometers** measure linear acceleration along three perpendicular axes (X, Y, Z). They work using MEMS (Micro-Electromechanical Systems) technology - tiny silicon structures, typically ranging from micrometers to millimeters in size. Inside, there's a small proof mass suspended by springs. When the device accelerates, this mass moves relative to the sensor housing, creating a measurable electrical signal proportional to the acceleration. Even when stationary, accelerometers detect gravitational acceleration, which is useful for determining orientation relative to the ground.

**Gyroscopes** measure angular velocity - how fast something is rotating around each axis. MEMS gyroscopes use the Coriolis effect: when an object moves in a rotating reference frame, it appears to experience a deflection perpendicular to its direction of motion. The gyroscope contains vibrating structures that, when the device rotates, experience this apparent deflection. This is converted into an electrical signal proportional to the rotation rate.

**Magnetometers** measure magnetic field strength and direction, essentially acting as electronic compasses. They provide an absolute reference for heading, compensating for the drift that accumulates when integrating gyroscope data over time.

The key advantage of IMUs is portability - they can be worn on the body or embedded in everyday objects like smartphones. Most modern smartphones contain IMUs, which is why your phone can track steps, detect screen orientation, and enable motion-controlled games.

However, IMUs have limitations. They measure acceleration and rotation rate, not position directly. To get position, you must integrate acceleration twice - first to get velocity, then to get position. Unfortunately, any small bias errors in the acceleration measurement accumulate rapidly during integration. This is why pure IMU-based positioning becomes inaccurate over time without correction from other sensors like GPS.

---

### [Slides 5-6: IMU Examples]

Let me show you some practical applications of IMUs.

In the EU-H2020 DANCE project, dancers wear two IMUs on their wrists. The captured acceleration data drives interactive sonification - the movement data is mapped to sound parameters in real-time, creating a kind of audio mirror of the dancer's motion. The captured data here is quite simple: just the three-dimensional components of acceleration from each sensor. Despite this simplicity, it's enough to create compelling interactive experiences.

Another example shows acceleration data captured from a smartphone using the EyesWeb platform. You can see a graph showing the acceleration magnitude over time - computed as the Euclidean norm of the three acceleration components. Notice the oscillating pattern - this is characteristic of rhythmic movement. The peaks correspond to moments of rapid motion change, while the valleys represent smoother portions of the movement.

---

### [Slides 7-8: Magneto-Inertial Motion Capture Suites]

When we need full-body tracking rather than just monitoring one or two body parts, we use **magneto-inertial motion capture suites**. These systems employ multiple IMUs - typically 15 to 20 sensors - distributed across the body.

The MVN Link system by Movella is a prime example. The suit includes sensors on each major body segment: head, upper arms, forearms, hands, torso, pelvis, upper legs, lower legs, and feet. Some configurations also include finger-tracking gloves and insole pressure sensors for detailed hand and foot data.

These systems can output data at two levels:

1. **Raw sensor measurements**: The actual acceleration and angular velocity readings from each IMU
2. **Reconstructed 3D skeletons**: Using sophisticated sensor fusion algorithms and biomechanical models, the system computes the 3D position and orientation of each body segment

The reconstructed output typically includes joint positions and rotations. Rotations are usually represented as quaternions - four-dimensional numbers that elegantly encode 3D rotations without the problematic singularities (called "gimbal lock") that plague Euler angle representations. We'll discuss quaternions more when we talk about data representations.

The GAME-ON dataset from Maman and colleagues (2020) demonstrates this technology in a research context. It captures group interactions during collaborative tasks, providing both 3D positions and rotations for multiple participants simultaneously. This kind of data is invaluable for studying social dynamics and group cohesion through movement.

---

### [Slides 9-13: Optical Motion Capture Systems]

For the highest precision, we turn to **optical motion capture systems** - the technology you've probably seen used in Hollywood film production and video game development.

Systems like Vicon and OptiTrack use multiple infrared cameras - anywhere from 8 to 54 or more - arranged around the capture volume. The subject wears small retroreflective markers - typically spherical, coated with a material that efficiently bounces light back toward its source.

Here's how it works: Each camera has a ring of infrared LEDs around its lens. These LEDs flood the capture space with infrared light. The markers reflect this light back to the cameras' sensors. By capturing images from multiple cameras simultaneously, the system triangulates the 3D position of each marker.

The precision is remarkable - sub-millimeter accuracy, typically 0.3 to 2mm depending on the setup. This makes optical motion capture the gold standard for applications requiring precise measurement: biomechanics research, clinical gait analysis, and high-end animation production.

Looking at the examples from OptiTrack, you can see the progression from raw data to visualization:
- First, we see just the markers - clouds of points in 3D space
- Next, the system connects these points into a skeleton representation
- Finally, we can map a full body model onto the skeleton for realistic visualization

The EU-H2020 DANCE project also used optical motion capture for interactive sonification research, capturing both 3D positions and the orientation of rigid bodies (clusters of markers that move together as a unit).

However, optical systems have significant drawbacks: they're expensive (professional systems cost $50,000 to $500,000 or more), require careful calibration, need controlled lighting conditions, and the capture volume is fixed. Subjects must wear markers, which can be problematic for some applications. And occlusion - when markers are hidden from camera view - requires sophisticated algorithms to handle.

---

### [Slides 14-17: Range Imaging Devices]

**Range imaging devices** offer a middle ground - markerless capture at a much lower cost than optical systems, though with reduced precision.

You're probably familiar with the Microsoft Kinect, which popularized this technology for consumer applications. Intel RealSense and Stereolab ZED are other examples. These devices produce depth images - pictures where each pixel value represents distance from the camera rather than color or brightness.

Several techniques exist for range imaging:
- **Stereo triangulation**: Using two cameras like human eyes
- **Structured light**: Projecting known patterns and analyzing their deformation
- **Time-of-flight (ToF)**: Measuring how long light takes to return from objects

**Time-of-Flight** is particularly important and worth understanding in detail. A ToF camera emits modulated infrared light - typically at wavelengths around 850nm or 940nm. This light bounces off objects in the scene and returns to the sensor. By measuring the phase shift between the emitted and reflected signals, the camera calculates distance.

There are two main variants:

**Direct ToF (dToF)** sends out short pulses and directly times how long they take to return. The formula is straightforward:
```
Depth = (c × Δt) / 2
```
where c is the speed of light and Δt is the round-trip time. The division by 2 accounts for the light traveling to the object and back.

**Indirect ToF (iToF)** - more common in consumer devices - uses continuously modulated light. Instead of timing individual pulses, it measures the phase shift of the returning wave. The Kinect v2 and many smartphone depth sensors use this approach.

The formula shown in the slides:
```
Depth = (c × Δφ) / (2 × 2πf)
```
relates depth to the phase shift Δφ at modulation frequency f.

ToF cameras work well in low light, provide instant depth maps, and have a compact form factor. However, they struggle with highly reflective or transparent surfaces and can be degraded by strong sunlight. The choice of wavelength matters here - 940nm is preferred for some applications because there's less solar interference at that wavelength due to atmospheric absorption.

The Kinect v1 output example shows three representations: a raw depth image (darker means closer), a binary blob detection result, and an extracted skeleton with joint positions.

---

### [Slide 18: Video Cameras]

Finally, we have ordinary **video cameras** - from expensive professional broadcast cameras to cheap webcams. These capture only 2D images and have no inherent depth sensing.

The advantage is ubiquity and cost - cameras are everywhere. The challenge is that extracting movement data requires computer vision techniques. We're essentially trying to recover 3D information from 2D projections, which is an inherently ill-posed problem that requires additional assumptions or multiple views.

---

## MOVEMENT DATA FROM VIDEO
### [Slides 19-20]

Now let's talk about how we extract movement data from ordinary video. There are two main approaches:

**Approach 1: Background Segmentation + Motion Tracking**
This is a two-step process:
1. First, detect what's moving by separating foreground from background
2. Then, track those moving objects across frames

The output is blob coordinates - the positions of detected foreground regions over time.

**Approach 2: Bottom-Up Pose Estimation**
Modern deep learning approaches like OpenPose can directly estimate body landmark positions from images, without needing to segment the foreground first.

Let's examine both approaches in detail.

---

## BACKGROUND SEGMENTATION
### [Slides 21-28]

**Background segmentation** refers to a collection of techniques for detecting moving objects in a video stream by separating them from the static background.

The underlying assumption is that moving objects - people, vehicles, etc. - are what we're interested in for interaction purposes. If we can identify which pixels belong to moving objects versus the background, we can focus our subsequent processing on the relevant regions.

The basic approach is:
1. Build (and possibly update) a model of what the background looks like
2. Compare each incoming frame to this model
3. Label pixels as foreground (different from model) or background (similar to model)

### Simple Background Subtraction

The simplest approach captures an image of the empty background and stores it. For each subsequent frame, we:
1. Compute the absolute pixel-wise difference between the current frame and the stored background
2. Apply a threshold - pixels with difference above the threshold are foreground
3. The result is a binary image: white for foreground, black for background

In pseudocode:
```
B = I(0);          // Capture empty background
loop time t
    I(t) = next frame;
    diff(t) = abs(B - I(t));
    M(t) = threshold(diff(t), λ);
end
```

**Pros:**
- Easy to implement
- Computationally inexpensive
- Works reasonably well in controlled conditions

**Cons:**
- Requires foreground colors to be sufficiently different from background
- Objects that enter and stop are still detected (they never match the original background)
- If part of the background moves (like a door opening), both the moving part and the revealed area behind it are detected as foreground - creating "ghost" artifacts

### Frame Differencing

An alternative approach uses the **previous frame** as the background model, rather than a fixed reference:

```
B(0) = I(0);
loop time t
    I(t) = next frame;
    diff(t) = abs(B(t) - I(t));
    M(t) = threshold(diff(t), λ);
    B(t) = I(t);     // Update background to current frame
end
```

**Pros:**
- Quickly adapts to scene changes
- No ghosts from stopped objects

**Cons:**
- Only detects edges of uniformly colored objects (the interior doesn't change between frames)
- Objects that stop moving immediately disappear
- Difficult to detect objects moving directly toward or away from the camera (minimal lateral motion means minimal difference)

---

### [Slide 29: Statistical Background Modeling]

The limitations of simple methods led to more sophisticated **statistical approaches**. The key insight is that a single pixel's value isn't constant even in a static background - it fluctuates due to noise, lighting changes, and small motions like leaves rustling.

**Gaussian Mixture Models (GMM)**, introduced by Stauffer and Grimson in 1999, model each pixel's history as a mixture of K Gaussian distributions (typically K=3 to 5). Each Gaussian has:
- A **weight** (ω): How often this appearance occurs
- A **mean** (μ): The typical color value
- A **variance** (σ²): How much this value fluctuates

The algorithm works as follows:

**Classification:** For each new pixel value, check if it matches any existing Gaussian (within 2.5 standard deviations of any mean). If it matches a high-weight Gaussian, classify as background; otherwise, classify as foreground.

**Update:** Matched Gaussians have their parameters updated toward the new observation. Unmatched observations may create new Gaussians, potentially replacing the lowest-weight existing one.

Over time, stable background appearances develop high weights, while transient foreground objects only briefly influence the model before fading away.

This approach handles many challenging scenarios:
- Gradual illumination changes (the model adapts)
- Dynamic backgrounds like waving trees (multiple Gaussians capture the variations)
- Shadows (can be modeled as a separate mode)

GMM-based background subtraction remains widely used in surveillance systems and is implemented in OpenCV's BackgroundSubtractorMOG and BackgroundSubtractorMOG2 classes.

---

### [Slides 30-43: SAM2 - Segment Anything Model 2]

Moving to cutting-edge technology, **SAM2 (Segment Anything Model 2)** from Meta represents a paradigm shift in video segmentation. Released in 2024, it extends the original SAM model to handle video sequences.

SAM2 focuses on the **Promptable Visual Segmentation (PVS)** task:
- **Input**: A video plus prompts (points, boxes, or masks) on any frame
- **Output**: The predicted spatio-temporal mask (called a "masklet") tracking the object throughout the video
- **Refinement**: The masklet can be iteratively improved by providing additional prompts on other frames

This is revolutionary because you can click on an object once, and SAM2 will track it through the entire video, handling occlusions, appearance changes, and complex motions.

### SAM2 Architecture

The architecture has several key components:

**Image Encoder**: Processes frames one at a time using Hiera, a hierarchical vision transformer, combined with a feature pyramid network. This produces rich multi-scale feature representations of each frame.

**Memory Attention**: The current frame's features are cross-attended to memories of the target object from previous frames. This is implemented with 4 self-attention and cross-attention layers. This memory mechanism is crucial - it's what allows SAM2 to maintain consistent tracking even when objects change appearance or temporarily disappear.

**Prompt Encoder**: Handles the user's input prompts:
- Sparse prompts (points, boxes) are converted to positional encodings plus learned type embeddings
- Mask prompts are embedded and added to the frame features

**Mask Decoder**: A modified transformer decoder that predicts:
1. Whether the target object is present in the current frame
2. The segmentation mask for that object
3. For ambiguous prompts, multiple possible masks

**Memory Encoder and Bank**: The output mask is processed through a convolutional module. The resulting embedding is combined with image features and stored in a FIFO queue of up to N recent frames. This memory bank allows the model to maintain temporal coherence.

### SA-V Dataset

SAM2 was trained on the **SA-V dataset** - the largest video segmentation dataset to date:
- 50,900 videos captured by 510 crowdworkers
- 642,600 masklets with 35.5 million masks
- 54% indoor, 46% outdoor scenes
- Average video duration: 14 seconds

The annotation process was remarkably efficient, evolving through three phases:

**Phase 1**: Manual annotation using original SAM for mask prediction plus editing tools. High quality but slow: 37.8 seconds per frame. Collected 16k masklets.

**Phase 2**: Annotators generate first-frame masks, then SAM2 (trained on Phase 1 data) propagates temporally. Annotators only intervene when predictions go wrong. Time reduced to 7.4 seconds/frame. Collected 63.5k masklets.

**Phase 3**: SAM2 (now trained on both Phase 1 and 2) predicts masklets, and annotators only provide refinement clicks. Time further reduced to 4.5 seconds/frame. Collected 197k masklets.

This iterative model-in-the-loop approach enabled creating a massive dataset efficiently.

### SAM2 Performance

SAM2 significantly outperforms previous approaches. The evaluation metric is **J&F accuracy** - the average of:
- **J (Jaccard Index)**: Region similarity (intersection over union)
- **F (F1 contour)**: Contour accuracy

Results across 17 benchmark datasets:
- SAM2 with 1 click: 64.3%
- SAM2 with 5 clicks: 75.4%
- SAM2 with ground-truth mask: 77.6%

Compared to baselines (SAM+XMem++ and SAM+Cutie), SAM2 shows substantial improvements across all prompt types.

You can try SAM2 yourself at https://sam2.metademolab.com/demo - it's quite impressive to see in action.

---

### [Slides 44-45: Connected Component Labeling]

After background segmentation, we have a binary image showing foreground pixels. But we need to group these pixels into coherent **blobs** - connected regions that likely correspond to individual objects.

**Connected Component Labeling (CCL)** is the algorithm that does this. It assigns a unique label to each group of connected foreground pixels. The definition of "connected" can be either:
- **4-connectivity**: Pixels sharing an edge are connected
- **8-connectivity**: Pixels sharing an edge or corner are connected

Typical processing after CCL includes:
- **Dilation**: Expanding blobs slightly to fill small holes
- **Erosion**: Shrinking blobs to remove noise
- **Area filtering**: Removing blobs that are too small or too large
- **Bounding box computation**: Finding the smallest rectangle containing each blob

The output is a set of labeled blobs, each with associated properties like area, centroid, and bounding box.

---

## BLOB TRACKING (MOTION TRACKING)
### [Slides 46-52]

Having detected blobs in each frame, we now need to **track** them across frames - maintaining consistent identities as objects move through the scene.

This is fundamental for gesture analysis. A gesture is inherently temporal - a sequence of movements. Without tracking, we'd have disconnected snapshots rather than continuous motion trajectories.

### The Tracking Problem

Given labeled blobs at frame t, we need to identify the same blobs at frame t+1 and assign consistent labels. This sounds simple but presents several challenges:

**Blob merging**: When two people walk close together, they may merge into a single blob. We need to recognize this is two objects, not one.

**Blob splitting**: When the merged blob separates again, we need to correctly identify which part is which person. Without additional information, this can be ambiguous.

**Occlusion**: When one object passes behind another, it temporarily disappears. We need to maintain its identity and recognize it when it reappears.

### Tracking Methods

Tracking typically assumes that frames are captured frequently enough that blob features don't change dramatically between consecutive frames. Correspondence is established by comparing:

**Geometrical features**:
- Location (distance between centroids)
- Size (area similarity)
- Shape (aspect ratio, compactness)
- Bounding box overlap

**Kinematic features**:
- Assuming constant velocity, predict where a blob should be
- Compare predictions with observations

**Color histograms**:
- Compare the color distribution within blobs
- Useful because appearance often remains consistent even when shape changes

### Mean-Shift Tracking

**Mean-shift tracking** (Comaniciu et al., 2000) models each blob as a color probability density function - essentially a weighted histogram of colors within the blob's region.

To track, the algorithm searches for the location in the new frame where the color distribution best matches the model. Mean-shift is a non-parametric technique that iteratively moves toward the mode (peak) of the similarity surface.

The key advantage is robustness to non-rigid deformation - the blob's shape can change significantly, but as long as its color distribution remains similar, tracking continues.

### BlazePose

**BlazePose** (Google, 2020) combines a lightweight body detector with a pose tracker network. Given a detected person, it predicts:
- 33 body landmark coordinates
- Presence detection (is the person still visible?)
- Refined region of interest for the next frame

The system achieves near real-time performance on mobile CPUs and super real-time on mobile GPUs - a remarkable engineering achievement.

### Google MediaPipe Pose

**MediaPipe Pose** uses a convolutional neural network similar to MobileNetV2 combined with a BlazePose variant. It additionally incorporates a 3D human shape model for better accuracy.

The system works across platforms - mobile phones, desktops, laptops, Python environments, and even web browsers. You can try it at https://codepen.io/mediapipe

---

## BOTTOM-UP POSE ESTIMATION: OpenPose
### [Slides 53-62]

Now let's discuss the alternative approach to extracting movement from video: **pose estimation** using deep learning.

**OpenPose** (Cao et al., 2016, CMU) was the first real-time multi-person 2D pose estimation system. It takes a fundamentally different approach than detection-then-pose methods.

### Top-Down vs. Bottom-Up

**Top-down approaches**:
1. First, detect all people in the image
2. Then, for each detected person, estimate their pose

The problem: Runtime scales with the number of people. For crowded scenes, this becomes slow.

**Bottom-up approaches** (like OpenPose):
1. First, detect all body parts in the image
2. Then, figure out which parts belong to the same person

The advantage: Runtime is constant regardless of how many people are in the image - all parts are detected in one forward pass.

### Part Affinity Fields (PAFs)

The key innovation in OpenPose is **Part Affinity Fields (PAFs)** - a representation that encodes both the location of limbs and their association with specific individuals.

For each limb type (e.g., right forearm), the PAF is a 2D vector field. At each pixel within a limb region, the vector points from one body part to the other. For example, in the forearm PAF, vectors point from elbow to wrist.

Why is this useful? When you have multiple people with multiple detected elbows and wrists, you need to determine which elbow connects to which wrist. The PAFs provide directional information that guides this association.

### Confidence Maps

**Confidence maps** are the other key output. Each confidence map is a 2D heatmap showing where a particular body part is likely located. High values indicate high confidence.

For a single person, each confidence map has one peak at the body part location. For multiple people, there are multiple peaks - one per visible instance of that body part.

### Architecture and Processing

OpenPose uses a multi-stage CNN architecture with two branches:
- **Blue/bottom branch**: Predicts Part Affinity Fields
- **Beige/top branch**: Predicts confidence maps

The network refines its predictions across multiple stages. Interestingly, the authors found that refining PAFs is more important than refining body part predictions - later versions focus computation on PAF refinement.

### Assembly Pipeline

1. The CNN predicts confidence maps S and PAFs L
2. Non-maximum suppression finds body part candidates from the peaks in confidence maps
3. PAFs are used to score potential connections between parts
4. Graph matching finds the optimal assignment of parts to people
5. Connected parts are assembled into full body poses

### Capabilities

OpenPose can detect:
- Body keypoints (15, 18, or 25 depending on model)
- Foot keypoints (big toes, small toes, heels)
- Hand keypoints (21 per hand)
- Facial keypoints (70 points)

The open-source library from CMU has become a standard tool for movement analysis research.

---

## INPUT: MOVEMENT REPRESENTATIONS
### [Slides 63-69]

Now let's formalize how we represent movement data mathematically.

### Movement as Time Series

Movement **X** is represented as a time-series of poses:

**X = {x(tᵢ)}ᵢ₌₁…N = (x(t₀), x(t₁), x(t₂), …, x(tN))**

where:
- **x(tᵢ)** is the pose at time tᵢ
- **N** is the number of poses
- Movement starts at time t₀

The **sampling period** Δt = tᵢ - tᵢ₋₁ is typically constant and depends on the capture device.

The **sampling frequency** fₛ = 1/Δt determines temporal resolution. Typical values:
- 100Hz - high-end motion capture
- 50Hz - standard motion capture
- 30Hz - standard video (NTSC)
- 25Hz - standard video (PAL)

### Pose Representation

A pose x(tᵢ) may consist of:

**Positions**: A set of Mp landmark positions
```
P(tᵢ) = {p¹(tᵢ), p²(tᵢ), …, pᴹᵖ(tᵢ)}
```
where each pᵏ = [xᵏ, yᵏ, zᵏ]ᵀ ∈ ℝ³

**Rotations**: A set of Mq joint angles, represented as quaternions
```
Q(tᵢ) = {q¹(tᵢ), q²(tᵢ), …, qᴹᵍ(tᵢ)}
```
where each qʰ ∈ ℝ⁴

### Why Quaternions?

Quaternions are four-dimensional numbers (w, x, y, z) that elegantly represent 3D rotations. They have several advantages over alternatives:

**Compared to rotation matrices (9 numbers)**:
- More compact (4 vs 9 values)
- Easier to normalize (just divide by magnitude)
- Natural interpolation (spherical linear interpolation - SLERP)

**Compared to Euler angles (3 numbers)**:
- No gimbal lock (singularities where degrees of freedom are lost)
- Unique representation (Euler angles have multiple conventions)
- Smooth interpolation (Euler angle interpolation can take unexpected paths)

A unit quaternion q = (w, x, y, z) with |q| = 1 represents a rotation of angle θ around axis (ax, ay, az):
```
w = cos(θ/2)
x = ax × sin(θ/2)
y = ay × sin(θ/2)
z = az × sin(θ/2)
```

Note that q and -q represent the same rotation - this "double cover" is a unique property of quaternions.

### Combined Representation

When both positions and rotations are available:
```
x = [p¹, p², …, pᴹᵖ, q¹, q², …, qᴹᵍ] ∈ ℝ³ᴹᵖ⁺⁴ᴹᵍ
```

### Blobs as Poses

Alternatively, when working with video-based blob tracking:
```
x(tᵢ) = B(tᵢ)
```
Movement becomes a time-series of blobs:
```
X = B = (B(t₀), B(t₁), B(t₂), …, B(tN))
```

### Device-Specific Outputs

Different capture systems produce different representations:

- **Inertial MoCap**: Positions + rotations of sensors
  x = [p¹, …, pᴹᴵ, q¹, …, qᴹᴵ]

- **Optical MoCap**: Marker positions only
  x = [p¹, …, pᴹᴼ]

- **RGB-D devices**: Landmark positions + blobs
  x = [p¹, …, pᴹᴿ, B]

- **Pose estimation**: 2D or 3D landmark positions
  x = [p¹, …, pᴱ]

- **Background subtraction + tracking**: Blobs only
  x = B

---

## OUTPUT: LOW-LEVEL FEATURES (LAYER 2)
### [Slides 70-80]

Layer 2 takes movement representations as input and outputs **feature time-series** that characterize movement.

Given movement X = (x(t₀), x(t₁), …, x(tN)), the output is a set of T time-series:
```
F = {F₁, F₂, …, Fₜ}
```

Each feature time-series:
```
Fₖ = (fₖ(x(t₀)), fₖ(x(t₁)), …, fₖ(x(tN)))
```
describes a specific facet of movement.

Features are derived from multiple disciplines:
- **Physics**: Kinematics (velocity, acceleration, jerk)
- **Biomechanics**: Joint angles, body segment orientations
- **Psychology**: Research by Boone & Cunningham (1998), de Meijer (1989), Wallbott (1998) on emotion in movement
- **Arts/Humanities**: Laban Movement Analysis concepts like effort, shape, and space

The slides show comprehensive tables from Kleinsmith and Bianchi-Berthouze (2013) cataloging which body features are associated with different emotional states - anger, joy, fear, sadness, etc. For example:
- Anger: High energy, fast movements, expanded limbs
- Sadness: Low energy, slow movements, collapsed upper body
- Fear: Head straight or bent back, elbows bent, arms lateral

Ahmed and colleagues (2020) identified 10 major groups of movement features from their comprehensive literature review.

---

### Pre-Processing Trajectories
### [Slides 75-77]

Before extracting features, trajectories typically need pre-processing:

**Down-sampling**: If the sampling frequency is higher than needed, reducing it saves computation. For example, if you're analyzing gross body movement, 100Hz might be excessive - downsampling to 50Hz or 25Hz may be sufficient.

**Low-pass filtering**: Removes high-frequency noise that can dramatically affect derivative-based features. Skogstad and colleagues (2013) proposed specific filters optimized for motion capture data, with different cut-off frequencies depending on the noise level and desired smoothness.

**Outlier removal**: Motion capture systems sometimes produce erroneous values - markers that jump impossibly far, or sensors that glitch. The **Hampel filter** is commonly used: it identifies points that deviate significantly from their local neighborhood and replaces them with the local median.

---

### Group 1: Kinematics Features
### [Slides 78-80]

For the trajectory of landmark k:
```
Pᵏ = (pᵏ(t₀), pᵏ(t₁), …, pᵏ(tN))
```

**Velocity** is the rate of change of position:
```
vᵏ(tᵢ) = [pᵏ(tᵢ) - pᵏ(tᵢ₋₁)] / [tᵢ - tᵢ₋₁]
```

Using a **centered derivative** reduces noise:
```
vᵏ(tᵢ₋₁) = [pᵏ(tᵢ) - pᵏ(tᵢ₋₂)] / [tᵢ - tᵢ₋₂]
```

**Speed** is the magnitude of velocity:
```
vᵏ(tᵢ) = √[(vₓᵏ)² + (vᵧᵏ)² + (vᵤᵏ)²]
```

**Acceleration** is the rate of change of velocity:
```
aᵏ(tᵢ) = [vᵏ(tᵢ) - vᵏ(tᵢ₋₁)] / [tᵢ - tᵢ₋₁]
```

From position directly:
```
aᵏ(tᵢ₋₁) = [pᵏ(tᵢ) - 2pᵏ(tᵢ₋₁) + pᵏ(tᵢ₋₂)] / [tᵢ - tᵢ₋₁]²
```

**Jerk** is the rate of change of acceleration:
```
jᵏ(tᵢ) = [aᵏ(tᵢ) - aᵏ(tᵢ₋₁)] / [tᵢ - tᵢ₋₁]
```

From position:
```
jᵏ(tᵢ₋₂) = [pᵏ(tᵢ) - 2pᵏ(tᵢ₋₁) + 2pᵏ(tᵢ₋₃) + pᵏ(tᵢ₋₄)] / [2(tᵢ - tᵢ₋₁)³]
```

**Jerk magnitude** is particularly meaningful - it describes movement smoothness:
- **High jerk**: Abrupt, jerky movement
- **Low jerk**: Smooth, fluid movement

This connects to Laban's concept of "flow" in movement quality. Trained dancers typically exhibit lower jerk than untrained movers, as do skilled versus novice performers of motor tasks.

Note that each successive derivative amplifies noise. This is why pre-processing (especially low-pass filtering) is crucial before computing these features.

---

## SUMMARY

In this first part of our body movement and gesture coverage, we've explored:

1. **Capture Devices**: From simple IMUs to sophisticated optical motion capture systems, each technology offers different trade-offs between cost, precision, portability, and ease of use.

2. **Video-Based Methods**: Background segmentation techniques ranging from simple subtraction to statistical GMM models and cutting-edge deep learning (SAM2).

3. **Tracking**: The challenges of maintaining object identity across frames, with solutions from mean-shift to modern pose trackers.

4. **Pose Estimation**: Bottom-up approaches like OpenPose that detect all body parts first, then assemble them into skeletons.

5. **Data Representation**: Mathematical frameworks for representing movement as time-series of poses, including positions and quaternion rotations.

6. **Basic Features**: Kinematic descriptors - velocity, speed, acceleration, jerk - that form the foundation for higher-level movement analysis.

In Part 2, we'll continue with additional feature groups and move up to mid-level features and conceptual interpretations of movement.

---


# Body Movement and Gesture - Lecture Notes Part 2
## Low-Level Features, Mid-Level Features, and Gesture Recognition
### Slides 81-185

---

## Introduction and Continuation

Welcome back, everyone. In our previous session, we established the foundational framework for body movement analysis—the four-layer architecture that takes us from raw physical signals all the way up to high-level concepts and meanings. We explored various capture devices, from inertial measurement units to optical motion capture systems, and we discussed pose estimation techniques including the powerful OpenPose system with its Part Affinity Fields.

Today, we're going to dive deep into the heart of movement analysis: the extraction and computation of low-level features. These are the quantitative descriptors that allow us to characterize human movement in meaningful ways. We'll cover ten distinct groups of features, each capturing different aspects of how the human body moves through space and time. Then we'll move on to mid-level features and conclude with gesture recognition approaches.

[Slides 81-82]

---

## The Savitzky-Golay Filter: Solving the Differentiation Problem

Before we can discuss kinematics properly, we need to address a fundamental signal processing challenge. When we compute derivatives—velocity from position, acceleration from velocity, jerk from acceleration—we encounter a serious problem: differentiation amplifies noise. Every time you differentiate a signal, you degrade the signal-to-noise ratio. This is a mathematical reality that we cannot escape.

[Slides 83-85]

Enter the Savitzky-Golay filter, developed by Abraham Savitzky and Marcel Golay in their landmark 1964 paper. This filter elegantly solves our problem by combining differentiation and smoothing into a single operation.

Here's how it works conceptually. Instead of differentiating raw, noisy data points directly, the Savitzky-Golay filter fits successive subsets of adjacent data points with a low-degree polynomial using least-squares regression. Typically, we use polynomials of degree 2 to 6. Then—and this is the key insight—we differentiate the fitted polynomial rather than the raw data.

Think about what this means. A polynomial is a smooth, continuous function. When you fit it to noisy data, you're essentially finding the underlying trend. Differentiating this smooth polynomial gives you a much cleaner derivative than you would get from differentiating the noisy data directly.

The filter operates via a sliding window approach. As the window moves across your data, a polynomial is fitted to the points within that window, and the derivative is computed at the central point. What makes this computationally efficient is that for equally-spaced data points, we can pre-calculate a set of "convolution coefficients" that can be applied to all data subsets through simple convolution. Savitzky and Golay published tables of these coefficients for various polynomial degrees and window sizes.

One of the beautiful properties of this filter is that it preserves important signal features—peaks and valleys in your data remain at their correct positions and maintain their relative heights, while the noise is significantly reduced. This is crucial for movement analysis where we care about the timing and magnitude of maximum velocities or accelerations.

The Savitzky-Golay filter has become one of the most cited tools in signal processing and chemometrics. Its ability to smooth data while preserving the shape of the original signal makes it ideal for preprocessing motion capture data before computing kinematic features.

---

## Group 1: Kinematic Features

[Slides 86-88]

Now let's discuss our first group of low-level features: kinematics. These are the most fundamental descriptors of movement.

**Velocity** is our starting point—the rate of change of position over time. For a landmark k at time t_i, velocity is computed as:

v^k(t_i) = (p^k(t_i) - p^k(t_{i-1})) / Δt

where p^k represents the 3D position of landmark k. In practice, we often work with speed, which is the magnitude of the velocity vector.

**Acceleration** is the rate of change of velocity:

a^k(t_i) = (v^k(t_i) - v^k(t_{i-1})) / Δt

Acceleration tells us about the forces acting on body parts. High acceleration indicates rapid changes in movement, which often correlates with effort and intentionality.

**Jerk** is the rate of change of acceleration:

j^k(t_i) = (a^k(t_i) - a^k(t_{i-1})) / Δt

Jerk is particularly important for understanding movement quality. Smooth, fluid movements have low jerk, while jerky, abrupt movements have high jerk values. This makes jerk an essential feature for expressive movement analysis.

Note that as we compute higher-order derivatives, we lose data points at the beginning of our time series. Velocity requires at least two points, acceleration requires three, and jerk requires four.

---

## Group 2: Shape of Trajectories

[Slides 89-91]

The second group of features describes the geometry of the paths traced by body parts through space.

**Direction of Movement** is captured as a unit vector of velocity:

d^k(t_i) = (p^k(t_i) - p^k(t_{i-1})) / ||p^k(t_i) - p^k(t_{i-1})||

This gives us a normalized vector pointing in the direction of movement, independent of speed.

**Curvature** measures how much a trajectory deviates from a straight line. Mathematically:

c^k(t_i) = ||a^k(t_i) × v^k(t_i)|| / (v^k(t_i))³

The **radius of curvature** is simply the reciprocal: r^k = 1/c^k(t_i). A straight-line movement has zero curvature (infinite radius), while a tight turn has high curvature (small radius).

These trajectory features are essential for understanding the spatial patterns of gestures. For instance, circular gestures have consistent curvature, while pointing gestures have very low curvature.

---

## Group 3: Quantity of Movement

[Slides 92-94]

How much is someone moving? This seemingly simple question is actually quite nuanced, and our third group of features addresses it.

**Quantity of Motion (QoM)**, as defined by Larboulette and Gibet in 2014, is computed as a weighted sum of speeds across multiple landmarks:

qom^L(t_i) = Σ(w_j × v^j(t_i)) / Σw_j

The weights w_j allow us to emphasize certain body parts over others depending on the application. We typically compute QoM separately for different body regions: arm region, head region, upper body, lower body, and whole body. This gives us a multi-scale view of movement activity.

**Kinetic Energy** provides a more physically grounded measure:

ke(t_i) = (1/2) × Σ m_j × (v^j(t_i))²

Here, m_j represents the mass of body segment j. But how do we know these masses? We use anthropometric tables—standardized data about human body proportions. The classic reference is Dempster and Gaughran's 1967 work, which provides body segment masses as percentages of total body weight.

For example, the head is approximately 8% of body weight, the upper arm about 3%, and the thigh about 10%. These percentages, combined with an estimate or measurement of total body mass, allow us to compute realistic kinetic energy values.

---

## Group 4: Use of Space

[Slides 95-97]

How much space does a person's movement occupy? This is captured by our fourth group of features.

The **Bounding Volume** is computed as the size of the smallest axis-aligned parallelepiped (essentially a 3D box) that contains a set of landmarks L at time t_i:

bv^L(t_i) = d_x^L × d_y^L × d_z^L

where d_x^L = max(x^j) - min(x^j), and similarly for y and z dimensions.

We compute bounding volumes for different body regions: arms, head, upper body, lower body, and the whole body. A dancer performing expansive movements will have large bounding volumes, while someone sitting quietly will have small bounding volumes.

This feature captures what dance theorists call the "kinesphere"—the sphere of space around the body that can be reached without taking steps. Rudolf Laban, whose work we'll discuss shortly, was fascinated by how people use their kinespheric space.

---

## Group 5: Displacements

[Slides 98-100]

The fifth group measures how body parts are positioned relative to reference points.

**Displacement from Reference** calculates the distance of a landmark from a reference point—typically the base of the spine:

ds^h(t_i) = ||p^h(t_i) - p^r(t_i)||

This tells us how far a hand or head is extended from the body's core at any moment.

The **Body Centroid** (or center of gravity) is computed as a first-order moment over all landmark coordinates:

p^c(t_i) = (x^c, y^c, z^c)

where x^c(t_i) = (1/M_p) × Σx^j(t_i), and similarly for y and z.

The body centroid is a crucial reference point for understanding balance and weight distribution. When you lean forward, your centroid shifts forward; when you raise your arms, your centroid shifts upward.

---

## Group 6: Compactness

[Slides 101-105]

Our sixth group of features describes the extent to which limbs are extended versus kept close to the body centroid. This relates to what dance and movement analysts call the "body attitude."

**Verticality** is simply the maximum height reached by any landmark:

vt(t_i) = max(y^j(t_i))

This increases when arms are raised overhead.

**Extension** measures the maximum distance any landmark reaches from the body centroid:

ex(t_i) = max(||p^j(t_i) - p^c(t_i)||)

**Elbow Flexion** is computed as the angle formed by the shoulder, elbow, and hand, using the dot product of the relevant vectors. A fully extended arm has low flexion; a bent arm has high flexion.

**Arm Shape** is the direct distance from hand to shoulder:

as(t_i) = ||p^H(t_i) - p^{SP}(t_i)||

When the arm is extended, this distance is large; when flexed, it's small.

**Hands Distance** and **Feet Distance** measure the spatial separation between the two hands and two feet, respectively.

**Point Density** calculates the average distance between all landmarks and the body centroid—another measure of overall body compactness.

The **3D Contraction Index**, as proposed by Piana and colleagues in 2016, compares the bounding volume at time t to a reference bounding volume from a neutral pose:

ci(t_i) = bv^T(t_i) / bv^P(t_i)

Values less than 1 indicate a contracted posture; values greater than 1 indicate an expanded posture.

Finally, the **Minimum-Volume Ellipsoid** approach uses the Khachiyan algorithm from 1996 to find the smallest ellipsoid that encloses the body landmarks. From this, we compute **sphericity**:

Ψ^L(t_i) = π^{1/3} × (6V)^{2/3} / A

where V is volume and A is surface area. Higher sphericity indicates more contracted, ball-like postures.

---

## Group 7: Laban's Effort

[Slides 106-115]

Now we come to one of the most influential frameworks in movement analysis: Rudolf Laban's Theory of Effort.

Rudolf Laban (1879-1958) was a pioneering choreographer, dance theorist, and movement analyst. Born in what is now Slovakia, he developed a comprehensive system for understanding human movement that has influenced not just dance, but also theater, physical therapy, psychology, and now computational movement analysis.

Laban's system, known today as Laban Movement Analysis (LMA), includes several components, but his Theory of Effort—developed in the 1940s—is particularly relevant to us. Effort describes the dynamic, expressive qualities of movement: how we move rather than what movement we perform.

Laban identified four **Effort factors**, each existing on a continuum between two poles:

1. **Space**: Direct ↔ Flexible (or Indirect)
   - Direct movement has a single, focused spatial pathway
   - Flexible movement has an indirect, multi-directional pathway
   - This factor relates to attention and focus

2. **Time**: Quick (Sudden) ↔ Sustained
   - Quick movement is urgent, accelerating
   - Sustained movement is leisurely, decelerating
   - This factor relates to our sense of urgency

3. **Weight**: Strong (Heavy) ↔ Light
   - Strong movement exerts power, uses force
   - Light movement is delicate, buoyant
   - This factor relates to our intended impact on the world

4. **Flow**: Bound ↔ Free
   - Bound movement is controlled, can be stopped at any instant
   - Free movement is fluid, continuous, hard to stop
   - This factor relates to our attitude toward bodily control

Laban combined Space, Weight, and Time into what he called the **Eight Effort Actions** or "Action Drive":

1. **Float**: Indirect + Light + Sustained — buoyant, carefree, like a feather drifting
2. **Punch/Thrust**: Direct + Strong + Sudden — forceful, aggressive, like striking
3. **Glide**: Direct + Light + Sustained — smooth, controlled, like stroking silk
4. **Slash**: Indirect + Strong + Sudden — violent, chaotic, like wielding a sword
5. **Dab**: Direct + Light + Sudden — quick, delicate, like dotting an i
6. **Wring**: Indirect + Strong + Sustained — twisting, struggling, like wringing a towel
7. **Flick**: Indirect + Light + Sudden — light, playful, like flicking away a crumb
8. **Press**: Direct + Strong + Sustained — firm, determined, like pushing a heavy door

These eight actions provide a vocabulary for describing and analyzing expressive qualities of movement. Actors and dancers use this framework extensively for character development—imagining how a character might move with primarily "punching" versus "gliding" qualities.

**Computational Implementation**

Now, how do we compute Laban's Effort factors from motion capture data? Here are the typical computational approaches:

**Laban's Space** (Directness Index):

ls^k(t_i) = ||p^k(t_i) - p^k(t_{i-W})|| / Σ||p^k(t_{i-j}) - p^k(t_{i-j-1})||

This compares the straight-line distance traveled over a temporal window W to the actual path length. Values near 1 indicate direct movement; values near 0 indicate flexible, indirect movement.

**Laban's Time** uses the average acceleration magnitude over the window:

lt^k(t_i) = (1/W) × Σa^k

Higher values indicate quick, sudden movement; lower values indicate sustained movement.

**Laban's Weight** uses the maximum kinetic energy over the window:

lw(t_i) = max(ke(t_j)) for t_j in window W

Higher energy indicates strong movement; lower energy indicates light movement.

**Laban's Flow** uses the average jerk magnitude:

lf^k(t_i) = (1/W) × Σj^k

Higher jerk indicates bound, controlled movement; lower jerk indicates free, flowing movement.

Note that all Laban features require a temporal window W and are therefore only available after time t_W in the recording.

---

## Group 8: Bounding Triangle

[Slides 116-118]

The eighth group introduces a compact representation developed by Glowinski and colleagues in 2011: the bounding triangle formed by the two hands and the head.

The **Barycenter** (center) of this triangle is:

p^{tc}(t_i) = (1/3) × (p^{RH} + p^{LH} + p^H)

where RH is right hand, LH is left hand, and H is head.

The **Spatial Extent** of this triangle measures how far its center is from a reference point (typically the spine base):

te(t_i) = ||p^{tc}(t_i) - p^r(t_i)||

This simple representation captures much of the upper body's expressive configuration. When someone gestures enthusiastically, this triangle expands and moves outward; when someone is closed off or dejected, it contracts and stays close to the body.

---

## Group 9: Symmetry

[Slides 119-122]

Symmetry is a fundamental aesthetic and expressive quality of movement. Our ninth group provides two different approaches to quantifying it.

**First Symmetry Index** (from Glowinski et al., 2011) is based on the bounding triangle. It computes horizontal and vertical symmetry indices and then takes their ratio:

si_1(t_i) = hs/vs

This measures postural symmetry at a single instant.

**Second Symmetry Index** uses a concept called Geometric Entropy, based on work by Cordier and colleagues from 1994:

h^k(t_i) = ln(2×tl^k / pc^k)

where tl^k is the trajectory length and pc^k is the perimeter of the convex hull of the trajectory over a time window.

For symmetry, we compare the left and right hands:

si_2(t_i) = h^{LH} / h^{RH}

This captures the symmetry of temporal development—whether both hands trace similar patterns over time.

Symmetric movements often convey balance, calm, and control. Asymmetric movements can convey dynamism, urgency, or imbalance. Analyzing symmetry is particularly important in dance analysis and in clinical applications assessing motor disorders.

---

## Group 10: Balance

[Slides 123-126]

Our tenth and final group of low-level features relates to balance and stability.

**Displacement of Body Centroid** tracks how the center of gravity moves between frames:

ds^c(t_i) = ||p^c(t_i) - p^c(t_{i-1})||

Large displacements indicate less stable postures or dynamic weight shifts.

**Upper/Lower Body Difference** compares the centroids of the upper and lower body:

bl_1(t_i) = ||p^{uc}(t_i) - p^{lc}(t_i)||

When these are well-aligned vertically, the body is in a stable configuration.

The **Support Polygon** approach, from Larboulette and Gibet in 2014, provides a more physically grounded stability measure. The support polygon is the convex hull of all contact points with the ground (typically the feet).

bl_2(t_i) = 1_{sp}(P × p^c)

This is a binary value indicating whether the projection of the body centroid onto the ground plane lies within the support polygon. If yes, the person is in a stable stance; if no, they're either falling, jumping, or in an unstable posture.

This feature is particularly important for robotics (ensuring humanoid robots remain stable) and for clinical gait analysis (detecting balance impairments in patients).

---

## Summary: Output Time-Series

[Slides 127-128]

At this point, we've covered all ten groups of low-level features. Let's step back and understand what we have.

From our motion capture data, we now produce multiple time-series of features:

F = {f_1, ..., f_T}

For example:
- V^k = (v^k(t_2), ..., v^k(t_N)) — velocity time-series starting at t_2
- S^k = speeds
- A^k = accelerations starting at t_3
- J^k = jerks starting at t_4

Notice the progressive delay as derivative order increases. Each derivative requires one more data point before it can be computed.

These time-series form the raw material for our mid-level feature extraction, which we'll discuss next.

---

## Blob Features: When You Only Have 2D Silhouettes

[Slides 129-133]

Before we move to mid-level features, let me briefly discuss an alternative approach for when you don't have full 3D skeletal data—when all you have are 2D silhouettes from video.

**Silhouette Motion Images (SMI)** capture movement by accumulating silhouettes over a window W:

SMI(t_i) = (Σ B(t_{i-k})) - B(t_i) for k in window W

This highlights areas where motion has occurred—stationary regions are subtracted out.

**Motion Index** measures the relative amount of motion:

mi(t_i) = Area(SMI) / Area(B)

This can be weighted—pixels near the center approximate kinetic energy (core movement), while contour pixels provide a perceptual measure (visible movement).

**Body Shape** features use second-order central moments to fit an ellipse to the silhouette. The **orientation angle θ**:

θ(t_i) = (1/2) × tan^{-1}(2μ_{1,1} / (μ_{2,0} - μ_{0,2}))

And the **eccentricity ε** (measuring contraction/expansion):

ε(t_i) = √((μ_{2,0} - μ_{0,2})² - 4μ_{1,1}²) / (μ_{2,0} + μ_{0,2})²

**Contraction Index** for silhouettes:

ci(t_i) = Area(B) / br^B

where br^B is the bounding rectangle area. Values near 1 indicate an expanded posture filling the bounding box; smaller values indicate a contracted posture.

**Asymmetry Index** divides the blob in half based on its centroid, flips one half, and computes a difference blob. The normalized area of this difference blob indicates asymmetry—smaller values mean more symmetry.

These blob features are computationally lighter and can be applied to standard video without specialized depth sensors.

---

## Mid-Level Features: From Time-Series to Events

[Slides 134-136]

Now we transition from Layer 2 (Low-level features: synchronous time-series) to Layer 3 (Mid-level features: asynchronous, event-based descriptors).

The key insight here is that raw movement data flows continuously, but human actions and gestures occur as discrete events. Mid-level processing bridges this gap.

Our input is a set of T time-series F = {f_1, ..., f_T}, each representing a feature characterizing movement over time, sampled at a device-dependent rate f_s.

The processing involves two steps:

1. **Unitizing**: Identification of movement units—segmenting the continuous stream into discrete, meaningful chunks
2. **Annotation**: Providing measures characterizing what is observed within each unit

---

## Unitizing: Dividing Observation into Meaningful Units

[Slides 137-145]

Unitizing—the process of dividing an observation into discrete smaller units—is crucial and often underappreciated. It can be done manually or automatically, and the choice of approach significantly impacts downstream analysis.

**Interval Coding** is the simplest approach: divide the recording into fixed-length time intervals. The "thin slices" approach, popularized by researchers like Gatica-Perez, uses windows ranging from 2 seconds to 5 minutes for efficient assessment of personality, affect, and interpersonal relations.

Pros: Fast, easy to automate, requires no prior knowledge
Cons: Boundaries may fall within actions, not suitable for segmenting individual gestures
Best for: Continuous movement without clear pauses

**Continuous Coding** identifies and annotates every utterance or gesture. The ACT4Teams system by Kauffeld and colleagues (2018) exemplifies this—it breaks observation into "thought units," the smallest meaningful segments, coded into 43 categories for group behavior analysis.

Pros: Tailored for specific research objectives
Cons: Very difficult to automate, extremely time-consuming when done manually

**Cognitive-Inspired Segmentation** grounds the approach in Event Segmentation Theory, developed primarily by Jeffrey Zacks and colleagues. According to this theory, an event is "a segment of time at a given location, conceived by an observer to have a beginning and end."

The theory proposes that our perceptual systems continuously make predictions about what will happen next. When something unpredictable occurs, we perceive an event boundary—the end of one event and the beginning of another.

Research shows that people spontaneously segment ongoing activity into hierarchically organized parts and sub-parts. This segmentation is automatic—it happens even during passive viewing—and it affects memory and learning. Those who identify appropriate event boundaries tend to remember more and learn more proficiently.

Event boundaries are typically perceived at changes along seven dimensions:
1. Time
2. Space
3. Objects involved
4. Characters present
5. Character interactions
6. Causes
7. Goals

Neural imaging studies have identified specific brain regions—including posterior temporal and parietal cortex and lateral frontal cortex—that show increased activity at event boundaries.

This cognitive perspective offers a principled way to think about movement segmentation, though implementing it automatically remains an open challenge.

---

## Unitizing Applied to Movement

[Slides 146-150]

When we apply unitizing specifically to movement data, we're distinguishing between movement units (when the user is moving) and pauses (when the user does not appear to move). This is also called movement segmentation. A movement unit may—or may not—correspond to what we'd call a gesture.

Let's pause and consider: what exactly is a gesture? Researchers have offered various definitions:

- "Natural gestures spontaneously generated by person telling story, speaking in public, or holding conversation" (Cassell et al., 1990)
- "Movements of arms and hands closely synchronized with flow of speech" (McNeill, 1992)
- "Movement of one's body that conveys meaning to oneself or partner in communication" (Hummels et al., 1998)
- "Movement of body that contains information" (Kurtenbach & Hulteen, 1990)

McNeill's taxonomy distinguishes between several types of gestures:
- **Iconic gestures**: Air pictures representing aspects of objects (e.g., tracing a shape)
- **Metaphoric gestures**: Represent abstract concepts (e.g., "big idea" with expansive hand motion)
- **Deictic gestures**: Pointing motions identifying locations of people, places, things
- **Beats**: Little hand waves that underscore speech, give accent, help turn-taking

Note that McNeill's framework focuses on natural gestures accompanying speech. For our purposes, we also need to consider what we might call **Expressive Gestures**: movements that contain and convey expressive information (emotion), independent from but superimposed on possible denotative meaning.

Classic research by Pollick (2001) demonstrated this beautifully: the same door-knocking action can be performed with different emotions—angry knocking versus tentative knocking versus joyful knocking. The denotative meaning (knocking) is the same, but the expressive quality differs dramatically.

Similarly, dance fragments or "micro-dances" can be performed with different expressive qualities—fluid versus rigid, timid versus aggressive—superimposing expressivity onto the same basic choreography.

---

## Unitizing by Thresholding

[Slides 151-153]

When movement consists of a regular sequence of gestures and pauses, a simple thresholding approach often works well.

The method is straightforward:
1. Compute an energy measure (e.g., Motion Index or Quantity of Motion)
2. Apply a threshold (fixed or adaptive)
3. Detect a boundary when energy falls below the threshold

Glowinski and colleagues (2009) showed that this approach reflects manual unitizing by human participants—people generally agree about when movement stops and starts.

The limitation is that thresholding doesn't account for cognitive or affective processes. If a person's goal changes mid-gesture, that cognitive event boundary won't be detected unless there's also a change in movement energy.

---

## Annotation: Computing Mid-Level Features

[Slides 154-158]

Once we've identified movement units (possibly gestures), we annotate each unit with descriptive features.

For each movement unit, our T input time-series are processed to produce N_1, N_2, ..., N_T scalar values from time-series 1, 2, ..., T. These are the features describing behavior during that movement unit.

At the end of a movement unit, we return a feature vector:

g = [g_{11}, ..., g_{1N_1}, g_{22}, ..., g_{2N_2}, ..., g_{T1}, ..., g_{TN_T}]

This vector of mid-level features represents the annotated scores for that movement unit.

Note what's happening here: we've moved from synchronous data (continuous time-series sampled at regular intervals) to asynchronous data (feature vectors computed at irregular intervals corresponding to movement units).

**Two Major Approaches for Computing Mid-Level Features:**

1. **Analysis Primitives**: Operators that summarize temporal dynamics
   - Statistical operations: average, variance, skewness, kurtosis
   - Salient events and shape: slope, peaks, valleys, initial slope, final slope, main peak duration, gesture duration
   - Complexity: sample entropy
   - Intrapersonal synchronization: recurrence quantification analysis
   - Causality and interdependence: Granger causality

2. **Complex Features**: Specifically defined combinations
   - **Impulsivity** (Niewiadomski et al., 2015): Sudden movement with high acceleration, performed with no premeditation
   - **Fluidity** (Piana et al., 2016): Smooth movement where energy propagates efficiently along kinematic chains, with minimal dissipation

---

## Representations of Mid-Level Features

[Slides 159-161]

How do we represent these mid-level features? Two main approaches exist:

**Symbolic Representation** treats movement units as logical predicates. For example:

motion_unit(26, 8, 0.0325, 0.0243, 2, 0.039, -0.407, 0.342)

This enables logic-based reasoning and formal languages for choreography description.

**Dimensional Representation** treats movement units as points or trajectories in feature spaces—for instance, a QoM versus Fluency space. This enables geometric reasoning, clustering, and classification. You can visualize movements as points in multidimensional space and apply all the standard machine learning techniques.

---

## Gesture Recognition: Two Major Tasks

[Slides 162-165]

Now we turn to gesture recognition: the interpretation of human gestures via algorithms. This is a vast field with applications ranging from sign language interpretation to touchscreen interaction to emotion recognition.

There are two major questions we might ask:

1. **Which gesture was performed?** (Traditional gesture recognition)
   - Classify input as one of a predefined vocabulary of gestures
   
2. **How was the gesture performed?** (Expressive gesture processing)
   - Analyze the qualitative, expressive properties of the movement

A **Gesture Recognizer** is a system that takes an unknown input gesture and classifies it as one element of a predefined vocabulary. There are two main approaches: template-based and machine learning-based.

**Template-Based Approaches** align the input sequence with reference templates and compute a distance. If the distance is small, it's a match. Examples include Rubine's algorithm (1991), the $ family ($1, $N) from Wobbrock and colleagues (2007), and Dynamic Time Warping (DTW).

**Machine Learning Approaches** learn to identify patterns through training. Examples include k-Nearest Neighbors (kNN), Hidden Markov Models (HMM), Support Vector Machines (SVM), Artificial Neural Networks (ANN), and deep learning architectures.

---

## The $1 Unistroke Recognizer

[Slides 166-172]

Let me describe in detail one elegant template-based algorithm: the $1 recognizer, developed by Jacob Wobbrock, Andrew Wilson, and Yang Li at the University of Washington and Microsoft Research in 2007.

The $1 recognizer is designed for 2D single-stroke gestures—the kind you might draw with a finger on a touchscreen. Despite its simplicity, it achieves remarkable accuracy: over 97% with just one template per gesture, and 99% with three or more templates. The entire algorithm is only about 100 lines of code.

**Representation**: A gesture G is a sequence of M 2D points. Each template gesture T_i consists of N equidistant points (typically N=64).

**The Four Steps:**

**Step 1: Resampling**
Resample the M-point path to N equidistantly spaced points. This normalizes for drawing speed—whether you draw slowly or quickly, you end up with the same number of points.
- Compute total path length L
- Calculate increment I = L/(N-1)
- Add new points (via linear interpolation) whenever cumulative distance exceeds I

**Step 2: Rotation**
Rotate the gesture based on its "indicative angle"—the angle between the centroid and the first point.
- Compute the gesture centroid (x̄, ȳ)
- Compute the angle between the first point, centroid, and horizontal axis
- Rotate all points so this indicative angle becomes 0°
This normalizes for orientation—a circle drawn starting from any position becomes comparable.

**Step 3: Scale and Translate**
- Compute the bounding box of the gesture
- Scale (non-uniformly) to fit a reference square
- Translate so the centroid is at origin (0,0)
This normalizes for size and position.

**Step 4: Recognition**
Compare the normalized candidate G with each template T_i by computing path-distance:

d_i = (Σ√((g_x - t_x^i)² + (g_y - t_y^i)²)) / N

The template with minimum path-distance is the recognition result:

î = argmin(d_i)

**Strengths of $1:**
- Resilient to sampling variations (speed, device)
- Rotation and scale invariant
- Simple to implement
- Fast to execute
- Only one example needed to teach a new gesture

**Limitations:**
- Cannot distinguish rectangle from square (aspect ratio lost in uniform scaling)
- Cannot distinguish ellipse from circle
- Cannot distinguish arrow orientations (rotation normalization eliminates this)

The $1 recognizer has had enormous impact. It sparked a whole "dollar family" of recognizers: $3, $N (for multi-stroke gestures), $P (point-cloud matching), and more. In 2024, the original paper received the ACM UIST Lasting Impact Award, recognizing its foundational contribution to gesture recognition technology.

---

## Machine Learning for 3D Hand Gesture Recognition

[Slides 173-177]

Let's look at a more sophisticated machine learning approach: De Smedt and colleagues' 2016 work on 3D hand gesture recognition from skeletal data.

**Input**: Motion capture data providing 3D positions of 22 hand keypoints from an Intel RealSense camera at 30fps.

**Dataset**: 14 gesture types (2800 gestures total), performed by 28 participants, with 1-10 repetitions each. Gesture duration ranges from 20-50 frames.

**Feature Extraction**:

*Motion Features*:
- Direction: d(t_i) = (p^{palm}(t_i) - p^{palm}(t_{i-c})) / ||...|| 
- Rotation: r(t_i) = (p^{palm} - p^{wrist}) / ||...||
- Aggregated into matrices M_{dir} and M_{rot}

*Shape Features*:
- Normalization (removing personal hand size differences)
- Shape of Connected Joints (SoCJ): 9 SoCJ descriptors per frame
- Fisher Vector representation

**Classifier**: SVM with linear kernel (appropriate for high-dimensional data)

**Evaluation**: Leave-One-Subject-Out (LOSO)—train on 27 subjects, test on the remaining one, rotate through all subjects.

**Results**: 82.90% accuracy for 14 gestures; 71.90% for 28 gestures (distinguishing the same gesture performed with one versus multiple fingers).

---

## Deep Learning for Hand Gesture Recognition

[Slides 178-180]

Devineau and colleagues (2018) applied deep learning to the same problem.

**Preprocessing**: Data resampled via linear interpolation to fixed-length gestures (100 time-steps). This standardizes input dimensions for the neural network.

**Architecture**: The network has three branches:
- Two branches for feature extraction at different resolutions (high versus low)
- One residual branch (skip connection)
The outputs are combined, enabling multi-resolution processing with residual learning.

**Experiments**: Same dataset (2800 gestures), split 70% training / 30% testing.

**Results**: Improved performance over traditional methods, demonstrating the power of learned hierarchical features.

---

## Expressive Gesture Recognition: Emotion from Dance

[Slides 181-185]

Now let's examine expressive gesture recognition—classifying not which gesture, but how it was performed.

Camurri and colleagues (2004) tackled automatic classification of basic emotions in "micro-dances."

**Dataset**: 5 dancers performed the same choreography 4 times each, with different expressive intentions: Anger, Fear, Grief, and Joy. Total: 20 dance performances.

**Features**: 18 standardized features including:
- Motion Index and its 1st/2nd order statistics
- Contraction Index and its statistics
- Upward Movement
- Directness Index
- Various kinematic features

**Unitizing**: 334 motion units identified using fixed threshold on Motion Index.

**Classification Method**: Five decision tree models, with 85% training / 15% testing (stratified by the 4 emotion classes).

**Results**:
- Human spectators (watching the same videos): 56% accuracy
- Automatic classification: 36% average accuracy (64% relative to human performance)
- Best model: 40% accuracy (71% relative to human)
- Chance level: 25%

Confusion matrix analysis showed Anger was best recognized (41.7%), while Joy was most confused with other emotions.

**Discussion**: Why was spectator recognition only 56%? Remember, dancers were viewed without facial expressions visible. The gap between automatic and human performance likely reflects neglected temporal aspects in the feature set.

This study demonstrated feasibility but highlighted the challenge: expressive movement analysis is hard even for humans, and automatic systems have considerable room for improvement.

---

## CNN-LSTM for Dance Emotion Recognition

[Slides 186-188]

Wang and colleagues (2020) applied modern deep learning to this challenge.

**Task**: Recognize 7 emotions (fear, anger, boredom, excitement, joy, relaxation, sadness) from Laban-inspired features.

**Input**: Motion capture data with 3D positions and Euler angles of 54 body landmarks.

**Features**: Inspired by Laban Movement Analysis:
- 52 distances between joints
- 54 speeds
- 54 accelerations
- Unitizing: Interval-coding with features averaged over 1-second windows

**Architecture**: Hybrid CNN-LSTM:
- Three CNN branches for spatial feature extraction
- Fully connected layer combining CNN outputs
- LSTM layers for temporal modeling
- Final fully connected layer outputting 7 emotion classes

**Results**:
- CNN-LSTM: 0.95 average F1-score
- CNN only: 0.88
- LSTM only: 0.87

The combination of spatial processing (CNN) and temporal modeling (LSTM) significantly outperformed either alone, demonstrating the importance of capturing both spatial patterns and temporal dynamics in expressive movement.

---

## Key Takeaways

Let me summarize the key points from today's lecture:

**Low-Level Features** (10 groups):
- Provide comprehensive movement characterization
- Range from basic kinematics to complex expressive qualities
- Laban's Effort theory bridges movement and emotion
- Blob features enable analysis when only silhouettes are available

**Mid-Level Features**:
- Unitizing is crucial for creating meaningful analysis units
- Multiple approaches: interval coding, continuous coding, cognitive-inspired
- Analysis primitives provide systematic feature extraction
- Representations: symbolic (logic-based) versus dimensional (geometric)

**Gesture Recognition**:
- Template-based methods: Fast, simple, but limited flexibility
- Machine learning: More robust, requires training data
- Deep learning: State-of-the-art performance, requires large datasets
- Expressive gesture recognition: Challenging but feasible, performance improving

**Practical Considerations**:
- Choice of features depends on application (denotative versus expressive)
- Unitizing strategy impacts all downstream analysis
- Evaluation protocols are critical (LOSO for generalization testing)
- Human performance provides an upper bound for automatic systems

This concludes our coverage of low-level features, mid-level features, and gesture recognition. You now have a comprehensive toolkit for analyzing human movement computationally—from the basic kinematics of how bodies move through space, to the expressive qualities that convey emotion and intention.

---

## References

- Camurri, A., et al. (2004). Automatic classification of expressive hand gestures in dance. ACM Multimedia.
- De Smedt, Q., et al. (2016). Skeleton-based dynamic hand gesture recognition. CVPR Workshops.
- Devineau, G., et al. (2018). Deep learning for hand gesture recognition on skeletal data. FG.
- Dempster, W. T., & Gaughran, G. R. (1967). Properties of body segments based on size and weight. American Journal of Anatomy.
- Glowinski, D., et al. (2011). Toward a minimal representation of affective gestures. IEEE Transactions on Affective Computing.
- Laban, R. (1971). The Mastery of Movement. MacDonald and Evans.
- Larboulette, C., & Gibet, S. (2014). A comprehensive workflow for keyframe-based expressive gesture synthesis. Computers & Graphics.
- Piana, S., et al. (2016). Automated analysis of non-verbal expressive gesture. Frontiers in Psychology.
- Savitzky, A., & Golay, M. J. E. (1964). Smoothing and differentiation of data by simplified least squares procedures. Analytical Chemistry.
- Wang, X., et al. (2020). Dance emotion recognition based on Laban effort and shape analysis. Multimedia Tools and Applications.
- Wobbrock, J. O., Wilson, A. D., & Li, Y. (2007). Gestures without libraries, toolkits or training: A $1 recognizer for user interface prototypes. UIST.
- Zacks, J. M., & Swallow, K. M. (2007). Event segmentation. Current Directions in Psychological Science.

---

*End of Lecture Notes - Part 2*
*Body Movement and Gesture Course*



# Lecture 4: Facial Expression Analysis

---

## Slides 1-2: Introduction to Facial Expression

Alright, let's begin our exploration of facial expression analysis. If you look at the first slide, you'll see we're starting chapter four of our course on multimodal systems.

So what exactly is a facial expression? According to Freitas-Magalhães from 2011, a facial expression is one or more motions or positions of the muscles beneath the skin of the face. Now, that sounds like a simple definition, but it actually describes something extraordinarily complex.

Facial expressions are a form of nonverbal communication. In fact, they're one of the primary means by which we humans convey social information to each other. Think about it - before you even say a word to someone, your face is already communicating. Are you happy to see them? Are you confused? Are you angry? All of this gets transmitted through your facial muscles.

Now if you look at slide 2, you can see examples of eight different facial expressions: angry, contempt, disgust, fear, happy, sadness, surprise, and neutral. These are what Paul Ekman called the "basic emotions" - facial configurations that his research suggested might be universal across human cultures.

Here's something fascinating to consider: the human face contains approximately 43 muscles, and about 36 of them are directly involved in producing expressions. What makes these muscles unique in your entire body is that many of them attach directly to the skin rather than to bone. That's why we can make such subtle, nuanced movements with our faces - movements that would be impossible if these muscles were anchored to rigid structures.

The scientific study of facial expressions actually goes back quite far. Charles Darwin wrote about this in 1872 in his work "The Expression of the Emotions in Man and Animals." He proposed that certain emotional expressions are innate and universal - that a smile means the same thing whether you're in London or a remote village in Papua New Guinea. This idea was largely ignored for almost a century before Ekman and his colleagues revived it in the 1960s and 70s through cross-cultural studies.

From a computational perspective - which is our focus in this course - analyzing facial expressions is genuinely difficult. The face is highly deformable, meaning it can assume countless configurations. Expressions unfold dynamically over time. Every person's face is different. And then you have environmental factors like lighting, occlusion when something blocks part of the face, and different head poses. All of these make automatic analysis challenging.

---

## Slides 3-4: The Analysis Framework

Moving to slides 3 and 4, we're going to look at a framework for how we actually analyze facial expressions computationally.

Fasel and Luettin proposed this analysis framework in 2003, and it's become quite influential in the field. What's interesting - and you should note this - is that this framework doesn't differ significantly from the general layered framework we use for other multimodal systems. The same basic pipeline applies whether we're analyzing faces, body movements, or speech.

Looking at the diagram on slide 3, you can see there are several interconnected stages. Let me walk you through them.

The first stage is face acquisition. This is about locating and tracking faces in complex scenes with cluttered backgrounds. On slide 4, you can see this is highlighted - it's step one in our pipeline. This isn't trivial at all. The system needs to distinguish faces from other objects, handle situations where there are multiple faces in the scene, and maintain tracking as people move around.

There are several sources of variation that face acquisition must handle. Scale - faces appear at different sizes depending on how far someone is from the camera. Illumination - lighting conditions dramatically change how a face looks. Pose - people don't always face the camera directly. And background separation - we need to distinguish the face from whatever else is in the scene.

---

## Slides 5-6: Face Normalization and Feature Extraction

On slide 5, we move to the second major stage: face normalization. You can see it's highlighted in the diagram now.

Here's the issue: appearance changes in faces aren't always meaningful. Sometimes a face looks different just because the lighting changed, or because the person tilted their head. We don't want our system to confuse these variations with actual changes in expression. So it's good practice to normalize faces with respect to these sources of variation before we do further processing.

Face normalization typically involves geometric normalization - aligning faces to a standard pose, usually frontal, with the eyes at consistent locations. It also involves photometric normalization - adjusting for illumination variations. And scale normalization - resizing all faces to a standard size.

Why is this so important? Because many downstream algorithms assume faces are presented in a consistent format. Without normalization, the same person making the exact same expression could look completely different to the analysis system just because they moved their head or the lighting changed.

Now slide 6 shows us the next stage: facial feature extraction. Look at the diagram - you can see the feature extraction box is now highlighted in green.

Features can focus on either motion or deformation of faces. They can act locally - analyzing specific facial regions like just the eyes or just the mouth - or holistically, analyzing the entire face as a unit.

What we're doing in this stage is transforming the raw pixel representation into something more compact and informative. We might use image-based approaches that work directly with pixel intensities. We might use model-based approaches with parametric models of face appearance. We might extract deformation information - measuring how the face shape changes. Or we might extract motion information using techniques like optical flow.

The choice of features profoundly affects how well the system performs. Good features should capture information relevant to expression while being robust to irrelevant variations like who the person is, what the lighting is like, and what angle we're viewing them from.

---

## Slides 7-8: Classification and the Conceptual Framework

Slide 7 brings us to facial expression classification. You can see in the diagram that the classification box is now highlighted - it's the final stage of processing.

This stage concerns what the literature calls unitizing, annotation, and classification of facial expressions, typically by means of machine learning. We take the features we extracted and assign them to expression categories. This involves recognition - identifying which expression or Action Unit is present - and potentially interpretation - mapping those recognized expressions to higher-level constructs like emotional states.

Now slide 8 gives us a really useful conceptual framework for thinking about this whole pipeline. Look at the left side of the slide - there's a hierarchy showing different levels of abstraction.

At the bottom, we have physical signals - that's the raw video or image data from the cameras. Moving up, we have low-level features computed directly from those signals through operations like filtering or gradient computation. Then mid-level features, which represent more abstract patterns like detected Action Units. And at the top, concepts and structures - the interpreted meanings like emotional states or social signals.

This hierarchical view emphasizes something important: facial expression analysis involves progressive abstraction. We start with raw sensory data and work our way up to meaningful semantic interpretations. It's not a single step - it's a journey through multiple levels of representation.

---

## Slides 9-10: Devices and Face Detection Introduction

Slide 9 talks about devices for capturing facial expressions. You can see some pictures of different capture technologies - a Kinect sensor, a webcam, and some professional motion capture examples.

The devices used for capturing facial expressions are usually the same video-based systems used for capturing body movement. Standard RGB cameras are the most common choice because they're everywhere and they're cheap.

However - and this is important - devices and markers may need to be set up differently for facial analysis compared to body movement. The face is much smaller than the body and the movements we're trying to capture are much more subtle. In professional motion capture for films, you'll see actors wearing helmets with cameras pointed at their faces, plus reflective markers placed at key positions on the face. That's what those images in the upper right are showing.

Now slide 10 introduces face detection, which is really the foundation of everything else we're going to discuss.

Face detection is one of the first computer vision applications to achieve widespread practical success. It's the first step for any face analysis pipeline - before you can analyze an expression, you need to find the face.

It can be regarded as a special case of object-class detection. The problem can be stated simply: the input is an image, and the output is bounding boxes around all the detected faces. You can see examples on the slide - photos of groups of people with rectangles drawn around each detected face.

This sounds simple, but it's actually quite challenging. Faces in real-world images vary enormously in size, orientation, lighting, and expression. Some faces might be partially hidden. A robust detector must handle all these variations while maintaining both high precision - meaning few false alarms - and high recall - meaning few missed faces.

---

## Slides 11-12: Classification of Face Detection Algorithms

Slides 11 and 12 present different ways to classify face detection algorithms.

Slide 11 shows a classification proposed by Yang, Kriegman, and Ahuja in 2002. They identified four categories:

First, knowledge-based methods. These use pre-defined rules based on human knowledge. For example, we know a face typically has two eyes that are roughly symmetric, a nose below them, and a mouth below that. We can encode these rules and search for patterns that match. The problem is faces vary so much that rigid rules often fail.

Second, feature invariant methods. These try to find face structure features that remain robust across different poses and lighting conditions. The challenge is finding features that are truly invariant while still being distinctive enough to identify faces.

Third, template matching methods. These use pre-stored face templates and slide them across the image looking for matches. Simple rigid templates work poorly because faces vary so much, which led to more sophisticated deformable template approaches.

Fourth, appearance-based methods. These use machine learning to discover what distinguishes faces from non-faces by training on many example images. This category has become dominant and includes eigenface methods, neural networks, support vector machines, and deep learning.

Slide 12 shows a more recent classification from Zafeiriou and colleagues in 2015. They divide approaches into algorithms based on rigid templates versus algorithms using Deformable Parts-based Models, or DPMs.

Look at the images on slide 12. On the left you see a rigid template approach - a single pattern that represents a face. On the right you see a DPM approach with multiple colored dots representing different face parts and their spatial relationships.

DPMs model the face as a collection of parts - eyes, nose, mouth, and so on - with flexible spatial relationships between them. This is powerful because DPMs can handle partial occlusion. If something is blocking the mouth, the model can still detect the face based on the visible eyes and nose. They also handle pose variation better because the parts can move relative to each other.

---

## Slides 13-14: The Viola-Jones Face Detector

Now we come to slides 13 and 14, which introduce what I consider one of the most elegant algorithms in computer vision: the Viola-Jones face detector.

The Viola and Jones face detector from 2001 and 2004 was historically the first algorithm that made face detection practically feasible in real-world applications. Before this work, face detection was either too slow for real-time use or too inaccurate for practical deployment.

You can see pictures of Paul Viola and Michael Jones on slide 13, along with a sample image showing their detector in action. What they achieved was remarkable - detecting faces in real-time on hardware that by today's standards was extremely limited. Their algorithm is still widely used in digital cameras and photo organization software more than twenty years later.

Slide 14 explains the core of their approach. The detector classifies images based on the value of three kinds of features, all involving rectangles:

The two-rectangle feature computes the difference between the sum of pixel values within two adjacent rectangular regions.

The three-rectangle feature computes the sum of pixels in two outside rectangles minus the sum in a center rectangle.

The four-rectangle feature computes the difference between diagonal pairs of rectangles.

Look at the bottom of slide 14 - you can see visual examples of these rectangle patterns. The shaded regions indicate where pixel sums are computed. These might seem like simple features, but they capture important facial structure. For example, the eye region is typically darker than the cheek region below it, and this relationship can be captured by a two-rectangle feature.

---

## Slides 15-16: Integral Images

Slides 15 and 16 explain a clever computational trick that makes Viola-Jones so fast.

Looking at slide 15, you can see the rectangle features with labels A, B, C, and D marking different rectangular regions. The pixel values in white rectangles are subtracted from pixel values in grey rectangles.

Now here's the key insight on slide 16: these rectangle features can be computed very rapidly using something called an integral image.

The integral image at any location x, y contains the sum of all the pixel values above and to the left of that point, inclusive. You can see the mathematical formula on the slide.

What's beautiful about this is that the integral image can be computed in just one pass over the original image. And once you have it, any rectangular sum can be computed with just four array references, regardless of the rectangle's size.

---

## Slides 17-18: Computing Rectangle Features Efficiently

Slide 17 shows exactly how this works with a concrete example. Look at the diagram with points labeled 1, 2, 3, and 4 at the corners of a rectangle D.

The value at location 1 is the sum of all pixels in rectangle A. The value at location 2 is A plus B. Location 3 is A plus C. And location 4 is A plus B plus C plus D.

Now if we want just the sum of pixels in rectangle D, we compute: value at 4 plus value at 1 minus value at 2 minus value at 3. That's just four operations, regardless of how big rectangle D is. A rectangle that's 100 pixels by 100 pixels takes the same time to compute as one that's 2 by 2.

So any rectangular sum takes four array references. The difference between two rectangular sums - which is what our features compute - takes eight references. This is incredibly efficient.

Slide 18 addresses a problem: the Viola-Jones face detector uses a 24 by 24 pixel sub-window, and in such a sub-window there are approximately 160,000 possible rectangle features. That's way too many to compute during detection.

The solution is to select a subset of relevant features. Look at the bottom of slide 18 - you can see two face images with highlighted rectangles. On the left is a "relevant feature" that captures something meaningful about face structure. On the right is an "irrelevant feature" that doesn't help distinguish faces from non-faces. We need a way to automatically find the relevant ones.

---

## Slides 19-20: AdaBoost for Feature Selection

Slides 19 and 20 explain how Viola-Jones selects relevant features using a technique called Adaptive Boost, or AdaBoost.

On slide 19, you can see the mathematical formulation. Classification is performed using AdaBoost over the most relevant features. This is an iterative algorithm that builds a "strong" classifier as a linear combination of weighted "weak" classifiers. Each weak classifier uses just one feature.

A weak classifier has the form shown in the equation - it returns 1 if a particular feature value is below a threshold, and 0 otherwise. It's called "weak" because individually it might only be slightly better than random guessing.

The strong classifier combines many weak classifiers, each weighted according to how accurate it is. The final decision is made by taking a weighted vote of all the weak classifiers.

Slide 20 gives the algorithm in pseudocode. Let me walk through it:

We start with N images labeled as face or not-face. Each image is given a weight, and initially all weights are equal.

Then we repeat T times: In step 1, we choose the most efficient weak classifier - the single feature that best separates faces from non-faces given the current weights. We estimate a threshold to maximize accuracy and assign the classifier a weight proportional to its accuracy.

In step 2, we update the image weights to emphasize examples that were incorrectly classified. This is the key insight of boosting - by up-weighting the hard examples, we force the next weak classifier to focus on cases the previous ones got wrong.

The final strong classifier is a weighted combination of all T weak classifiers.

---

## Slides 21-22: Performance and the Cascade Architecture

Slide 21 shows what features AdaBoost actually selects. Look at the face images with overlaid rectangles - these are the first and second features selected by the algorithm.

The first feature captures the fact that the eye region is typically darker than the cheek region below it. The second feature captures the relationship between the eyes and the bridge of the nose. These make intuitive sense - they're finding the most discriminative aspects of face structure.

The slide also shows performance numbers: a 200-feature classifier achieved 95% detection rate with only 0.14 times 10 to the negative 3 false positive rate - that's about 1 false positive in 14,084 windows. It could scan all sub-windows of a 384 by 288 pixel image in 0.7 seconds on an Intel Pentium III at 700 MHz. That was impressive for 2001!

But the verdict was: good and fast, but not enough for practical use.

Slide 22 introduces the solution: a cascade of classifiers.

Here's the key observation: on average, only about 0.01% of sub-windows actually contain faces. But a single classifier spends equal time on all sub-windows. This is wasteful - we should spend most of our time only on sub-windows that might actually contain faces.

The solution is a cascade architecture, shown in the diagram on slide 22. All sub-windows enter the cascade. At each stage, a classifier makes a quick decision: if it says "definitely not a face," that sub-window is rejected immediately. Only sub-windows that pass are sent to the next stage for more careful analysis.

Early stages use very simple, fast classifiers. Later stages use more complex classifiers for the harder cases. Most non-face windows are rejected very quickly by the first few stages, so the overall system is extremely fast.

---

## Slides 23-24: Cascade Details and Final Performance

Slide 23 explains the cascade design in more detail.

A simple 2-feature classifier can achieve almost 100% detection rate with 50% false positive rate. That sounds bad - half of non-faces are incorrectly classified as faces. But it's perfect for the first stage of a cascade! We catch essentially all real faces while quickly eliminating half of the non-faces.

The second layer with 10 features tackles the "harder" negative windows that survived the first layer. And so on through the cascade. A cascade of gradually more complex classifiers achieves better detection rates than any single classifier could.

The final Viola-Jones face detector is a 38-layer cascade of classifiers, including a total of 6060 features.

Slide 24 gives the impressive final performance numbers:

Training time was on the order of weeks - this is a one-time cost.

Since a large majority of sub-windows are discarded by the first two stages, an average of only 8 features out of 6060 are evaluated per sub-window. That's remarkable efficiency.

On a 700 MHz Pentium III processor, the detector could process a 384 by 288 pixel image in about 0.067 seconds - that's roughly 15 frames per second. Real-time performance on twenty-year-old hardware!

Detection rate on the MIT test set was 77.8% with only 5 false positives.

You can see example detection results on the slide - faces successfully detected in group photos. This algorithm transformed face detection from a research curiosity into a practical technology that's now ubiquitous.

---

## Slides 25-26: Facial Landmark Localization and SDM Introduction

Now we move beyond detecting faces to a more refined task. Slide 25 introduces facial landmark localization.

Facial landmarks are defined as distinctive face locations: corners of the eyes, center of the bottom lip, tip of the nose, and so on. Look at the image on slide 25 - you can see green dots placed at dozens of key locations on the face.

Why do we care about landmarks? Taken in sufficient numbers, they define the face shape. And localization and tracking of facial landmarks can improve the accuracy of facial expression analysis. If we know exactly where the mouth corners are, for example, we can more precisely measure whether someone is smiling.

Slide 26 introduces a quite recent and common approach to facial landmark localization: the Supervised Descent Method, or SDM, proposed by Xiong and De la Torre in 2013.

SDM works by minimizing a Non-linear Least Squares function. During training, it learns a sequence of descent directions that minimizes the mean of NLS functions sampled at different points. In testing, it uses these learned descent directions to find the landmarks.

If that sounds abstract, don't worry - let's build up to it step by step.

---

## Slides 27-28: Newton's Method Background

Before we can understand SDM, we need to understand Newton's method, which is covered on slides 27 and 28.

Newton's method is a classic technique for finding the minimum of a nonlinear function. Slide 27 shows the setup: we have a function f that maps from n-dimensional space to the real numbers, and we want to find its minimum.

If f is smooth enough - technically, if it's twice continuously differentiable and can be approximated by a quadratic near the minimum - then Newton's method finds a sequence of points x_k that converge to the minimum.

The update rule is shown on the slide: the next point equals the current point minus the inverse Hessian matrix times the gradient. The Hessian is the matrix of second derivatives, and the gradient is the vector of first derivatives.

Slide 28 gives a visual intuition. You can see a curve representing function f, with a minimum point marked. Starting from some initial point x_1, Newton's method computes a descent direction and takes a step to x_2. Then it recomputes and steps to x_3, then x_4, each time getting closer to the minimum.

The term "descent direction" is key - at each step, we're computing which direction leads downhill toward the minimum.

---

## Slides 29-30: The SDM Concept

Slide 29 explains the core concept of SDM: remove the constraints about smoothness of function f and properties of the Hessian matrix by learning descent directions from data.

Here's the problem with Newton's method for landmark localization: the function we're trying to minimize - which measures how well our estimated landmark positions match the true positions - is not smooth. It's defined by image pixels, which are discrete and noisy. We can't easily compute the Hessian because the function isn't differentiable in the traditional sense.

SDM's insight is: instead of computing descent directions mathematically, learn them from training data! If we have many example faces with known landmark positions, we can learn what direction to move the landmarks to get closer to the truth.

Look at the diagram on slide 29. The top shows traditional gradient descent getting stuck because the function is non-smooth. The bottom shows SDM learning different descent directions for different starting points, allowing it to navigate the non-smooth landscape.

This also improves computational complexity because we don't have to compute the inverse Hessian at every step - we just use the pre-learned descent directions.

Slide 30 formalizes the problem. We have an image d with m pixels. We have landmark coordinates x - a vector of 2p values representing the x and y positions of p landmarks. In the training set, faces are manually annotated with true landmark coordinates x_star.

The algorithm is initialized by placing landmarks at some initial coordinates x_0 - typically based on the average face shape, positioned using the face detector's bounding box.

We also compute features h(x) - a vector describing the neighborhood of each landmark. These features capture what the image looks like around each estimated landmark position.

---

## Slides 31-32: SDM Problem Formulation

Slide 31 states the problem mathematically. We want to find the displacement delta-x that minimizes the difference between features at our estimated positions and features at the true positions.

In other words, we're looking for landmark positions where the local image features match what we'd expect to see at true landmark locations. The function f measures this mismatch.

Slide 32 shows how Newton's method would approach this. The sequence of displacements follows a formula involving the inverse Hessian of f and the Jacobian of the feature function h.

Don't worry too much about the exact formula. The key point is that this involves computing H_f inverse at each step, which is computationally expensive and requires the function to be smooth. SDM will avoid this by learning the descent directions instead.

---

## Slides 33-34: The SDM Update Rule

Slide 33 shows the reformulated update rule for SDM. We can rewrite the displacement update as:

delta-x at step k+1 equals delta-x at step k plus R_k times phi_k plus b_k

Here R_k is a matrix and b_k is a bias term that together represent the learned descent direction. The vector phi_k contains the features computed at the current landmark positions.

The crucial insight is that during testing, we don't need the ground truth phi_star. The equation is reformulated so that R_k and b_k absorb everything that depends on training data. During testing, we just compute features at current positions and apply the learned transformation.

SDM learns these matrices R_k and bias terms b_k for each iteration of the descent.

Slide 34 begins explaining the training process. We have N training faces with manually annotated landmark positions. For each face, we apply the update equation to compute descent directions.

The first training step learns R_0 and b_0. This is done by randomly assigning initial positions, computing where we'd end up after one descent step, and minimizing the error - the distance from the target positions.

---

## Slides 35-36: SDM Training Details

Slides 35 and 36 get into the mathematical details of training.

On slide 35, we formalize the randomness in initial positions by treating the initial position as a random vector. The error we make at step 0 is the squared distance between where we end up and where we should be.

Slide 36 shows that we want to minimize the expected error across all training faces and all possible initial positions. Assuming the initial positions follow a Gaussian distribution, we can approximate this expectation using Monte Carlo sampling - basically, trying many different random starting positions.

The resulting optimization problem is shown at the bottom of the slide. For each training face, for each sampled initial position, we minimize the squared distance between where we end up and where we should be.

---

## Slides 37-38: Iterative Training and Testing

Slide 37 shows the good news: this minimization is the well-known linear least squares problem, which can be solved in closed form - meaning we can compute the exact solution directly without iterative optimization.

After learning R_0 and b_0, we move to the next step and learn R_1 and b_1 using the same approach. The diagram on slide 37 shows this visually - starting from x_0, we take a step using the first learned descent direction to reach x_1, then another step to x_2, and so on, gradually approaching the true landmark positions x_3.

Slide 38 explains the general pattern. At step k, we move to position x_k and learn R_k and b_k. The next increment is computed, and we move to the next position. We need to minimize the error we make when moving to x_{k+2}, which gives us another least squares problem.

The random vectors propagate through the iterations - because initial positions are random, all subsequent positions are also random variables. But the training procedure handles this correctly.

---

## Slides 39-40: Training Completion and Testing

Slide 39 continues the training derivation. For learning R_{k+1} and b_{k+1}, we have the same structure: compute features at current positions, apply the transformation, and minimize the error relative to ground truth.

Slide 40 summarizes the complete training and testing procedure.

We can apply the same least squares method at each step, learning R_{k+1} and b_{k+1} by minimizing the appropriate objective function.

The algorithm often converges in just 4 or 5 steps - that's 4 or 5 descent iterations to go from initial guess to accurate landmark positions.

During the testing phase, the algorithm starts from an initial position x_0 and applies the sequence of learned displacements to find the actual landmark positions. We don't need any ground truth - we just compute features and apply the learned transformations.

---

## Slide 41: SDM Examples

Slide 41 shows example results from SDM on the Labeled Faces in the Wild dataset. You can see a grid of face images with green landmarks overlaid.

The results are impressive - the landmarks accurately capture the face shape even with significant variations in pose, expression, lighting, and image quality. Some faces are smiling, some are serious. Some are tilted. Some have glasses or facial hair. Yet the landmarks are consistently placed at the correct locations.

This robustness comes from the learning approach - because the descent directions are learned from many examples, they generalize to new faces that weren't in the training set.

---

## Slide 42: Face Registration

Slide 42 introduces face registration, which builds on landmark localization.

Misalignments produce large variations in face appearance, resulting in large intra-class variance. What does this mean? If we're trying to recognize that two faces show the same expression, but the faces are rotated or scaled differently, a naive algorithm might think they're different because the pixel patterns don't match up.

Face registration refers to adjusting faces with respect to a common pre-defined reference coordinate system. Once we have landmarks, we can compute a transformation that aligns those landmarks to standard positions.

The slide mentions two approaches: Procrustes transformation and piecewise affine transformation.

Procrustes transformation is a rigid alignment - it finds the best rotation, translation, and scale to align landmarks to a reference. It preserves the shape of the face.

Piecewise affine transformation is more flexible - it divides the face into triangular regions and warps each region independently. This can correct for non-rigid deformations but may distort the face if not done carefully.

You can see the effect in the images on slide 42 - the input picture on the left shows a tilted face, the face acquisition step finds the face, and the face registration step produces an aligned, frontal view.

---

## Slides 43-44: Facial Features Overview

Slides 43 and 44 return to our conceptual framework to discuss facial features in more depth.

Slide 43 shows the framework diagram again, now highlighting the low-level and mid-level feature stages. We've covered how to detect faces, localize landmarks, and register faces. Now we need to extract features that actually capture expression-relevant information.

Slide 44 presents different categorizations of facial features.

Fasel and Luettin in 2003 distinguished between features focusing on deformation versus motion. Deformation features capture static shape changes - how the face configuration differs from neutral. Motion features capture dynamic changes - how the face is moving over time.

More recently, Martinez and colleagues in 2019 grouped feature extraction methods into four categories: appearance-based features that describe texture and color, geometry-based features that describe shape, motion-based features that describe movement, and hybrid methods that combine approaches.

Features can also be local or holistic. Local features analyze specific facial regions - just the eyes, just the mouth. Holistic features analyze the entire face as a unit.

---

## Slides 45-46: Appearance Features Introduction

Slide 45 presents another useful categorization from Fei and colleagues in 2019. They identify three axes along which approaches differ:

Geometric versus appearance approaches - do we measure shape or texture?

Holistic versus local approaches - do we analyze the whole face or specific regions?

Static versus dynamic approaches - do we analyze single frames or sequences?

The diagram on slide 45 shows these as three branches stemming from "Facial Feature Extraction."

Slide 46 dives into appearance features specifically. Appearance features describe the color and texture of a facial region. Martinez and colleagues identify five major categories:

Image intensity descriptors - statistical descriptions of raw pixel values.

Descriptors computed by using filter banks - features from convolving with sets of filters.

Binarized local texture descriptors - patterns encoded as binary strings.

Gradient-based descriptors - features based on image gradients.

Two-layer descriptors - combinations of the above approaches.

The images on slide 46 show examples of filter responses - the face on the left and various filtered versions on the right, capturing different aspects of facial texture.

---

## Slide 47: Where Appearance Features Are Computed

Slide 47 shows the different spatial strategies for computing appearance features.

Looking at the four face images on the slide:

"Whole Face" shows features computed on the entire face region - a holistic approach.

"Block-based" shows the face divided into a regular grid of blocks, with features computed in each block independently - this is called tiling.

"RAPs" stands for Regions Around Points. Features are computed in small regions centered on facial landmarks - the red dots show the landmark locations.

"ROIs" stands for Regions Of Interest. Features are computed in specific regions defined by multiple points - for example, a region encompassing the entire mouth.

Each approach has tradeoffs. Whole-face features capture global patterns but may be sensitive to misalignment. Block-based features provide spatial resolution but the grid is arbitrary and may not align with facial structures. Landmark-based approaches (RAPs and ROIs) focus on anatomically meaningful regions but depend on accurate landmark localization.

---

## Slide 48: Image Intensity Descriptors

Slide 48 discusses the simplest type of appearance features: image intensity descriptors.

These consist of the histogram or other statistical descriptors of the raw pixel intensities, computed on selected face regions after registration.

The advantages are clear: they're easy to implement and quick to compute. Just take the pixels, compute some statistics, and you have features.

But the limitations are significant. They're very sensitive to all kinds of variations - lighting changes, camera differences, skin tone variations. Non-frontal head poses are problematic because the same expression looks completely different from different angles. They often need to be combined with other descriptors to work well.

Despite these limitations, intensity-based features can be useful as part of a larger system, especially when combined with careful normalization and other feature types.

---

## Slides 49-50: Filter Banks and Gabor Filters

Slides 49 and 50 introduce filter banks, a much more sophisticated approach to feature extraction.

On slide 49, you can see the idea: features are obtained by convolving a region of the input face with a set of filters called a filter bank. Look at the images - the original face on the upper left is convolved with different filters to produce the various filtered images shown.

Filter banks are particularly useful for analyzing deformations - shape and texture changes that create high spatial gradients. Different filters respond to different types of patterns in the image.

Gabor filters are commonly used in face analysis. These are special band-pass filters that can be viewed as a sinusoidal signal of a particular frequency and orientation, modulated by a Gaussian envelope.

Slide 50 shows the mathematical definition and visualizes a Gabor filter. In the upper right, you can see a sinusoid oriented at 30 degrees to the horizontal. Below that is a 2D Gaussian. And below that is the resulting Gabor filter - the sinusoid multiplied by the Gaussian.

The key parameters of a Gabor filter are its frequency (how many oscillations) and its orientation (which direction the stripes run). A filter bank typically includes Gabor filters at multiple frequencies and orientations to capture patterns at different scales and angles.

---

## Slides 51-52: Gabor Filter Banks in Action

Slide 51 shows a bank of 16 Gabor filters oriented at angles 11.25 degrees apart - so they cover the full 180 degrees of possible orientations.

When an image is passed through each filter in the bank, different edges get detected depending on their orientation. The slide shows this with a simple example: a white circle on a black background. Each filter responds most strongly to the part of the circle's edge that matches its orientation.

Slide 52 shows this more concretely. The input image on the left is a circle. The filter bank in the middle shows the 16 Gabor filters at different orientations. The output on the right shows 16 response images - each one shows where that particular filter found matching patterns.

Notice how different parts of the circle light up in different response images. The vertical filter finds the left and right edges. The horizontal filter finds the top and bottom. Each filter captures a different aspect of the edge structure.

---

## Slides 53-54: Gabor Features for Faces

Slide 53 shows how this applies to faces. The upper left shows test faces - two different expressions. The right side shows the Gabor representations - the responses when these faces are convolved with a bank of 4 Gabor filters at different orientations.

You can see how the different orientations capture different aspects of facial structure. Some filters respond strongly to the eyes, others to the mouth, others to facial contours. The magnitude values of each filter's response are concatenated into a feature vector.

Slide 54 summarizes the advantages and limitations of Gabor features.

Advantages: They can be a powerful representation if the parameters are chosen correctly. They're robust to small registration errors because they capture local patterns rather than exact pixel positions.

Limitations: The resulting feature dimensionality is very large - with multiple frequencies, multiple orientations, and dense sampling across the face, you can easily get tens of thousands of features. Computational cost is high because you need to perform many convolutions. As a result, it's hard to make Gabor features work in real-time systems.

---

## Slides 55-56: Binarized Local Texture Descriptors

Slides 55 and 56 introduce another class of appearance features: binarized local texture descriptors.

Slide 55 mentions two major approaches: Local Binary Patterns (LBP) and Local Phase Quantization (LPQ).

Many works on facial expression have successfully used LBP features in a block-based holistic manner. The slide notes that 10 by 10 blocks were found to be a suitable choice. LPQ was also frequently applied with 4 by 4 blocks.

LBP and LPQ can also be combined to get the benefits of both.

Slide 56 explains how Local Binary Patterns work. It's a texture descriptor originally proposed by Ojala and colleagues in 2002.

For each pixel, we look at its 8 neighbors. We create an 8-bit binary code by comparing each neighbor to the center pixel - if the neighbor is greater than or equal to the center, we write a 1, otherwise a 0.

Look at the example on slide 56. The center pixel has value 4. We compare each neighbor: 5 > 4, so bit 1. 8 > 4, so bit 1. 1 < 4, so bit 0. And so on around the circle.

The resulting 8-bit pattern is converted to a decimal number between 0 and 255. This becomes the LBP value for that pixel. In the example, the binary pattern 00011011 equals 23 in decimal.

The bottom of slide 56 shows an LBP-transformed face - each pixel's intensity now represents its local binary pattern code rather than its original brightness.

---

## Slides 57-58: LBP Histograms and LPQ

Slide 57 shows the final step for LBP features: computing a histogram.

After computing the LBP code for every pixel, we count how often each code occurs. This gives us a 256-dimensional histogram - one bin for each possible 8-bit pattern.

This histogram is treated as a feature vector. Different textures produce different histograms. Smooth regions have many uniform patterns. Edges and complex textures have more varied patterns.

The images on slide 57 show an example face and its corresponding LBP histogram. Different facial regions would produce different histograms, which is why block-based LBP is common - compute separate histograms for different face regions and concatenate them.

Slide 58 introduces Local Phase Quantization. LPQ uses local phase information extracted using a 2D Fourier transform computed over a neighborhood at each pixel position.

The Fourier transform represents an image patch in terms of frequencies and phases. The phase information turns out to be quite robust to blur and certain types of noise. LPQ quantizes this phase information by keeping the signs of the real and imaginary parts of selected frequency components.

The diagram shows how local phase values are mapped to one of 8 regions in the complex plane, based on their angle.

---

## Slides 59-60: LPQ Details and Binarized Descriptor Summary

Slide 59 continues the LPQ explanation. The quantized phase coefficients are represented as integer values between 0 and 255 using binary coding - similar to LBP, we get an 8-bit code for each pixel.

A histogram of these integer values from all image positions is composed and used as a 256-dimensional feature vector - again, similar to the LBP approach.

Slide 60 summarizes the advantages and limitations of binarized local texture descriptors.

Advantages: They're robust to illumination conditions because they measure relative patterns rather than absolute intensities. They're robust to misalignments. They're suitable for holistic representations. And they're computationally simple - just comparisons and histogram counting.

Limitations: LBPs are not robust to rotations - if you rotate the face, the patterns change. LBPs need correct normalization of the face to an upright position to work properly. This is why good face registration is so important for LBP-based systems.

---

## Slides 61-62: Gradient-Based Descriptors

Slides 61 and 62 introduce gradient-based descriptors.

Slide 61 shows the key idea: these methods use a histogram to encode the gradient information of the image or a portion of it.

The image on the right shows a Histogram of Oriented Gradients, or HOG, visualization. The original face is transformed into a representation showing the dominant gradient directions in each region - you can see the edge structure of the face captured by small oriented line segments.

Common approaches include HOG, SIFT (Scale-Invariant Feature Transform), and DAISY.

Slide 62 gives more detail on how HOG works. It was introduced by R.K. McConnell in 1986, though it became famous through work by Dalal and Triggs on pedestrian detection in 2005.

The steps are:

First, compute horizontal and vertical gradients by filtering with simple kernels: [-1, 0, 1] for horizontal and the same thing transposed for vertical.

Second, compute the magnitude and phase of the gradient for each pixel.

Third, divide the image into blocks and compute a histogram of gradient orientations for each block. The histogram bins represent different directions, and we weight each pixel's contribution by its gradient magnitude.

Fourth, group blocks into larger blocks and normalize the concatenated histograms. This provides some invariance to illumination.

Fifth, concatenate all the normalized histograms into one feature vector.

---

## Slides 63-64: HOG Visualization and Gradient Descriptor Summary

Slide 63 shows the HOG computation process visually.

A detection window slides over the image. At each location, gradients are computed. The window is partitioned into cells, and each pixel contributes to its cell's gradient orientation histogram. Overlapping blocks of cells are normalized together. The final descriptor is the collection of all these normalized histograms.

The result captures edge structure at multiple locations across the face, providing a rich description of face shape.

Slide 64 summarizes gradient-based descriptors.

Advantages: They're robust to misalignment because they capture local patterns. They're robust to uniform illumination variations because gradients depend on intensity differences, not absolute values. They're robust to affine transformations.

Limitations: They need to be applied locally to avoid larger gradients dominating the representation. Strong edges somewhere in the image shouldn't overwhelm subtle patterns elsewhere.

---

## Slides 65-66: Two-Layer Descriptors and Appearance Feature Summary

Slide 65 introduces two-layer appearance descriptors. These result from applying two feature descriptors in sequence, where the second operates on the output of the first.

An example is Local Gabor Binary Patterns, or LGBP. First, we compute Gabor filter responses over the image - this captures local structure at multiple orientations and scales. Then, we apply LBP over the Gabor response maps - this makes the representation more robust to misalignment and illumination while reducing dimensionality.

The combination gets benefits from both approaches: Gabor features capture rich local structure, and LBP adds robustness and compactness.

Slide 66 summarizes appearance features overall.

Advantages: They're flexible and can be extracted from the whole face or from specific regions. They're nowadays the most commonly used features in facial expression analysis.

Limitations: They can be sensitive to non-frontal head poses - most appearance features assume you're looking at the face from the front. They can be sensitive to illumination changes, though techniques like LBP help.

The images on slide 66 show examples of different appearance feature representations - original faces alongside their filtered versions, showing how different techniques capture different aspects of facial appearance.

---

## Slides 67-68: Geometric Features

Slides 67 and 68 introduce geometric features as an alternative to appearance features.

Slide 67 explains that geometric features capture statistics derived from the location of facial landmarks. Most facial muscle activations result in landmark displacement, so by tracking where landmarks move, we can infer what muscles are active.

Geometric features measure variations in shape, location, and distance of relevant facial regions - mouth, eyes, eyebrows, nose, and so on.

The images on slide 67 show different ways to represent geometric information: raw landmark locations, distances between landmarks, and angles formed by landmarks.

Slide 68 gives a concrete example from Majumder and colleagues in 2013. They use an analytical model consisting of 23 facial points.

The table shows what these points represent: 4 times 2 points for the eyebrows (two extreme corners plus upper and lower mid points for each), 4 times 2 points for the eyes in the same arrangement, 3 points for the nose, and 4 points for the lips.

This gives a sparse but meaningful representation of facial structure that can be tracked across frames.

---

## Slides 69-70: Geometric Feature Examples

Slide 69 continues the Majumder example. From the 23-point model, they compute 26-dimensional geometric features.

These consist of the displacement of 8 eyebrow points and 4 lip points along both x and y directions, plus projection ratios of the two eyes. Displacement is calculated using the neutral expression as a reference - we measure how much each point moved from its position in the neutral face.

The images show how different expressions cause different patterns of displacement. When someone raises their eyebrows in surprise, those points move up. When someone smiles, the lip corners move up and out.

Slide 70 shows another example from Zhao and colleagues in 2013. They use 27 geometric features consisting of Euclidean distances between landmarks and corner angles spanned by landmarks.

The image shows a baby's face with landmarks and baseline measurements. Euclidean distances are divided into horizontal and vertical lines according to their directions, and both are normalized by their baselines. This normalization makes the features robust to face size variations. Angles are analyzed via linear statistics.

---

## Slide 71: Geometric Feature Summary

Slide 71 summarizes geometric features.

Advantages: They're easy to register - once you have landmarks, computing distances and angles is straightforward. They're independent of lighting conditions - a distance between two points doesn't change when the lighting changes.

Limitations: They're unable to capture facial expressions that don't cause landmark displacements. Some expressions involve texture changes - like wrinkling of the forehead or dimpling of the cheeks - without moving landmarks significantly.

Because of these limitations, geometric features are often combined with appearance features. The combination captures both the shape changes that geometric features detect and the texture changes that appearance features detect.

The images on slide 71 show faces with overlaid geometric landmarks and connections, illustrating how the same person's geometric structure changes with different expressions.

---

## Slides 72-74: Motion Features

Slides 72 through 74 introduce motion features, which capture dynamic information from video sequences.

Slide 72 explains that motion features capture flexible deformations of the skin caused by contraction of facial muscles. Unlike geometric or appearance features that analyze single frames, motion features analyze how the face changes over time.

Motion extraction methods include difference images and optical flow. The diagram shows arrows indicating the direction of facial movement during an expression - eyebrows moving up, mouth corners moving back, and so on.

Slide 73 explains difference images, also called delta-images. These are computed as the pixel-wise difference between the current frame and an expressionless-face frame of the same person.

The first frame of a facial expression sequence is often used as the neutral reference. By subtracting it from subsequent frames, we get images that highlight what changed - areas that moved or changed in intensity show up brightly, while static areas appear dark.

The images on slide 73 show examples: original faces on the left, and difference images on the right showing where changes occurred.

Slide 74 introduces Motion History Images (MHIs) and Motion Energy Images (MEIs), originally developed for full-body movement analysis.

MEIs are binary images indicating whether any pixel change occurred over a window of frames - they show where motion happened.

MHIs go further by encoding when motion happened. Recent motion appears bright, while older motion fades toward black over time. This temporal encoding can help distinguish expressions that involve the same regions but different timing.

---

## Slides 75-78: Optical Flow Foundation

Slides 75 through 78 develop the theory of optical flow, a fundamental technique for motion analysis.

Slide 75 introduces the concept. Optical flow was introduced by psychologist James Gibson in the 1940s to describe the visual stimulus experienced by animals moving through the world.

Formally, it's defined as the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by relative motion between observer and scene. More practically, it can be thought of as the distribution of apparent velocities of brightness patterns in an image - a measure of the direction and intensity of motion for every pixel.

Slide 76 develops the mathematics. Consider a pixel at location (x_0, y_0) with intensity I at time t_0. At time t_0 + delta-t, assume this pixel has moved by delta-x and delta-y.

The brightness constancy constraint states that the intensity of the pixel doesn't change as it moves - the same point in the world should have the same brightness regardless of where it appears in the image.

Using Taylor series expansion and assuming small movements, we can derive a relationship between the spatial and temporal intensity gradients and the velocity.

Slides 77 and 78 complete the derivation. The optical flow equation is:

I_x times v_x plus I_y times v_y equals negative I_t

Where I_x and I_y are the spatial derivatives of intensity, I_t is the temporal derivative, and v_x and v_y are the velocity components we want to find.

The problem is that this is one equation with two unknowns - v_x and v_y. We cannot solve it as stated. This is called the aperture problem: looking through a small aperture at a moving edge, you can only determine the component of motion perpendicular to the edge, not the full motion vector.

---

## Slides 79-82: Solving Optical Flow

Slides 79 through 82 explain how we actually compute optical flow despite the aperture problem.

Slide 79 notes that optical flow methods introduce additional constraints to estimate the actual flow. These methods can be classified by what constraints they add: phase correlation methods, block-based methods, differential methods, and discrete optimization methods.

Slides 80 through 82 explain the Lucas-Kanade method, one of the most commonly used differential methods, proposed in 1981.

The additional constraint in Lucas-Kanade is that pixels in a neighborhood all have the same velocity. This makes sense for faces - if a region of the cheek is moving, all the pixels in that region probably move together.

With n pixels in a neighborhood, we get n equations but still only two unknowns (v_x and v_y). This system is over-determined, so we use least squares to find the best compromise solution.

Slide 81 shows the matrix formulation. We stack the spatial derivatives into a matrix A, the velocity components into vector v, and the negative temporal derivatives into vector b. The least squares solution minimizes the squared error in Av = b.

Slide 82 gives the closed-form solution: v equals (A-transpose A) inverse times A-transpose times b.

The solution requires that A-transpose A be invertible, which happens when there's sufficient texture variation in the neighborhood. In uniform regions without texture, optical flow cannot be computed reliably - another manifestation of the aperture problem.

---

## Slides 83-84: Optical Flow for Faces and Motion Feature Summary

Slide 83 shows optical flow applied to faces. The images show facial expressions on the left and corresponding optical flow fields on the right - arrows indicating the direction and magnitude of motion at each point.

The average normalized velocity of each pixel, computed over time during a facial expression, can be used as a feature vector. With two velocity components per pixel, this produces very large feature vectors, but they capture rich information about expression dynamics.

Slide 84 summarizes motion features.

Advantages: They're less person-specific than appearance features. Different people may have different facial appearances, but the motion patterns for expressions are more similar across individuals.

Limitations: They require full elimination of rigid head motion - if the person is moving their head while making an expression, we need to separate the head motion from the facial motion. They're affected by misalignment and varying illumination conditions.

The images on slide 84 show dramatic optical flow patterns for a surprised expression, with arrows radiating outward from the center of the face as the eyebrows raise and mouth opens.

---

## Slides 85-86: Introduction to Face Action Units

Now we move from low-level features to mid-level features. Slides 85 and 86 introduce Face Action Units.

Slide 85 shows our conceptual framework again, now highlighting the mid-level feature stage and the facial expression classification stage. We're moving up the abstraction hierarchy.

Slide 86 introduces the key concept: facial expression can be described by means of discrete face action units and their dynamics.

The images on slide 86 show examples of different Action Units - individual muscle movements that combine to create expressions. AU1 raises the inner brow. AU4 lowers the brow. AU7 tightens the eyelid. AU12 pulls the lip corners up in a smile. And so on.

Notice how these are atomic units - each one describes a single, identifiable muscle action. Complex expressions are combinations of multiple AUs. A smile typically involves AU6 (cheek raiser) and AU12 (lip corner puller). Fear involves AU1, AU2, AU4, AU5, AU20, AU25, AU26, and AU27.

---

## Slides 87-88: The Facial Action Coding System

Slides 87 and 88 explain the Facial Action Coding System, or FACS.

Slide 87 provides the background. FACS is a system to taxonomize human facial movements by their appearance. It encodes movements of individual facial muscles.

The system is based on work originally developed by Swedish anatomist Carl-Herman Hjortsjö. It was later adopted and expanded by Paul Ekman and Wallace Friesen, first published in 1978 and revised in 2002.

The images on slide 87 show a face morphing through different AU configurations, demonstrating how different muscle combinations produce different appearances.

Slide 88 details what FACS includes:

Action Units (AUs): 32 atomic facial muscle actions performed by either individual muscles or groups of muscles. These are the building blocks.

Action Descriptors (ADs): 14 additional items accounting for head pose, gaze direction, and miscellaneous actions like jaw thrust, blow, and bite.

Intensities: Each AU can be coded with an intensity from A (trace) through E (maximum). This captures not just which muscles are active but how strongly they're activated. A slight smile and a broad grin both involve AU12, but at different intensities.

---

## Slides 89-90: FACS Morphology and Dynamics

Slide 89 explains that FACS describes both morphology and dynamics of a facial display.

Morphology refers to facial configuration - what the face looks like at a given moment. This is observed from static frames and encoded in the detected AUs.

Dynamics refers to the temporal evolution from one facial display to another. This is observed from video and encoded in the timing, duration, and speed of AU activation and deactivation.

Facial dynamics can be explicitly analyzed by detecting temporal phase boundaries: neutral (no activation), onset (activation beginning), apex (maximum activation), and offset (activation ending). These phases carry important information - for example, genuine smiles have different timing characteristics than posed smiles.

Slide 90 emphasizes the importance of FACS. Using this system, nearly any anatomically possible facial expression can be coded by deconstructing it into specific Action Units and their temporal segments.

Critically, Action Units are independent of interpretation. They describe muscle movements, not emotions or intentions. This separation allows the same AU coding to be used for different downstream tasks - emotion recognition, lie detection, clinical assessment, and more.

However, FACS is usually applied by manual annotators. It takes over 100 hours of training to achieve minimal competency as a FACS coder, and each minute of video takes approximately one hour to score. This is why automatic AU detection is so valuable.

---

## Slides 91-92: AU Catalog

Slides 91 and 92 provide a catalog of Action Units.

Slide 91 shows a comprehensive table of upper and lower face AUs. Upper face AUs include AU1 (inner brow raiser), AU2 (outer brow raiser), AU4 (brow lowerer), AU5 (upper lid raiser), AU6 (cheek raiser), and AU7 (lid tightener). The table continues with lid droop, slit, eyes closed, squint, blink, and wink.

Lower face AUs include AU9 (nose wrinkler), AU10 (upper lip raiser), AU11 (nasolabial deepener), AU12 (lip corner puller), AU13 (cheek puffer), AU14 (dimpler), and many more related to lip and jaw movements.

Some AUs are marked with asterisks, indicating they're defined differently in different versions of FACS or have special properties.

Slide 92 shows these AUs on actual faces. You can see AU4 (brow lowerer), AU6 (cheek raise), AU7 (lids tight), AU9 (nose wrinkle), AU12 (lip corner puller), AU20 (lip stretch), AU25 (lips part), AU26 (jaw drop), and AU43 (eye closure) demonstrated on photographs of real people expressing various emotions.

The combination of AU4 + AU6 + AU7 + AU9 + AU12 + AU25 + AU26 creates the expression of intense laughter visible in the rightmost image.

---

## Slides 93-94: Automatic AU Coding

Slides 93 and 94 discuss automatic coding of Action Units.

Slide 93 notes that manually coding AUs is extremely time-consuming. This motivated the development of automatic systems for coding AUs from video.

These systems typically include face detection, computation of facial features, and machine learning. The diagram shows the pipeline: preprocessing (face detection, tracking, normalization), feature extraction (appearance-based, geometry-based, motion-based, or hybrid), and machine analysis (AU classification, recognition, temporal segment detection, or intensity estimation).

Slide 94 distinguishes four major problems in automatic AU coding:

AU detection: produces a binary frame-level label per target AU - is it active or not?

AU intensity estimation: infers frame-level intensity labels (A through E) per AU.

AU temporal segment detection: infers frame-level temporal phase labels (neutral, onset, apex, offset) per AU.

AU classification: deals with pre-segmented AU activation episodes - less common nowadays.

The table shows the output space for each problem. Detection outputs plus or minus 1 (active or not). Intensity estimation outputs 0-5 as discrete classes or continuous values. Temporal segment detection outputs 0-3 for the four phases.

---

## Slides 95-96: AU Detection Example

Slides 95 and 96 present a specific example of AU detection: the Automated Facial Action Coding System by Hamm, Kohler, Gur, and Verma, published in the Journal of Neuroscience Methods in 2011.

Slide 96 shows the face detection and landmark localization step. They use the Viola-Jones face detector to find faces, then apply an Active Shape Model to localize 159 landmarks. The images show detected faces with landmarks overlaid.

Active Shape Models, or ASMs, are statistical models of shape learned from training data. They represent the face as a set of landmarks whose relative positions are constrained by the model. During fitting, the model deforms to match the face while staying within the learned space of plausible face shapes.

---

## Slides 97-98: Feature Extraction in the AU Detection Example

Slides 97 and 98 show the feature extraction approach.

Slide 97 explains the geometric features. They compute the deformation of the face mesh relative to a neutral state. The face is represented as a mesh of connected landmarks, and they measure how the edges of this mesh compress or expand during expressions.

The images show the mesh with color-coded edges - green indicates compression, red indicates expansion. For example, when someone raises their eyebrows, the edges on the forehead stretch vertically, while when someone smiles, edges around the mouth corners stretch horizontally.

Slide 98 shows the appearance features. Gabor filters are applied to relevant facial regions - the blue rectangles on the face image show where. The difference between filtered values in the current frame and filtered values in a neutral reference frame provides features that capture texture changes associated with expressions.

The right side shows the filter bank and example responses, illustrating how different Gabor orientations capture different aspects of facial texture.

---

## Slides 99-100: AU Detection Results and Other Approaches

Slide 99 presents the results of this AU detection system.

Training set size: 3,419 faces.

They trained 15 AdaBoost classifiers, one for each of 15 AUs. Four AUs were discarded due to insufficient training samples.

Accuracy ranged from 87% for lid tightener (AU7) to over 99% for several AUs. The overall accuracy across all AUs was 95.9%.

The table shows results for each AU: inner brow raiser at 95.8%, outer brow raiser at 97.8%, brow lowerer at 91.0%, and so on. These are strong results, though it's worth noting they were achieved on a controlled dataset - performance in unconstrained real-world conditions would likely be lower.

Slide 100 shows that many other approaches have been explored. The table lists studies from 1998 through 2010, showing the features used (dense-flow tracking, tracked feature points, eigenfaces, Gabor filters, manifold learning, Haar features, active appearance models, motion history images, normalized pixels, free-form deformations) and the classifiers used (hidden Markov models, quadratic discriminant classifiers, nearest neighbor, boosting plus SVM, Bayesian methods, deep belief networks, and more).

This diversity of approaches reflects the difficulty of the problem and the many possible solutions.

---

## Slides 101-102: Facial Expression Recognition

Slides 101 and 102 move from AU detection to facial expression recognition - interpreting what expressions mean.

Slide 101 shows our conceptual framework with the interpretation stage highlighted. We've moved to the highest level of abstraction.

Slide 102 introduces facial expression recognition: interpretation of facial expression via algorithms.

The slide notes that depending on context, facial expressions may have varied communicative functions. They can regulate conversations by signaling turn-taking. They can convey biometric information - your face identifies who you are. They can express intensity of mental effort. And they can signal emotion.

Most existing facial expression recognizers focus on emotion. The simple diagram shows an image of an angry child going into a recognizer, which outputs "Anger."

---

## Slides 103-104: Mapping AUs to Expressions

Slides 103 and 104 show how Action Units map to expressions.

Slide 103 provides a table showing which AUs are associated with different states:

Anger involves AUs 4, 5, 7, 10, 17, and 22-26.
Disgust involves AUs 9, 10, 16, 17, 25, and 26.
Fear involves AUs 1, 2, 4, 5, 20, 25, 26, and 27.
Happiness involves AUs 6, 12, and 25.
Sadness involves AUs 1, 4, 6, 11, 15, and 17.
Surprise involves AUs 1, 2, 5, 26, and 27.

Also listed are pain (a complex pattern of 10 AUs), cluelessness, and speech patterns.

Slide 104 shows this visually with photographs. Each row shows an expression with the corresponding AU codes: happiness is 6+7+12+25+26, sadness is 1+4+6+15+17, interest is 1+2+12, shame is 54+64 (head down plus eyes down), and so on for pain, surprise, sympathy, disgust, embarrassment, and fear.

This mapping from AUs to expressions is what enables automatic emotion recognition from automatically detected AUs.

---

## Slides 105-106: Recognition Approaches

Slides 105 and 106 describe different approaches to facial expression recognition.

Slide 105 discusses frame-based expression recognition. This approach doesn't use temporal information - it analyzes single frames with or without a reference frame.

Many approaches classify AUs as a preliminary step, then map AUs to expressions. Both categorical models (discrete emotion categories) and dimensional models (continuous arousal-valence space) are used.

Various machine learning methods have been applied: neural networks, support vector machines, linear discriminant analysis, Bayesian networks, rule-based classifiers. More recently, deep learning approaches perform feature extraction and recognition jointly.

Slide 106 discusses sequence-based expression recognition. This approach uses temporal information from sequences of images to recognize expressions.

Techniques like hidden Markov models, recurrent neural networks, and rule-based classifiers handle the temporal aspect. Most sequence-based recognizers use categorical models, though continuous models using deep bidirectional LSTMs have also been developed.

The sequence-based approach can capture the dynamics of expressions - how they unfold over time - which provides additional information beyond what single frames reveal.

---

## Slides 107-108: Open Challenges

Slides 107 and 108 conclude with open challenges in facial expression analysis.

Slide 107 lists several important challenges:

Not just basic emotions - we need to recognize expressions of complex mental states like fatigue, frustration, pain, mood, personality traits, drowsiness, emotional attachment, and indices of psychiatric disorders.

Micro-expressions - these are brief facial expressions lasting only fractions of a second that people make when trying to conceal feelings. They're subtle and extremely difficult to detect automatically.

Reliable ground truth - many datasets exist, but manual annotation is difficult and reliability of labels is often unknown.

Robust recognition in naturalistic environments - most research uses controlled lab conditions, but real applications need to work in the wild.

Slide 108 presents a table listing properties of an ideal facial expression analysis system. Under robustness: deal with different ages, genders, ethnicities; handle lighting changes; handle large head motion; handle occlusion; handle different image resolutions; recognize all possible expressions; recognize different intensities; recognize asymmetrical expressions; recognize spontaneous expressions.

Under automatic and real-time processing requirements: automatic face acquisition, feature extraction, and expression recognition, all in real-time.

Under autonomic process requirements: output recognition with confidence measures and adapt to different difficulty levels based on input quality.

These properties represent aspirational goals - current systems achieve some but not all of them. The field continues to advance, with deep learning bringing significant recent progress, but substantial challenges remain for truly robust, general-purpose facial expression analysis.





# LECTURE NOTES: SPEECH PROCESSING
## Complete Guide to Speech Signal Analysis and Recognition

---

## PART 1: INTRODUCTION AND CONCEPTUAL FRAMEWORK
### [SLIDES 1-2]

Welcome everyone to our lecture on Speech Processing. Today we're going to explore speech from multiple perspectives, building up from the most fundamental physical signals all the way to high-level concepts and structures.

Let me draw your attention to our conceptual framework here on slide 2. Think of this as a pyramid or hierarchy. At the bottom, we have **physical signals**—the raw acoustic waveforms that our microphones capture. Moving up, we extract **low-level features** like amplitude, frequency, and spectral characteristics. These feed into **mid-level features** such as voice activity detection and speech segmentation. Finally, at the top, we arrive at **concepts and structures**—the actual linguistic content, words, and meaning.

This hierarchical framework will guide our entire discussion today. Each level builds upon the one below it, and understanding this architecture is essential for grasping how modern speech recognition systems work.

---

## PART 2: THE PHYSICS OF SOUND
### [SLIDES 3-4: Sound Fundamentals]

Let's begin with the most fundamental question: What exactly is sound?

Sound is a vibration that propagates as an audible wave of pressure through a medium—typically air, but it could be water, solid materials, or any other substance that can transmit mechanical waves. When you speak, your vocal cords vibrate, creating pressure variations in the air that travel outward like ripples in a pond.

The simplest type of sound is what we call a **pure sound** or pure tone. Mathematically, we describe it as:

$$s_P(t) = A \cdot \sin(2\pi ft + \varphi)$$

Where:
- **A** is the amplitude (how loud the sound is)
- **f** is the frequency (how many oscillations per second, measured in Hertz)
- **φ** (phi) is the phase (where in the cycle the wave starts)
- **t** is time

For example, a pure tone at 220 Hz with amplitude 1 and zero phase would produce a simple, smooth sine wave oscillating 220 times per second. This is actually the pitch of the note A3 on a piano, one octave below concert A.

### [SLIDES 5-6: Complex Sounds]

Now here's the crucial insight: **most sounds we encounter are NOT pure sounds**. They're what we call **complex sounds**, which are combinations of many sound waves at different frequencies all summed together.

Mathematically, a complex sound looks like this:

$$s_C(t) = A_1 \cdot \sin(2\pi f_1 t + \varphi_1) + A_2 \cdot \sin(2\pi f_2 t + \varphi_2) + ... + A_N \cdot \sin(2\pi f_N t + \varphi_N)$$

Think about it—when you hear a violin and a clarinet playing the same note, they sound completely different. Why? Because even though they might share the same fundamental frequency, they have different combinations of additional frequencies layered on top. This is what gives each instrument its unique timbre or "color."

The example on slide 6 shows a complex sound with three components: 220 Hz, 440 Hz, and 880 Hz. Notice how the resulting waveform is much more intricate than a simple sine wave—it has a richer, more textured shape.

### [SLIDES 7-8: Fourier's Insight and Harmonic Series]

This brings us to one of the most important concepts in signal processing, courtesy of the French mathematician Jean-Baptiste Joseph Fourier (1768-1830).

Fourier made a remarkable discovery: **any periodic sound wave can be decomposed into a sum of pure sinusoidal sounds whose frequencies are integer multiples of a fundamental frequency**. This is called a **harmonic series**.

For a periodic complex sound, we can write:

$$s_H(t) = A_0 + A_1 \cdot \sin(2\pi f_0 t + \varphi_1) + A_2 \cdot \sin(2\pi(2f_0)t + \varphi_2) + A_3 \cdot \sin(2\pi(3f_0)t + \varphi_3) + ...$$

The frequency $f_0$ is called the **fundamental frequency**, and its period is $T = 1/f_0$. All the other frequency components—at $2f_0$, $3f_0$, $4f_0$, and so on—are called **harmonics**.

This is profound because it means we can analyze any complex periodic sound by breaking it down into its constituent pure tones. This decomposition is the foundation of spectral analysis and underlies virtually all modern audio processing.

### [SLIDES 9-10: Non-Periodic Sounds and White Noise]

But what about sounds that aren't periodic? Non-periodic sounds also consist of sums of pure sounds, but here's the key difference: their frequency components don't follow the harmonic relationship. They're not constrained to be integer multiples of any fundamental frequency.

We call these non-harmonic frequency components **partials** rather than harmonics, and there can be infinitely many of them at arbitrary frequencies.

The classic example is **white noise**—that hissing sound you might hear from a TV tuned to a dead channel. White noise is defined as having equal intensity at ALL frequencies. Ideally, it contains every possible frequency with the same amplitude. The waveform of white noise looks completely random and chaotic, with no discernible pattern.

### [SLIDES 11-12: Frequency Spectrum]

How do we visualize these frequency components? We use the **frequency spectrum**.

For a periodic sound (harmonic sound), the spectrum displays vertical lines at the frequencies of each harmonic. You'll see discrete peaks at $f_0$, $2f_0$, $3f_0$, and so on. The height of each line indicates the amplitude of that component.

For a non-periodic sound like white noise, the spectrum looks completely different—it's spread across all frequencies without any prominent peaks. It's essentially a flat, noisy distribution.

Understanding how to read and interpret frequency spectra is absolutely essential for speech processing, as we'll see when we discuss spectrograms and formants later.

---

## PART 3: PHONETICS - THE SOUNDS OF SPEECH
### [SLIDES 13-14: Phones and Phonetic Alphabets]

Now let's transition from general acoustics to speech specifically. The basic units of speech sounds are called **phones** (not to be confused with telephones!). Phones serve as the fundamental building blocks for phonetic analysis.

Any word's pronunciation can be represented as a string of phones. For instance, the word "cat" consists of three phones: [k], [æ], [t].

To transcribe these sounds consistently across all languages, linguists developed the **International Phonetic Alphabet (IPA)**. The IPA provides a unique symbol for every distinctive sound that humans can produce. It's like a universal alphabet for pronunciation.

For computational work, especially with American English, there's a simpler alternative called **ARPAbet**. ARPAbet uses ASCII characters instead of special symbols, making it easier to work with in computer systems. For example, the IPA symbol [æ] becomes "AE" in ARPAbet.

Note that phonetic transcriptions are enclosed in square brackets. So "budgerigar" would be transcribed as [ˈbʌdʒərɪɡɑːr] in IPA.

### [SLIDES 15-16: How Humans Produce Speech]

How do we actually produce these phones? Let me walk you through the vocal apparatus.

The process begins when air is expelled from the **lungs**. This air travels up through the **trachea** (windpipe) and into the **larynx** (voice box). Inside the larynx, we have the **vocal folds** (sometimes called vocal cords)—two small muscular folds that can be positioned in different ways.

Here's what's crucial: when the vocal folds are close together, they vibrate as air passes through them. When they're far apart, they don't vibrate.

This distinction gives us two categories of sounds:
- **Voiced sounds**: Made with the vocal folds vibrating. Examples include [b], [d], [g], [v], [z], and ALL English vowels. If you place your fingers on your throat while saying "zzzzz," you'll feel the vibration.
- **Unvoiced (voiceless) sounds**: Made without vocal fold vibration. Examples include [p], [t], [k], [f], [s]. Try saying "ssss"—no vibration.

### [SLIDES 17-18: Consonants vs. Vowels and Place of Articulation]

Let's distinguish between consonants and vowels:

**Consonants** are produced by restricting or completely blocking the airflow somewhere in the vocal tract. They can be either voiced or unvoiced.

**Vowels** have much less obstruction—the air flows relatively freely. They're almost always voiced and tend to be louder and longer-lasting than consonants.

Consonants are classified by their **place of articulation**—where the maximum restriction of airflow occurs:

1. **Labial**: The two lips come together (like [p], [b], [m])
2. **Dental**: Tongue against the teeth (like [θ] in "think")
3. **Alveolar**: Tongue against the alveolar ridge—that bony ridge just behind your upper teeth (like [t], [d], [n], [s])
4. **Palatal**: Tongue against the hard palate (like [ʃ] in "ship")
5. **Velar**: Tongue against the velum (soft palate) at the back of the mouth (like [k], [g])
6. **Glottal**: The vocal folds themselves come together (like [h] or the glottal stop [ʔ])

### [SLIDES 19-20: Manner of Articulation]

Consonants are also classified by their **manner of articulation**—how the restriction is made:

1. **Stops (plosives)**: Airflow is completely blocked for a short time, then released explosively ([p], [t], [k], [b], [d], [g])
2. **Nasals**: Air passes through the nasal cavity instead of the mouth ([m], [n], [ŋ] as in "sing")
3. **Fricatives**: Airflow is constricted but not completely blocked, creating turbulent, noisy sound ([f], [v], [s], [z], [ʃ], [ʒ]). Higher-pitched fricatives like [s] and [ʃ] are called **sibilants**
4. **Approximants**: Articulators come close together but not close enough to create turbulence ([w], [j] as in "yes", [r], [l])
5. **Taps/flaps**: A quick, single contact of the tongue against the alveolar ridge (like the American English pronunciation of "t" in "butter")

The IPA chart on slide 20 systematically organizes all pulmonic consonants by place of articulation (columns) and manner of articulation (rows), with voiced sounds on the right and voiceless on the left within each cell.

### [SLIDE 21: Vowels]

Vowels are characterized by the position of the tongue:

**Front vs. Back**: Where is the tongue raised? For front vowels like [i] (as in "beet"), the tongue is raised toward the front of the mouth. For back vowels like [u] (as in "boot"), it's raised toward the back.

**High, Mid, Low**: How high is the tongue raised? High vowels ([i], [u]) have the tongue close to the roof of the mouth. Low vowels ([æ], [ɑ]) have the tongue low. Mid vowels ([e], [o]) are in between.

The tongue positions on slide 21 show this clearly: [iy] (beet) is high-front, [ae] (bat) is low-front, and [uw] (boot) is high-back.

---

## PART 4: DIGITAL AUDIO RECORDING
### [SLIDES 22-24: The Recording Chain]

Now that we understand speech sounds, how do we capture them digitally? Let's trace through the recording chain.

It all starts with a **microphone**. A microphone is a transducer—it converts one form of energy (acoustic pressure variations) into another form (electrical voltage variations). The sensitive element inside, called the **capsule** or **element**, contains a **diaphragm** that moves in response to sound pressure.

Different microphones use different transduction principles:
- **Carbon microphones**: Granules change resistance with pressure (old telephone technology)
- **Piezoelectric**: Crystals generate voltage when stressed
- **Dynamic**: Coil moves in a magnetic field (robust, common in live sound)
- **Ribbon**: Thin metal ribbon moves in a magnetic field (delicate, warm sound)
- **Condenser/electret**: Capacitance changes with diaphragm movement (high quality, studio standard)

### [SLIDES 25-26: Amplification and Digital Conversion]

The electrical signal from the microphone is typically quite weak, so it needs **amplification**. An amplifier increases the amplitude of the signal. The amount of amplification is measured by **gain**—the ratio of output amplitude to input amplitude.

Ideally, an amplifier would perfectly replicate the input signal at a higher amplitude. In reality, all amplifiers introduce some **distortion**—slight changes to the waveform shape.

After amplification, we need to convert the analog (continuous) signal to digital (discrete) form using an **Analog-to-Digital Converter (ADC)**. But first, an important step: **antialiasing filtering**. This low-pass filter removes high frequencies that would cause problems during digitization (due to a phenomenon called aliasing, which we'll discuss).

The ADC performs three operations:
1. **Sampling**: Measuring the signal value at discrete time points
2. **Quantization**: Rounding each measurement to the nearest allowed level
3. **Coding**: Representing each quantized value as a binary number

### [SLIDES 27-28: Recording Parameters and Example]

Different applications use different recording parameters:

| Application | Sample Rate | Bits/Sample | Channels | Data Rate | Frequency Band |
|-------------|------------|-------------|----------|-----------|----------------|
| Telephone | 8 kHz | 8 bits | Mono | 8 kB/sec | 0.2-3.4 kHz |
| AM Radio | 11.025 kHz | 8 bits | Mono | 11 kB/sec | 0.1-5.5 kHz |
| FM Radio | 22.05 kHz | 16 bits | Stereo | 88.2 kB/sec | 0.02-11 kHz |
| CD | 44.1 kHz | 16 bits | Stereo | 176.4 kB/sec | 0.005-20 kHz |
| DAT | 48 kHz | 16 bits | Stereo | 192 kB/sec | 0.005-20 kHz |

The sample rate must be at least twice the highest frequency you want to capture (Nyquist theorem). CD quality at 44.1 kHz can faithfully reproduce frequencies up to about 20 kHz—the upper limit of human hearing.

The waveform example on slide 28 shows the sentence "She just had a baby" with phone labels. Notice how different phones have different acoustic signatures—vowels show clear periodic patterns, while fricatives like [sh] and [s] appear noisy.

---

## PART 5: FRAME-BASED ANALYSIS
### [SLIDES 29-32: Audio Frames and Windowing]

Here's a key insight: we don't analyze entire audio files at once. The content of speech changes rapidly—sounds come and go within fractions of a second. To capture these dynamics, we perform analysis on short **frames** or **windows** of the signal.

Each frame represents a snapshot of the signal at a particular moment. Key parameters include:

1. **Frame length**: The duration of each window, typically 20-40 milliseconds (often 25 ms). At 16 kHz sampling, a 25 ms frame contains 400 samples.

2. **Hop factor**: The distance between consecutive frames. If frames are 25 ms long but spaced 10 ms apart, they overlap significantly.

3. **Overlapping**: When the hop factor is shorter than the frame length, frames overlap. **Half-frame overlapping** is common—meaning 50% of each frame overlaps with its neighbors.

4. **Frame rate**: How many frames we get per second. With 10 ms hop, we get 100 frames per second (100 Hz feature rate).

**Example calculation** (slide 32):
- Audio sampling rate: 44,100 Hz
- Frame duration: 40 ms = 44,100 × 0.04 = 1,764 samples per frame
- Hop factor: 20 ms = 882 samples (half overlapping)
- Feature sample rate: 1 / 20 ms = 50 Hz

### [SLIDES 33-34: Window Functions]

Simply chopping the signal into rectangular segments (multiplying by a rectangular window) causes problems, especially for frequency analysis. The abrupt edges create spurious frequency components called **spectral leakage**.

To reduce this artifact, we apply smoother window functions:

**Hamming window**:
$$w(n) = 0.54 - 0.46 \cos\left(\frac{2\pi n}{N-1}\right)$$

**Blackman window**:
$$w(n) = a_0 - a_1 \cos\left(\frac{2\pi n}{N-1}\right) + a_2 \cos\left(\frac{4\pi n}{N-1}\right)$$

where typically $a_0 = 0.42$, $a_1 = 0.5$, $a_2 = 0.08$ (for $\alpha = 0.16$).

These windows taper smoothly to zero at the edges, reducing spectral artifacts. The Hamming window is most commonly used in speech processing.

---

## PART 6: LOW-LEVEL ACOUSTIC FEATURES
### [SLIDES 35-36: Amplitude Measurement]

The simplest feature we can extract is amplitude—how loud is the signal? The most common measure is **Root Mean Square (RMS)**:

$$RMS_n = \sqrt{\frac{1}{N} \sum_{i=n-N+1}^{n} s_i^2}$$

Where $N$ is the frame size and $s_i$ are the signal samples.

RMS gives us a single value representing the "average" amplitude within each frame. It's more meaningful than peak amplitude because it accounts for the full waveform shape.

### [SLIDES 37-39: Frequency and Autocorrelation]

To estimate frequency, we use the **autocorrelation function**. Autocorrelation measures how similar a signal is to a time-shifted version of itself:

$$R(k) = \sum_{i=-\infty}^{+\infty} s_i \cdot s_{i+k}$$

For a frame of N samples, the **short-time autocorrelation** is:

$$R_n(k) = \sum_{i=n-N+1}^{n-k} s_i \cdot s_{i+k}$$

Here's the key insight: **if the signal is periodic with period T, its autocorrelation is also periodic with the same period**, showing maxima at T, 2T, 3T, etc.

To estimate **fundamental frequency (F0)**:
1. Pre-process: Filter and denoise
2. Find $k^*$: The first maximum of autocorrelation after $k=0$
3. Calculate: $F_0 = f_s / k^*$ where $f_s$ is the sampling frequency
4. Post-process: Fine-tune and correct errors

### [SLIDES 40-42: Spectral Analysis and Spectrograms]

For richer frequency information, we use the **Discrete Fourier Transform (DFT)**, typically implemented via the efficient **Fast Fourier Transform (FFT)** algorithm. The DFT converts our windowed time-domain signal into the frequency domain, giving us the magnitude spectrum.

A **spectrogram** visualizes how the spectrum changes over time. It's a 3D representation:
- X-axis: Time
- Y-axis: Frequency
- Color/brightness: Magnitude (darker = more energy)

The spectrograms on slides 41-42 show three vowels: [ih], [ae], and [uh]. Notice the dark horizontal bands? These are called **formants**—frequency bands that are particularly amplified by the vocal tract.

Different vowels produce different formant patterns because they're produced with different vocal tract configurations. This is how we distinguish vowels acoustically!

### [SLIDES 43-45: The Mel Scale]

Human hearing has a peculiar property: we're **not equally sensitive at all frequencies**. We have fine frequency resolution at low frequencies but coarser resolution at high frequencies.

The **mel scale** models this perceptual property. On the mel scale, pairs of sounds that sound equally far apart are separated by equal numbers of mels.

The conversion formula is:

$$mel(f) = 2595 \log_{10}\left(1 + \frac{f}{700}\right) = 1127 \ln\left(1 + \frac{f}{700}\right)$$

By definition, 1000 Hz = 1000 mels. So:
- 440 Hz ≈ 550 mels
- 2000 Hz ≈ 1500 mels
- 4000 Hz ≈ 2150 mels

Notice how the mel scale compresses high frequencies—doubling the frequency doesn't double the mel value.

### [SLIDES 46-48: Mel Filter Bank and Log Mel Spectrum]

To apply this perceptual model, we create a **mel filter bank**: a set of triangular bandpass filters whose center frequencies are spaced equally on the mel scale.

This means:
- Filters at low frequencies are narrowly spaced (fine resolution)
- Filters at high frequencies are widely spaced (coarse resolution)

Typically, we use 20-40 filters.

To compute the **log mel spectrum**:
1. Apply FFT to get the power spectrum
2. Multiply by each filter's transfer function
3. Sum the energy in each filter band
4. Take the logarithm

The logarithm is important because human loudness perception is also roughly logarithmic—we perceive loudness differences in terms of ratios, not absolute differences.

### [SLIDES 49-54: Cepstral Analysis]

Now we come to one of the most elegant techniques in speech processing: **cepstral analysis**.

Look at a speech spectrum—you'll see it has two components:
1. A smooth **spectral envelope** representing the vocal tract filter characteristics
2. Fine **spectral details** representing the source excitation (vocal fold vibration)

Cepstral analysis separates these. The mathematics involves a clever trick:

1. Take the log spectrum and treat it as if it were a time-domain signal
2. Compute the DFT of this log spectrum
3. The result is the **cepstrum**: $c_n = DFT(\log|S_k|)$

Fun fact: "Cepstrum" is an anagram of "spectrum"! The independent variable is called **quefrency** (anagram of frequency), and filtering in this domain is called **liftering** (anagram of filtering).

To extract the spectral envelope, apply a **low-pass lifter**:

$$w_n^{LP} = \begin{cases} 1 & |n| < n_c \\ 0.5 & |n| = n_c \\ 0 & |n| > n_c \end{cases}$$

Then: $\log|H_k| = DFT(w_n^{LP} \cdot c_n)$

### [SLIDES 55-57: MFCCs - Mel-Frequency Cepstral Coefficients]

**MFCCs** combine mel-scale analysis with cepstral analysis to create the most widely used features in speech processing.

The MFCC computation pipeline:
1. **Window** the signal (Hamming window)
2. **FFT** to get spectrum
3. Apply **mel filter bank** to get mel spectrum
4. Take **logarithm** to get log mel spectrum
5. Apply **DCT** (Discrete Cosine Transform) instead of DFT—this decorrelates the values
6. **Lifter**: Keep only the first 12-13 coefficients

Why DCT? Because the mel filter banks overlap, their outputs are correlated. The DCT decorrelates them, which is important for statistical classifiers that assume feature independence.

Why discard higher coefficients? They represent rapid spectral changes that are more related to pitch than phone identity, and can actually hurt recognition accuracy.

A complete MFCC feature vector often includes:
- 12-13 MFCCs
- Log energy
- 13 **delta** coefficients (first derivatives—how features change)
- 13 **delta-delta** coefficients (second derivatives—acceleration)

Total: 39 dimensions per frame.

### [SLIDES 58-60: GeMAPS Features]

While MFCCs dominate speech recognition, other feature sets exist for specialized applications. The **Geneva Minimalistic Acoustic Parameter Set (GeMAPS)** was designed specifically for **affective computing**—analyzing emotions in speech.

The **minimalistic set** has 18 low-level descriptors in three groups:
1. **Frequency-related**: F0, jitter, formant frequencies
2. **Energy/amplitude**: Loudness, shimmer
3. **Spectral balance**: Various spectral shape parameters

The **extended set** adds 7 more:
- Formant 2-3 bandwidth
- MFCCs 1-4
- Spectral flux

GeMAPS was designed based on which parameters best index physiological changes during emotional expression, supported by scientific literature and theoretical understanding.

---

## PART 7: MID-LEVEL FEATURES - SPEECH SEGMENTATION
### [SLIDES 61-64: Voice Activity Detection]

Moving up our hierarchy, **speech segmentation** partitions the continuous speech signal into discrete units. The most fundamental segmentation task is **Voice Activity Detection (VAD)**—simply determining when speech is present and when it's absent.

VAD is formulated as binary classification: for each frame, decide speech (1) or non-speech (0).

Most VAD algorithms have two stages:
1. **Feature extraction**: Compute discriminative features
2. **Detection**: Apply a decision rule to those features

Different approaches exist:
- Energy-based methods
- Spectral methods
- Statistical methods
- Machine learning classifiers
- Multimodal fusion (audio + video)

### [SLIDES 65-66: Energy Thresholding]

The simplest approach is **energy thresholding**:

$$E_n = \sum_{i=n-N+1}^{n} s_i^2$$

$$VAD_n = \begin{cases} 0 & \text{if } E_n < T \\ 1 & \text{if } E_n \geq T \end{cases}$$

The challenge is setting threshold $T$. In noisy environments, noise energy might exceed the threshold, or quiet speech might fall below it.

Solution: **Adaptive thresholds** that update during prolonged silence periods. The algorithm tracks the noise floor and adjusts the threshold accordingly.

### [SLIDES 67-70: Cepstrum-Based VAD and G.729]

A more sophisticated approach uses **cepstral distance**:

$$d_n = \frac{1}{H} \sum_{h=1}^{H} (c_{nh} - c'_h)^2$$

Where $c_{nh}$ is the cepstrum of frame $n$ and $c'_h$ is a reference noise cepstrum. Large distances indicate speech; small distances indicate noise.

The **G.729 standard** (used in VoIP telephony) implements a comprehensive VAD algorithm:
1. Use 10 ms frames
2. Compute 4 features: full-band energy, low-band energy, line spectral frequencies (LSF), zero-crossing rate
3. Apply 14 decision rules to feature differences from running averages
4. Smooth decisions using energy and neighboring frames
5. Update noise estimates only during confirmed background noise

This multi-feature, adaptive approach is much more robust than simple energy thresholding.

### [SLIDES 71-73: Advanced VAD and Evaluation]

Modern research explores:
- **Machine learning**: Train classifiers (neural networks, SVMs) on labeled speech/non-speech data
- **Multi-class**: Distinguish noise, speech, singing, music
- **Multimodal**: Combine audio with visual lip-reading information

VAD evaluation uses these metrics:
- **FEC (Front End Clipping)**: Speech missed at utterance onset
- **MSC (Mid Speech Clipping)**: Speech missed during an utterance
- **OVER (Overhang)**: Noise classified as speech at utterance offset
- **NDS (Noise Detected as Speech)**: Noise classified as speech during silence

Ground truth comes from careful manual annotation.

---

## PART 8: AUTOMATIC SPEECH RECOGNITION
### [SLIDES 74-76: Introduction to ASR]

Now we reach the pinnacle of our framework: converting acoustic waveforms into text. The task of **Automatic Speech Recognition (ASR)** is to take an acoustic waveform as input and produce a string of words as output.

Let me share a delightful piece of history. The first "speech recognizer" was a 1920s toy called **Radio Rex**—a celluloid dog in a house. When you called "Rex!", a spring released and the dog jumped out. The mechanism? It triggered on acoustic energy around 500 Hz—roughly the first formant of the vowel [e] in "Rex."

Obviously, we've come a long way since Radio Rex!

### [SLIDES 77-78: Encoder-Decoder Architecture]

Historically, **Hidden Markov Models (HMMs)** dominated ASR for decades. Today, the standard approach uses **encoder-decoder networks**, also called **sequence-to-sequence (seq2seq)** models.

The key insight: an encoder network processes the input sequence and produces a condensed representation (the **context vector**). A decoder network then generates the output sequence from this context.

This architecture can handle inputs and outputs of different—even vastly different—lengths, which is exactly what ASR requires.

### [SLIDES 79-80: Input/Output Representation]

The **input** is a sequence of $N$ acoustic feature vectors:
$$\mathbf{F} = (\mathbf{f}_1, \mathbf{f}_2, ..., \mathbf{f}_N)$$

Each vector represents one 10 ms frame, typically containing log mel spectral features or MFCCs.

The **output** is a sequence of $M$ characters:
$$Y = (\langle SOS \rangle, \hat{y}_1, \hat{y}_2, ..., \hat{y}_M, \langle EOS \rangle)$$

Where $\langle SOS \rangle$ and $\langle EOS \rangle$ are special start/end tokens, and each $\hat{y}_i$ comes from an alphabet:
$$A = \{a, b, c, ..., z, 0, 1, ..., 9, \langle space \rangle, \langle comma \rangle, \langle period \rangle, \langle apostrophe \rangle, \langle unknown \rangle\}$$

### [SLIDES 81-83: Subsampling]

There's a major challenge: the input sequence is MUCH longer than the output. A 2-second word might require 200 acoustic frames but only 5 letters!

**Subsampling** addresses this by shortening the acoustic sequence before encoding.

The simplest method is **low frame rate**:
1. Concatenate feature vector $\mathbf{f}_i$ with $\mathbf{f}_{i-1}$ and $\mathbf{f}_{i-2}$
2. Delete $\mathbf{f}_{i-1}$ and $\mathbf{f}_{i-2}$

Result: Instead of 40-dimensional vectors every 10 ms, we have 120-dimensional vectors every 30 ms. The sequence is 3× shorter but vectors are 3× longer.

More sophisticated approaches use convolutional networks or pyramidal RNNs.

### [SLIDES 84-86: The Encoder]

The **encoder** transforms the subsampled sequence $\mathbf{X} = (\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_K)$ into contextualized representations $\mathbf{H} = (\mathbf{h}_1^e, \mathbf{h}_2^e, ..., \mathbf{h}_K^e)$.

For an RNN encoder:
$$\mathbf{h}_t^e = g(\mathbf{x}_t, \mathbf{h}_{t-1}^e, \theta)$$

For example: $\mathbf{h}_t^e = ReLU(W\mathbf{x}_t + U\mathbf{h}_{t-1}^e)$

The **context vector** $\mathbf{c}$ captures the essence of the entire input. For simple RNNs, it's just the final hidden state: $\mathbf{c} = \mathbf{h}_K^e$.

More sophisticated encoders use:
- Bidirectional RNNs (process forward AND backward)
- LSTMs or GRUs (handle long-range dependencies)
- Transformers with self-attention (current state-of-the-art)

### [SLIDES 87-90: The Decoder]

The **decoder** generates the output sequence one character at a time:

$$\mathbf{h}_0^d = \mathbf{c}$$
$$\mathbf{h}_1^d = g(\langle SOS \rangle, \mathbf{h}_0^d, \mathbf{c}, \theta)$$
$$\mathbf{h}_t^d = g(\hat{y}_{t-1}, \mathbf{h}_{t-1}^d, \mathbf{c}, \theta)$$

To produce output:
$$\mathbf{s}_t = \text{softmax}(f(\mathbf{h}_t^d, \rho))$$
$$\hat{y}_t = \text{argmax}(\mathbf{s}_t)$$

The softmax gives a probability distribution over all characters; we pick the most likely one.

This is **autoregressive** generation—each output depends on all previous outputs. During training, we use **teacher forcing** (feed in ground truth). During inference, we use our own predictions.

The standard architecture combining these elements is called **Listen, Attend, and Spell (LAS)** or **Attention-based Encoder-Decoder (AED)**.

Modern systems use **Transformers** for both encoder and decoder, achieving remarkable accuracy.

### [SLIDES 91-92: Evaluation with WER]

How do we measure ASR performance? The standard metric is **Word Error Rate (WER)**:

$$WER = 100 \cdot \frac{n_i + n_s + n_d}{n}$$

Where:
- $n_i$ = number of insertions (extra words in hypothesis)
- $n_s$ = number of substitutions (wrong words)
- $n_d$ = number of deletions (missing words)
- $n$ = total words in reference transcript

We compute these using **minimum edit distance** between the hypothesis and reference.

**Example**:
- Reference: "I *** ** UM the PHONE IS I LEFT THE portable **** PHONE UPSTAIRS last night"
- Hypothesis: "I GOT IT TO the ***** FULLEST I LOVE TO portable FORM OF STORES last night"
- Insertions: 3, Substitutions: 6, Deletions: 1, Total words: 13
- WER = (3+6+1)/13 = 76.9%

Note: WER can exceed 100% if there are many insertions!

Modern transformer-based systems achieve WERs below 3% on clean read speech (LibriSpeech benchmark)—approaching human-level accuracy. However, challenging conditions (noise, accents, spontaneous speech) still pose difficulties.

---

## SUMMARY

Today we've journeyed through the complete speech processing pipeline:

1. **Physical Foundations**: Sound as pressure waves, pure vs. complex sounds, Fourier decomposition, harmonic series

2. **Phonetics**: Phones, IPA/ARPAbet, vocal production, voiced/unvoiced sounds, consonant classification (place and manner), vowel classification

3. **Digital Recording**: Microphones, amplification, ADC (sampling, quantization, coding), recording parameters

4. **Frame Analysis**: Windows, overlapping frames, Hamming/Blackman functions

5. **Low-Level Features**: RMS, autocorrelation, F0 estimation, spectrograms, formants, mel scale, mel filter banks, cepstral analysis, MFCCs, GeMAPS

6. **Mid-Level Features**: Speech segmentation, VAD algorithms, evaluation metrics

7. **ASR**: Encoder-decoder networks, subsampling, attention mechanisms, WER evaluation

This hierarchical framework—from physical signals through features to semantic content—provides the foundation for understanding all modern speech technology, from voice assistants to transcription services to speech synthesis.

---

*Ready for the next lecture slides when you are! The framework we've established here will serve as the foundation for more advanced topics.*




# Lecture 6: Multimodal Machine Learning
## Comprehensive Lecture Notes

---

### Introduction to Multimodal Machine Learning [Slides 1-2]

Welcome to our final lecture in this series, where we'll be exploring one of the most exciting and rapidly evolving areas in modern machine learning: multimodal machine learning. This topic represents a fundamental shift in how we think about building intelligent systems—moving away from single-source data processing toward systems that can integrate and reason across multiple types of information simultaneously.

Let me start with a fundamental observation: our experience of the world is inherently multimodal. We see objects, hear sounds, feel textures, smell odors, and taste flavors. A "modality" refers to the way in which something happens or is experienced, and a research problem is characterized as multimodal when it includes multiple such modalities. For artificial intelligence to make genuine progress in understanding the world around us, it needs to be able to interpret these multimodal signals together—just as humans do naturally every day.

The foundational work we'll be drawing upon throughout this lecture comes from Baltrusaitis, Ahuja, and Morency's influential 2019 survey paper, "Multimodal Machine Learning: A Survey and Taxonomy," published in IEEE Transactions on Pattern Analysis and Machine Intelligence. This paper has become the definitive reference in the field, going beyond the typical early and late fusion categorization to identify broader challenges that multimodal machine learning must address.

The objective of multimodal machine learning is straightforward in concept but remarkably challenging in practice: building models that can process and relate information from multiple modalities. Consider the diagram shown in your slides—we see EEG signals being processed into EEG features, eye tracking signals being processed into eye features, and facial expressions being captured. All of these different data streams need to be combined meaningfully to perform tasks like emotion classification or make other high-level predictions.

Why is this so important? There are several compelling reasons:

First, **robustness**: when one modality is noisy, corrupted, or missing entirely, other modalities can compensate. Think about how you can still understand someone speaking in a noisy environment by reading their lips—you're performing multimodal integration.

Second, **complementary information**: different modalities often capture different aspects of the same underlying phenomenon. Audio might capture emotional tone while video captures facial expressions and body language. Together, they provide a richer understanding than either alone.

Third, **disambiguation**: sometimes a single modality is genuinely ambiguous, but combining multiple modalities resolves the ambiguity. The classic example is speech recognition—acoustic signals alone might be ambiguous between similar-sounding words, but lip movements can disambiguate them.

---

### The Five Core Technical Challenges [Slide 3]

Baltrusaitis and colleagues identified five core technical open challenges that any multimodal machine learning system must address. Understanding these challenges provides a conceptual framework for thinking about the entire field. Let me walk through each one in detail.

**1. Representation**

The first fundamental challenge is learning how to represent and summarize multimodal data in a way that exploits the complementarity and redundancy of multiple modalities. The heterogeneity of multimodal data makes this particularly challenging. Consider the stark differences between modalities: language is often symbolic and discrete, while audio and visual modalities are typically represented as continuous signals. How do we create a unified representation that captures the essential information from such diverse sources?

There are broadly two approaches to multimodal representation. **Joint representations** combine the unimodal signals into a single representation, typically through concatenation or more sophisticated methods. **Coordinated representations** process unimodal signals separately but enforce certain constraints that bring them into the same representational space—for instance, ensuring that semantically similar concepts in different modalities map to nearby points in the representation space.

**2. Translation**

The second challenge is translation: how do we translate or map data from one modality to another? This is not a straightforward mapping problem because the relationship between modalities is often open-ended or subjective. Think about image captioning—there isn't a single "correct" description of an image; there are many valid ways to describe what you see. Similarly, generating an image from a text description involves creative interpretation.

Translation tasks include speech-to-text, text-to-speech, image captioning, visual question answering, and even more ambitious tasks like generating video from text descriptions or music from visual input.

**3. Alignment**

The third challenge—alignment—involves identifying the direct relations between elements from two or more different modalities. This is often about temporal or spatial correspondence. For example, we may want to align the steps in a recipe (text) to specific moments in a video showing the dish being made. Or we might want to align phonemes in speech to corresponding mouth movements in video.

Alignment can be explicit (where we directly identify correspondences) or implicit (where alignment happens as an intermediate step in achieving some other task). The challenge involves measuring similarity between fundamentally different modalities and dealing with possible long-range dependencies and ambiguities.

**4. Fusion**

The fourth challenge—and the one we'll focus on most heavily today—is fusion: how do we join information from two or more modalities to perform a prediction? As marked on your slides, this is our primary topic for this lecture. Fusion is perhaps the most practically important challenge because it's central to any system that needs to make decisions based on multimodal input.

The key insight is that fusion is not just about combining data—it's about combining data in a way that leverages the complementary and redundant nature of different modalities to improve task performance. We'll explore various approaches to fusion in considerable detail.

**5. Co-learning**

The fifth challenge is co-learning: how do we transfer knowledge between modalities, their representations, and their predictive models? This is particularly valuable in scenarios where one modality has abundant labeled data while another has very limited data. Can we use what we learn from the data-rich modality to improve learning in the data-poor modality?

Co-learning encompasses transfer learning across modalities, zero-shot learning (recognizing concepts in one modality based on information from another), and various forms of semi-supervised and weakly supervised learning that leverage multiple modalities.

---

### Defining Multimodal Fusion [Slide 4]

Now let's focus specifically on multimodal fusion. According to D'Ulizia (2009), multimodal fusion is defined as "the process of integrating information from various input modalities and combining them into a complete command" for a multimodal system.

It's worth noting that the literature uses various terms for this concept, which can sometimes cause confusion when reading different papers:

- **Combination** (Neal et al., 1989)
- **Cooperation of modalities** (Martin et al., 1998)
- **Integration / multimodal integration** (Pfleger, 2004; Shikler et al., 2004; Johnston and Bangalore, 2005)

All of these terms refer essentially to the same fundamental problem: how do we bring together information from different sources to make better decisions than we could with any single source alone?

The benefits of multimodal fusion, as documented in the research literature, include:

1. **Robustness**: The system can continue functioning even when one or more modalities fail or are degraded.
2. **Complementary information gain**: Different modalities provide different types of information that, when combined, give a more complete picture.
3. **Functional continuity**: The system remains operational even when some modalities are unavailable, though perhaps with reduced accuracy.

---

### Categories of Multimodal Fusion Approaches [Slide 5]

The taxonomy presented by Baltrusaitis et al. (2019) divides multimodal fusion approaches into two major categories: model-agnostic approaches and model-based approaches. Understanding this distinction is crucial for selecting appropriate methods for different applications.

**Model-Agnostic Approaches**

Model-agnostic approaches do not depend on specific machine learning methods. They use techniques that were not specifically designed to cope with multimodal data but can be adapted for this purpose. The key characteristic is that the fusion strategy is independent of the learning algorithm—you could swap out the classifier or regressor without changing how the modalities are combined.

Model-agnostic approaches are further divided into:
- **Early fusion** (feature-level fusion)
- **Late fusion** (decision-level fusion)  
- **Hybrid fusion** (combining elements of both)

**Model-Based Approaches**

Model-based approaches address fusion in their construction—the fusion mechanism is built into the model architecture itself. These approaches include:

- **Kernel-based methods** (particularly Multiple Kernel Learning)
- **Graphical models** (like Dynamic Bayesian Networks)
- **Neural networks** (which have become dominant in recent years)

Looking at the taxonomy table from the slides, we can see how different fusion types are applied to various tasks. For instance:

- Early fusion with classification output is commonly used for emotion recognition
- Late fusion with regression output and temporal modeling is used for emotion recognition
- Kernel-based methods are applied to object classification and emotion recognition
- Graphical models are used for audio-visual speech recognition (AVSR), emotion recognition, and media classification
- Neural networks are employed across the board for various tasks including emotion recognition and AVSR

The table also distinguishes between methods that use temporal information and those that don't—this is particularly important for applications involving sequential data like video or speech.

---

### Model-Agnostic Approaches: Early Fusion [Slide 6]

Let me now walk through each of the model-agnostic approaches in detail, starting with early fusion, also known as feature-level fusion.

In early fusion, the features extracted from input data are first combined by a **Feature Fusion unit (FF)** and then sent to a single **Analysis Unit (AU)** that performs the analysis task—for example, making a classification decision.

Looking at the diagram from Atrey et al. (2009) shown in your slides, we see feature vectors F₁, F₂, through Fₙ coming from different modalities. These are fed into the Feature Fusion unit, which combines them into a single fused feature vector F₁,ₙ. This combined vector is then processed by a single Analysis Unit to produce the decision D.

The most common implementation of early fusion is simple **concatenation**—literally appending the feature vectors from different modalities end-to-end to create one long feature vector. However, more sophisticated fusion methods exist:

- **Element-wise operations** (max, min, sum, product) when feature vectors have the same dimensionality
- **Tensor products** or outer products to capture interactions between features
- **Learned projections** that transform features into a common space before concatenation

**When does early fusion make sense?**

Early fusion is particularly effective when:
- Modalities are closely related and interact at a low level
- You want to capture cross-modal correlations from the raw or low-level features
- You have sufficient training data to learn from high-dimensional combined representations
- All modalities are reliably available at both training and test time

**Practical Considerations**

From a practical standpoint, early fusion requires that features from different modalities be synchronized—they need to correspond to the same time point or the same sample. If your audio is sampled at 16kHz and your video at 30fps, you need to handle this temporal alignment before concatenation.

---

### Model-Agnostic Approaches: Late Fusion [Slide 7]

Late fusion, also known as decision-level fusion, takes a fundamentally different approach. Instead of combining features before learning, we first learn from each modality independently and then combine the resulting decisions or predictions.

Looking at the diagram, we see that each feature vector F₁, F₂, through Fₙ goes to its own dedicated Analysis Unit (AU). Each AU produces a local decision: D₁, D₂, through Dₙ. These local decisions are then combined using a **Decision Fusion (DF)** unit to make a fused decision vector D₁,ₙ. This fused decision vector can then be further analyzed by another Analysis Unit to get the final decision D.

The key insight here is that each modality is processed through its own complete pipeline before any fusion happens. This has profound implications for system design and performance.

**Methods for Decision Fusion**

The Decision Fusion unit can employ various strategies:

1. **Voting methods**: Majority voting is the simplest—the final class is whichever class receives the most "votes" from individual classifiers.

2. **Score-level fusion**: Instead of hard class predictions, each classifier outputs probability scores or confidence values, which are then combined through:
   - Simple averaging
   - Weighted averaging (more confident or accurate classifiers get higher weights)
   - Product rule
   - Max rule

3. **Stacking or meta-learning**: Train a second-level classifier that takes the predictions of the first-level classifiers as input.

4. **Learned fusion**: Use a neural network or other learnable function to combine the outputs.

**When does late fusion make sense?**

Late fusion is particularly effective when:
- Modalities are relatively independent and capture different aspects of the phenomenon
- You want to use different, specialized models for each modality
- Some modalities may be missing at test time (the system can still make predictions from available modalities)
- You have limited training data (training separate smaller models may be easier than one large multimodal model)

---

### Model-Agnostic Approaches: Hybrid Fusion [Slide 8]

Hybrid fusion represents a combination of early and late fusion strategies, attempting to capture the benefits of both approaches. The architecture can become quite complex, but let me walk through the diagram.

In the hybrid approach shown:

1. Some features (F₁ and F₂) are first fused by a Feature Fusion (FF) unit, creating a combined feature vector F₁,₂
2. This combined vector is then analyzed by an Analysis Unit, producing decision D₁,₂
3. Simultaneously, other features (Fₙ₋₁ and Fₙ) are analyzed by their own individual AUs, producing decisions Dₙ₋₁ and Dₙ
4. These individual decisions are fused by a Decision Fusion unit, producing D₍ₙ₋₁,ₙ₎
5. Finally, all the decisions (D₁,₂ and D₍ₙ₋₁,ₙ₎) are further fused by another DF unit to obtain the final decision D

This architecture allows for strategic choices about which modalities to fuse early (those that are closely related and benefit from feature-level interaction) and which to fuse late (those that are more independent or benefit from specialized processing).

**Design Considerations for Hybrid Fusion**

The challenge with hybrid fusion is that the design space is enormous. You need to decide:
- Which modalities to group for early fusion
- At what level of feature abstraction to perform early fusion
- How to combine the outputs of different fusion streams

Recent research has explored "gradual fusion" architectures where highly correlated modalities are fused earlier in the network while other modalities are fused at later stages. This allows the architecture to adapt to the actual statistical relationships between modalities.

---

### Fusion Methods [Slide 9]

Now let's discuss the specific methods used to actually combine information, regardless of whether we're doing early or late fusion.

**Statistical Rule-Based Methods**

These include simple operators like max, min, and, or, majority voting, and linear weighted fusion.

**Max, Min, And, Or Operations**

These apply simple operators mainly for feature-level fusion. For example:
- **Max fusion**: At each feature dimension, select the maximum value across modalities. This is useful when you want to preserve strong signals regardless of which modality they come from.
- **Min fusion**: Select the minimum value, which can act as a kind of "AND" operation—a feature must be strong in all modalities to be strong in the fused representation.

**Majority Voting**

Majority voting is mainly used in decision-level fusion. The final decision is the one made by the majority of the classifiers. For a three-class problem with three modality classifiers:
- If modality 1 predicts "happy"
- Modality 2 predicts "happy"
- Modality 3 predicts "sad"

The final prediction is "happy" because it received 2 out of 3 votes.

Variations include:
- **Simple majority**: Just count votes
- **Weighted majority**: Different classifiers contribute different vote weights based on their reliability
- **Plurality voting**: For multi-class problems, the class with the most votes wins (doesn't require more than 50%)

**Linear Weighted Fusion**

For score-level fusion, we might compute:

$$\text{fused\_score}(c) = \sum_{m=1}^{M} w_m \cdot \text{score}_m(c)$$

where $w_m$ is the weight for modality $m$ and $\text{score}_m(c)$ is the score assigned to class $c$ by modality $m$. The weights might be:
- Equal (simple averaging)
- Based on validation performance
- Learned during training
- Dynamically adjusted based on input characteristics

---

### Early / Late Fusion: Pros and Cons [Slide 10]

Let me now provide a comprehensive comparison of early and late fusion, synthesizing the material from the slides with additional insights from the research literature.

**Early Fusion**

*Advantages:*

1. **Easy to implement**: The most basic form—concatenation of features—requires minimal additional infrastructure. You simply combine feature vectors and feed them to your favorite classifier.

2. **Single model training**: You only need to train one model, which can be computationally efficient and simplifies the development pipeline.

3. **Can capture low-level cross-modal interactions**: Because features are combined before any high-level processing, the model can potentially learn correlations between raw features from different modalities. For example, the correlation between a particular visual pattern and a specific acoustic feature.

4. **End-to-end optimization**: When using neural networks, early fusion allows gradients to flow back through the entire system, enabling joint optimization of feature extraction and fusion.

*Disadvantages:*

1. **High dimensionality**: Concatenating features from multiple modalities can result in very high-dimensional feature vectors. This can lead to the "curse of dimensionality"—you need exponentially more training data as dimensionality increases, and many machine learning algorithms struggle with very high-dimensional inputs.

2. **Different framerates pose challenges**: If modalities have different temporal resolutions (e.g., audio sampled at 16kHz vs. video at 30fps), aligning them for concatenation is non-trivial. You may need to upsample, downsample, or aggregate features, potentially losing important information.

3. **Less flexible**: Once features are fused, it becomes difficult to modify or remove specific modalities without re-evaluating the entire feature extraction process.

4. **Sensitive to missing modalities**: If one modality is unavailable at test time, the entire concatenated feature vector is incomplete, often requiring special handling.

**Late Fusion**

*Advantages:*

1. **Different models for each modality**: You can select the best model architecture for each modality—a CNN for images, an RNN for audio, a transformer for text—and optimize each independently.

2. **Handles missing modalities gracefully**: If one modality is unavailable, you can still make predictions from the available modalities, though perhaps with reduced confidence.

3. **Modularity**: You can update or improve individual modality models without retraining the entire system.

4. **Uncorrelated errors**: Because classifiers are trained independently, their errors tend to be uncorrelated. Combining uncorrelated predictions often improves overall accuracy.

*Disadvantages:*

1. **Does not model low-level interactions**: Because each modality is processed independently before fusion, the system cannot learn low-level correlations between modalities. The relationship between a specific pixel pattern and a specific audio frequency, for instance, cannot be directly modeled.

2. **Multiple training stages**: You need to train separate models for each modality and then potentially train a fusion mechanism, adding complexity.

3. **Higher computational cost**: During inference, you must run multiple separate models rather than a single unified model.

**Hybrid Fusion**

The slides note that hybrid fusion "combines benefits of both early and late fusion approaches." This is true in principle, but it comes with increased architectural complexity. Designing an effective hybrid architecture requires understanding which modalities benefit from early fusion and which should be processed independently.

---

### Model-Based Approaches: Multiple Kernel Learning [Slides 11-12]

Now let's move to model-based approaches, starting with Multiple Kernel Learning (MKL), which represents an elegant mathematical framework for multimodal fusion.

**Background: Support Vector Machines and Kernels**

To understand MKL, you first need to understand Support Vector Machines (SVMs) and kernel methods. An SVM is a supervised learning algorithm that finds the optimal hyperplane separating different classes. The "kernel trick" allows SVMs to operate in high-dimensional feature spaces without explicitly computing the coordinates in that space—instead, we compute inner products using a kernel function.

A kernel function $k(x_i, x_j)$ measures the similarity between two data points. Common kernels include:
- Linear kernel: $k(x_i, x_j) = x_i^T x_j$
- Polynomial kernel: $k(x_i, x_j) = (x_i^T x_j + c)^d$
- RBF (Gaussian) kernel: $k(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)$

**Multiple Kernel Learning**

The key insight of MKL is that instead of selecting one specific kernel function, we can use multiple kernels and learn how to combine them optimally. This is particularly powerful for multimodal fusion because we can design different kernels for different modalities.

Looking at the diagram in the slides, we see:
- Feature space $x$ (MRI data) mapped through kernel $k^{(x)}$
- Feature space $y$ (PET data) mapped through kernel $k^{(y)}$
- Feature space $z$ (CSF data) mapped through kernel $k^{(z)}$

Each base kernel captures similarity within its own modality. The combined kernel $k$ is formed as a weighted combination:

$$k = \beta_x k^{(x)} + \beta_y k^{(y)} + \beta_z k^{(z)}$$

The weights $\beta_x$, $\beta_y$, $\beta_z$ are learned during training along with the SVM parameters. This allows the algorithm to automatically determine how much each modality should contribute to the final decision.

**Advantages of MKL**

1. **Broad applicability**: MKL can be applied across various domains and modalities. The same mathematical framework works whether you're combining imaging modalities, text and images, or audio and video.

2. **Convex optimization**: The loss function in standard MKL formulations is convex, meaning we're guaranteed to find the global optimum (no local minima problems).

3. **Applicable to both classification and regression**: The framework extends naturally to both supervised learning paradigms.

4. **Interpretable weights**: The learned kernel weights provide insight into which modalities are most important for the task.

**Disadvantages of MKL**

1. **Reliance on training data during test**: SVMs (and hence MKL) require storing support vectors, which are a subset of training examples. This can be problematic for very large datasets.

2. **Fixed global weights**: Traditional MKL learns a single set of weights for the entire dataset. This may be suboptimal if different regions of the feature space benefit from different modality combinations.

3. **Scalability**: While more scalable than some alternatives, MKL still faces computational challenges with very large datasets.

**Localized Multiple Kernel Learning**

To address the limitation of global weights, researchers have developed Localized MKL (LMKL), which introduces a gating function for each kernel. This allows the combination weights to vary across the input space—different samples can have different modality contributions based on their characteristics.

---

### Model-Based Approaches: Neural Networks [Slide 12]

The slides mention that neural networks have become another common choice for multimodal fusion, and this is something of an understatement—neural networks have come to dominate the field in recent years, particularly with the rise of deep learning.

**Advantages of Neural Networks for Multimodal Fusion**

1. **Learning from large amounts of data**: Deep neural networks excel when large amounts of training data are available. Their capacity to learn complex functions scales well with data size.

2. **End-to-end training**: Perhaps the most significant advantage. With neural networks, we can jointly optimize feature extraction, representation learning, and fusion in a single unified framework. Gradients flow through the entire system, allowing all components to be trained together.

3. **Learning both multimodal representation and fusion**: Neural networks can simultaneously learn how to represent each modality and how to combine them. This contrasts with traditional approaches where representation and fusion are often separate stages.

4. **Complex decision boundaries**: Deep networks can learn highly non-linear decision boundaries that may be necessary for complex multimodal relationships.

5. **Flexible architectures**: Neural networks offer tremendous flexibility in architecture design. Different modalities can use specialized architectures (CNNs for images, RNNs/Transformers for sequences) that are then combined in various ways.

**Neural Network Fusion Architectures**

Modern neural approaches to multimodal fusion include:

**Simple Concatenation Fusion**: Features from different modality-specific networks are concatenated and fed through fully connected layers. This is essentially early fusion implemented with neural networks.

**Attention-Based Fusion**: Attention mechanisms allow the network to dynamically weight different modalities based on the input. Cross-modal attention enables features from one modality to "attend to" relevant features from another modality.

**Transformer-Based Fusion**: Vision-language models like CLIP, VisualBERT, and LXMERT use transformer architectures that can naturally handle multiple modalities through attention mechanisms.

**Tensor Fusion**: Methods like Tensor Fusion Networks compute outer products between modality representations to capture complex interactions.

**Disadvantages of Neural Networks**

1. **Lack of interpretability**: Neural networks are often "black boxes"—it's difficult to understand why they make particular predictions or how different modalities contribute to decisions.

2. **Need for large training datasets**: While neural networks can leverage big data, they often require it. With limited data, they may overfit or fail to learn meaningful representations.

3. **Computational requirements**: Training deep multimodal networks requires significant computational resources, including GPUs/TPUs and substantial memory.

4. **Hyperparameter sensitivity**: Neural networks have many hyperparameters (learning rate, architecture choices, regularization) that can significantly impact performance and require careful tuning.

---

### Other Model-Based Examples [Slide 13]

The final slide presents a comprehensive table of model-based approaches from Atrey et al. (2009), showing the diversity of methods applied to multimodal fusion. Let me highlight some key observations from this table.

**Support Vector Machines**

SVMs have been applied at multiple fusion levels:
- **Decision level**: Adams et al. combined video (color, structure, shape), audio (MFCC), and textual cues for semantic concept detection.
- **Hybrid level**: Wu et al. combined video and audio for multimedia data analysis; Zhu et al. worked on image classification using multiple visual features.

**Bayesian Inference**

Bayesian methods provide a principled probabilistic framework for fusion:
- **Feature level**: Pitsikalis et al. combined audio (MFCC) with video (shape and texture) for speech recognition.
- **Decision level**: Meyer et al. combined audio with video (lips contour) for spoken digit recognition.
- **Hybrid level**: Xu and Chua; Atrey et al. worked on sports video analysis and event detection/surveillance respectively.

**Dempster-Shafer Theory**

This theory provides a mathematical framework for combining evidence from multiple sources:
- **Feature level**: Mena and Malpica worked on segmentation of satellite images using trajectory coordinates.
- **Decision level**: Guironnet et al. (video classification), Singh et al. (fingerprint classification), and Reddy (human-computer interaction) used audio-visual features.

**Dynamic Bayesian Networks**

These extend Bayesian networks to handle temporal data:
- **Feature level**: Wang et al. (video shot classification), Nefian et al. (speech recognition), Nock et al. (speaker localization), Chaisorn et al. (story segmentation).
- Various applications combining audio and video with features like MFCC, visual DCT coefficients, lip region features, and motion features.

**Applications Across the Table**

The variety of applications is striking:
- Biometric verification and identification
- Semantic concept detection
- Speech recognition
- Emotion recognition
- Object tracking
- Video classification and shot classification
- Human-computer interaction
- Multimedia analysis
- Event detection and surveillance

This diversity demonstrates that multimodal fusion is not a niche technique but a fundamental approach applicable across computer vision, speech processing, multimedia analysis, and human-computer interaction.

---

### Current Trends and Future Directions

While the slides focus on the foundational approaches, let me briefly mention some current trends in multimodal fusion that build on these foundations.

**Transformer-Based Multimodal Models**

The transformer architecture has revolutionized multimodal learning. Models like CLIP (Contrastive Language-Image Pre-training), ALIGN, and Florence learn joint representations of images and text through contrastive learning on massive datasets. These models can perform zero-shot classification, image-text retrieval, and visual question answering with impressive accuracy.

**Large Multimodal Models (LMMs)**

Building on large language models, LMMs like GPT-4V, Gemini, and LLaVA can process images, text, and sometimes other modalities in a unified framework. These represent a new paradigm where the fusion happens implicitly within a massive pre-trained model.

**Attention and Cross-Modal Transformers**

Cross-modal attention mechanisms allow models to align and integrate information across modalities at multiple levels of abstraction. This provides a learnable, flexible form of alignment and fusion that can adapt to the specific relationships in the data.

**Self-Supervised Multimodal Learning**

Methods that learn from unlabeled multimodal data are increasingly important. Contrastive learning objectives that bring corresponding items from different modalities closer in representation space while pushing non-corresponding items apart have proven highly effective.

---

### Summary

Let me conclude with a summary of the key points from this lecture:

1. **Multimodal machine learning** aims to build models that process and relate information from multiple modalities, addressing five core challenges: representation, translation, alignment, fusion, and co-learning.

2. **Multimodal fusion** specifically concerns how to join information from multiple modalities to perform predictions. It's central to any practical multimodal system.

3. **Model-agnostic approaches** include:
   - **Early fusion**: Combine features before learning. Simple, captures low-level interactions, but can be high-dimensional and inflexible.
   - **Late fusion**: Learn from each modality independently, then combine decisions. Flexible, handles missing modalities, but misses low-level interactions.
   - **Hybrid fusion**: Combines elements of both for potentially better performance at the cost of increased complexity.

4. **Model-based approaches** include:
   - **Multiple Kernel Learning**: Elegant mathematical framework with convex optimization, but limited scalability.
   - **Neural Networks**: Dominant current approach, enabling end-to-end learning of representations and fusion, but requires large data and lacks interpretability.
   - **Graphical models**: Provide principled probabilistic frameworks, especially useful for temporal data.

5. **Fusion methods** range from simple rules (max, min, voting) to sophisticated learned functions. The choice depends on the nature of the modalities, the amount of data, and the specific requirements of the application.

6. **No single approach is universally best**. The optimal fusion strategy depends on:
   - The nature and relationships between modalities
   - The amount and quality of training data
   - Whether modalities may be missing at test time
   - Computational constraints
   - Interpretability requirements

As AI systems increasingly need to understand our multimodal world, the techniques we've discussed today will continue to evolve and find new applications. From autonomous vehicles processing camera, lidar, and radar data to healthcare systems integrating imaging, genomics, and clinical records, multimodal fusion is becoming a cornerstone capability for artificial intelligence.

Thank you for your attention throughout this course. The concepts you've learned—from basic machine learning principles to advanced topics like adversarial robustness, fairness, and now multimodal fusion—provide a strong foundation for understanding and contributing to the field of trustworthy artificial intelligence.

---

### References

- Baltrusaitis, T., Ahuja, C., & Morency, L. P. (2019). Multimodal machine learning: A survey and taxonomy. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(2), 423-443.
- Atrey, P. K., Hossain, M. A., El Saddik, A., & Kankanhalli, M. S. (2010). Multimodal fusion for multimedia analysis: a survey. Multimedia Systems, 16(6), 345-379.
- D'Ulizia, A. (2009). Exploring multimodal input fusion strategies. Handbook of Research on Multimodal Human Computer Interaction and Pervasive Services.
- Gönen, M., & Alpaydin, E. (2011). Multiple kernel learning algorithms. The Journal of Machine Learning Research, 12, 2211-2268.
- Ramachandram, D., & Taylor, G. W. (2017). Deep multimodal learning: A survey on recent advances and trends. IEEE Signal Processing Magazine, 34(6), 96-108.

---

*End of Lecture Notes*
















